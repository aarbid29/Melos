{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "This is the training section of the Spectrogram U-Net for Music Source Separation. Before running this, make sure the Spectrograms directory (along with the data points for testing and training, of course) is generated by running the preprocessor.py file once.\n",
    "\n",
    "The training process utilizes the architecture of U-Net implemented in the UNet.py file and the loss functions implemented in the loss_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from architectures.UNet.UNet import SpectrogramUNet\n",
    "from architectures.UNet.loss_functions import VocalLoss, InstrumentLoss\n",
    "from dataset import DSDDataset\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializations and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECTROGRAMS_PATH = './Spectrograms'\n",
    "VOCAL_ONLY = True\n",
    "MODEL_PATH = \"./models/vocal-accompaniment-separation\" if VOCAL_ONLY else \"./models/all-separation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_CHANNELS = 1\n",
    "OUT_CHANNELS = 2 if VOCAL_ONLY else 4\n",
    "FEATURES = [32, 64, 128, 256, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAL_ALPHA = 0.707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA1 = 0.297\n",
    "ALPHA2 = 0.262\n",
    "ALPHA3 = 0.232\n",
    "ALPHA4 = 0.209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpectrogramUNet(in_channel=IN_CHANNELS, out_channel=OUT_CHANNELS, features=FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─ModuleList: 1-1                             --\n",
      "|    └─DoubleConv: 2-1                        --\n",
      "|    |    └─Sequential: 3-1                   9,696\n",
      "|    └─DoubleConv: 2-2                        --\n",
      "|    |    └─Sequential: 3-2                   55,680\n",
      "|    └─DoubleConv: 2-3                        --\n",
      "|    |    └─Sequential: 3-3                   221,952\n",
      "|    └─DoubleConv: 2-4                        --\n",
      "|    |    └─Sequential: 3-4                   886,272\n",
      "|    └─DoubleConv: 2-5                        --\n",
      "|    |    └─Sequential: 3-5                   3,542,016\n",
      "├─ModuleList: 1-2                             --\n",
      "|    └─UpSampling: 2-6                        --\n",
      "|    |    └─Sequential: 3-6                   3,277,568\n",
      "|    └─DoubleDeConv: 2-7                      --\n",
      "|    |    └─Sequential: 3-7                   1,771,008\n",
      "|    └─UpSampling: 2-8                        --\n",
      "|    |    └─Sequential: 3-8                   819,584\n",
      "|    └─DoubleDeConv: 2-9                      --\n",
      "|    |    └─Sequential: 3-9                   443,136\n",
      "|    └─UpSampling: 2-10                       --\n",
      "|    |    └─Sequential: 3-10                  204,992\n",
      "|    └─DoubleDeConv: 2-11                     --\n",
      "|    |    └─Sequential: 3-11                  110,976\n",
      "|    └─UpSampling: 2-12                       --\n",
      "|    |    └─Sequential: 3-12                  51,296\n",
      "|    └─DoubleDeConv: 2-13                     --\n",
      "|    |    └─Sequential: 3-13                  27,840\n",
      "├─MaxPool2d: 1-3                              --\n",
      "├─Conv2d: 1-4                                 66\n",
      "======================================================================\n",
      "Total params: 11,422,082\n",
      "Trainable params: 11,422,082\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─ModuleList: 1-1                             --\n",
       "|    └─DoubleConv: 2-1                        --\n",
       "|    |    └─Sequential: 3-1                   9,696\n",
       "|    └─DoubleConv: 2-2                        --\n",
       "|    |    └─Sequential: 3-2                   55,680\n",
       "|    └─DoubleConv: 2-3                        --\n",
       "|    |    └─Sequential: 3-3                   221,952\n",
       "|    └─DoubleConv: 2-4                        --\n",
       "|    |    └─Sequential: 3-4                   886,272\n",
       "|    └─DoubleConv: 2-5                        --\n",
       "|    |    └─Sequential: 3-5                   3,542,016\n",
       "├─ModuleList: 1-2                             --\n",
       "|    └─UpSampling: 2-6                        --\n",
       "|    |    └─Sequential: 3-6                   3,277,568\n",
       "|    └─DoubleDeConv: 2-7                      --\n",
       "|    |    └─Sequential: 3-7                   1,771,008\n",
       "|    └─UpSampling: 2-8                        --\n",
       "|    |    └─Sequential: 3-8                   819,584\n",
       "|    └─DoubleDeConv: 2-9                      --\n",
       "|    |    └─Sequential: 3-9                   443,136\n",
       "|    └─UpSampling: 2-10                       --\n",
       "|    |    └─Sequential: 3-10                  204,992\n",
       "|    └─DoubleDeConv: 2-11                     --\n",
       "|    |    └─Sequential: 3-11                  110,976\n",
       "|    └─UpSampling: 2-12                       --\n",
       "|    |    └─Sequential: 3-12                  51,296\n",
       "|    └─DoubleDeConv: 2-13                     --\n",
       "|    |    └─Sequential: 3-13                  27,840\n",
       "├─MaxPool2d: 1-3                              --\n",
       "├─Conv2d: 1-4                                 66\n",
       "======================================================================\n",
       "Total params: 11,422,082\n",
       "Trainable params: 11,422,082\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = DSDDataset(spectrograms_path=SPECTROGRAMS_PATH, vocal_only=VOCAL_ONLY, train=True)\n",
    "val_set = DSDDataset(spectrograms_path=SPECTROGRAMS_PATH, vocal_only=VOCAL_ONLY, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: torch.Size([8, 1, 1025, 173])\n",
      "\n",
      "Targets: dict_keys(['vocals', 'accompaniment'])\n",
      "\n",
      "Vocal target shape: torch.Size([8, 1, 1025, 173])\n",
      "\n",
      "Accompaniment target shape: torch.Size([8, 1, 1025, 173])\n"
     ]
    }
   ],
   "source": [
    "for feature, target in train_loader:\n",
    "    print(f\"Feature shape: {feature.shape}\\n\")\n",
    "    print(f\"Targets: {target.keys()}\\n\")\n",
    "    print(f\"Vocal target shape: {target['vocals'].shape}\\n\")\n",
    "    print(f\"Accompaniment target shape: {target['accompaniment'].shape}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(model, dataloader, loss_fn, optimizer, device):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "         \n",
    "        feature, target = data\n",
    "\n",
    "        feature = feature.to(device)\n",
    "        target['accompaniment'] = target['accompaniment'].to(device)\n",
    "        target['vocals'] = target['vocals'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(feature)\n",
    "\n",
    "        vocal_channel_output = outputs[:, 0, :, :].unsqueeze(1)\n",
    "        accompaniment_channel_output = outputs[:, 1, :, :].unsqueeze(1)\n",
    "\n",
    "        loss = loss_fn(vocal_channel_output ,target['vocals'],  accompaniment_channel_output ,target['accompaniment'])\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss+=loss.item()\n",
    "\n",
    "        if (i+1)%5==0:\n",
    "         last_loss = running_loss/5\n",
    "         print(f'Batch {i+1},  loss: {last_loss}')\n",
    "         running_loss=0\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_fn, optimizer, device, epochs):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f'EPOCH {epoch+1}:')\n",
    "\n",
    "        model.train(True)\n",
    "        avg_loss = train_one(model, train_loader, loss_fn, optimizer, device)\n",
    "\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                vfeature, vtarget = vdata\n",
    "                vfeature = vfeature.to(device)\n",
    "                vtarget['accompaniment'] = vtarget['accompaniment'].to(device)\n",
    "                vtarget['vocals'] = vtarget['vocals'].to(device)\n",
    "                voutput = model(vfeature)\n",
    "\n",
    "                vocal_channel_output = voutput[:, 0, :, :].unsqueeze(1)\n",
    "                accompaniment_channel_output = voutput[:, 1, :, :].unsqueeze(1)\n",
    "                vloss = loss_fn(vocal_channel_output, vtarget['vocals'], accompaniment_channel_output, vtarget['accompaniment'])\n",
    "                running_val_loss += vloss.item()\n",
    "            avg_vloss = running_val_loss / (i + 1)\n",
    "            print(f'LOSS train {avg_loss}. Validation loss: {avg_vloss} \\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lr=LEARNING_RATE, params=model.parameters())\n",
    "loss_fn = VocalLoss(alpha=VOCAL_ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "Batch 5,  loss: 3.9038992404937742\n",
      "Batch 10,  loss: 3.633120584487915\n",
      "Batch 15,  loss: 3.5168219566345216\n",
      "Batch 20,  loss: 4.338421297073364\n",
      "Batch 25,  loss: 4.140998888015747\n",
      "Batch 30,  loss: 3.906294250488281\n",
      "Batch 35,  loss: 3.830732536315918\n",
      "Batch 40,  loss: 3.8443391799926756\n",
      "Batch 45,  loss: 3.7897018432617187\n",
      "Batch 50,  loss: 3.743812608718872\n",
      "Batch 55,  loss: 3.8130212783813477\n",
      "Batch 60,  loss: 3.274475860595703\n",
      "Batch 65,  loss: 4.14614520072937\n",
      "Batch 70,  loss: 4.723071956634522\n",
      "Batch 75,  loss: 3.003020095825195\n",
      "Batch 80,  loss: 4.229183101654053\n",
      "Batch 85,  loss: 3.394450235366821\n",
      "Batch 90,  loss: 4.240008401870727\n",
      "Batch 95,  loss: 3.442086172103882\n",
      "Batch 100,  loss: 3.189823341369629\n",
      "Batch 105,  loss: 4.46625714302063\n",
      "Batch 110,  loss: 4.261644268035889\n",
      "Batch 115,  loss: 3.829698944091797\n",
      "Batch 120,  loss: 3.0099472999572754\n",
      "Batch 125,  loss: 3.420317029953003\n",
      "Batch 130,  loss: 3.505078363418579\n",
      "Batch 135,  loss: 4.108437633514404\n",
      "Batch 140,  loss: 3.473347473144531\n",
      "Batch 145,  loss: 3.4814338207244875\n",
      "Batch 150,  loss: 3.6633839130401613\n",
      "Batch 155,  loss: 3.683137130737305\n",
      "Batch 160,  loss: 4.043483400344849\n",
      "Batch 165,  loss: 3.642202043533325\n",
      "Batch 170,  loss: 3.8344107627868653\n",
      "Batch 175,  loss: 3.432477521896362\n",
      "Batch 180,  loss: 4.240088891983032\n",
      "Batch 185,  loss: 4.290774536132813\n",
      "Batch 190,  loss: 4.398760223388672\n",
      "Batch 195,  loss: 2.7989267826080324\n",
      "Batch 200,  loss: 3.8120080471038817\n",
      "Batch 205,  loss: 4.183983659744262\n",
      "Batch 210,  loss: 3.715614080429077\n",
      "Batch 215,  loss: 3.594482660293579\n",
      "Batch 220,  loss: 3.4761640071868896\n",
      "Batch 225,  loss: 3.1324025630950927\n",
      "Batch 230,  loss: 4.011910343170166\n",
      "Batch 235,  loss: 3.565698671340942\n",
      "Batch 240,  loss: 3.596330976486206\n",
      "Batch 245,  loss: 3.4279835700988768\n",
      "Batch 250,  loss: 3.784531831741333\n",
      "Batch 255,  loss: 3.6871880531311034\n",
      "Batch 260,  loss: 3.5709073066711428\n",
      "Batch 265,  loss: 3.495109128952026\n",
      "Batch 270,  loss: 3.3950525760650634\n",
      "Batch 275,  loss: 4.095525646209717\n",
      "Batch 280,  loss: 4.753271865844726\n",
      "Batch 285,  loss: 3.4962759971618653\n",
      "Batch 290,  loss: 3.6095338821411134\n",
      "Batch 295,  loss: 3.588922643661499\n",
      "Batch 300,  loss: 3.675079584121704\n",
      "Batch 305,  loss: 3.8241127014160154\n",
      "Batch 310,  loss: 3.3640071868896486\n",
      "Batch 315,  loss: 3.2458950996398928\n",
      "Batch 320,  loss: 3.3897659301757814\n",
      "Batch 325,  loss: 3.190843868255615\n",
      "Batch 330,  loss: 3.5922727584838867\n",
      "Batch 335,  loss: 3.9614917278289794\n",
      "Batch 340,  loss: 4.133727979660034\n",
      "Batch 345,  loss: 3.2899261474609376\n",
      "Batch 350,  loss: 4.143265628814698\n",
      "Batch 355,  loss: 3.9073793411254885\n",
      "Batch 360,  loss: 4.029748439788818\n",
      "Batch 365,  loss: 3.5469094276428224\n",
      "Batch 370,  loss: 3.4875789165496824\n",
      "Batch 375,  loss: 3.4482584953308106\n",
      "Batch 380,  loss: 3.6406118869781494\n",
      "Batch 385,  loss: 3.2216795921325683\n",
      "Batch 390,  loss: 3.831152629852295\n",
      "Batch 395,  loss: 3.8341082096099854\n",
      "Batch 400,  loss: 3.6657762050628664\n",
      "Batch 405,  loss: 3.461622190475464\n",
      "Batch 410,  loss: 3.727361822128296\n",
      "Batch 415,  loss: 4.222571992874146\n",
      "Batch 420,  loss: 3.140455436706543\n",
      "Batch 425,  loss: 4.0666755676269535\n",
      "Batch 430,  loss: 3.5746908664703367\n",
      "Batch 435,  loss: 3.564700698852539\n",
      "Batch 440,  loss: 3.7155188083648683\n",
      "Batch 445,  loss: 2.9548622131347657\n",
      "Batch 450,  loss: 3.4369826316833496\n",
      "Batch 455,  loss: 3.8592687129974363\n",
      "Batch 460,  loss: 3.4673647403717043\n",
      "Batch 465,  loss: 4.1911827564239506\n",
      "Batch 470,  loss: 3.658655309677124\n",
      "Batch 475,  loss: 3.616795015335083\n",
      "Batch 480,  loss: 3.742516350746155\n",
      "Batch 485,  loss: 3.3821125984191895\n",
      "Batch 490,  loss: 4.439652061462402\n",
      "Batch 495,  loss: 4.3999615669250485\n",
      "Batch 500,  loss: 3.4751888275146485\n",
      "Batch 505,  loss: 3.5530241966247558\n",
      "Batch 510,  loss: 4.234341812133789\n",
      "Batch 515,  loss: 3.8366956233978273\n",
      "Batch 520,  loss: 3.257440710067749\n",
      "Batch 525,  loss: 3.6011093616485597\n",
      "Batch 530,  loss: 4.362414741516114\n",
      "Batch 535,  loss: 4.803772258758545\n",
      "Batch 540,  loss: 3.6764123916625975\n",
      "Batch 545,  loss: 4.344115495681763\n",
      "Batch 550,  loss: 3.8135929107666016\n",
      "Batch 555,  loss: 3.3840499401092528\n",
      "Batch 560,  loss: 2.824585199356079\n",
      "Batch 565,  loss: 3.953415632247925\n",
      "Batch 570,  loss: 3.913460206985474\n",
      "Batch 575,  loss: 3.773380661010742\n",
      "Batch 580,  loss: 3.8246052265167236\n",
      "Batch 585,  loss: 3.3298081874847414\n",
      "Batch 590,  loss: 2.984263038635254\n",
      "Batch 595,  loss: 3.259510040283203\n",
      "Batch 600,  loss: 3.6172646045684815\n",
      "Batch 605,  loss: 3.6227786540985107\n",
      "Batch 610,  loss: 3.847710132598877\n",
      "Batch 615,  loss: 3.6620139598846437\n",
      "Batch 620,  loss: 4.4155517578125\n",
      "Batch 625,  loss: 3.8849875926971436\n",
      "Batch 630,  loss: 3.965767002105713\n",
      "Batch 635,  loss: 3.617434024810791\n",
      "Batch 640,  loss: 4.394305276870727\n",
      "Batch 645,  loss: 4.058540773391724\n",
      "Batch 650,  loss: 3.4646435260772703\n",
      "Batch 655,  loss: 3.71132550239563\n",
      "Batch 660,  loss: 3.747825288772583\n",
      "Batch 665,  loss: 3.3300736427307127\n",
      "Batch 670,  loss: 3.7132198333740236\n",
      "Batch 675,  loss: 4.017143249511719\n",
      "Batch 680,  loss: 3.727753257751465\n",
      "Batch 685,  loss: 3.7995601654052735\n",
      "Batch 690,  loss: 3.5279959201812745\n",
      "Batch 695,  loss: 3.4052742958068847\n",
      "Batch 700,  loss: 3.862222194671631\n",
      "Batch 705,  loss: 3.861960744857788\n",
      "Batch 710,  loss: 3.087085723876953\n",
      "Batch 715,  loss: 4.382462978363037\n",
      "Batch 720,  loss: 2.8645181179046633\n",
      "Batch 725,  loss: 3.4614709854125976\n",
      "Batch 730,  loss: 3.703450918197632\n",
      "Batch 735,  loss: 3.9827057838439943\n",
      "Batch 740,  loss: 3.1060757637023926\n",
      "Batch 745,  loss: 3.803391361236572\n",
      "Batch 750,  loss: 4.046149730682373\n",
      "Batch 755,  loss: 2.9287423849105836\n",
      "Batch 760,  loss: 4.3412492036819454\n",
      "Batch 765,  loss: 5.47886848449707\n",
      "Batch 770,  loss: 3.569128131866455\n",
      "Batch 775,  loss: 3.6703081130981445\n",
      "Batch 780,  loss: 2.960799789428711\n",
      "Batch 785,  loss: 3.5746707916259766\n",
      "Batch 790,  loss: 3.498437023162842\n",
      "Batch 795,  loss: 3.4916006565093993\n",
      "Batch 800,  loss: 4.115332889556885\n",
      "Batch 805,  loss: 3.078081464767456\n",
      "Batch 810,  loss: 3.7692902088165283\n",
      "Batch 815,  loss: 3.8079057693481446\n",
      "Batch 820,  loss: 3.3502861976623537\n",
      "Batch 825,  loss: 3.1732939720153808\n",
      "Batch 830,  loss: 3.406056118011475\n",
      "Batch 835,  loss: 3.651120090484619\n",
      "Batch 840,  loss: 3.572741651535034\n",
      "Batch 845,  loss: 3.8583248138427733\n",
      "Batch 850,  loss: 3.0246112823486326\n",
      "Batch 855,  loss: 3.348575210571289\n",
      "Batch 860,  loss: 3.184885025024414\n",
      "Batch 865,  loss: 3.9216372966766357\n",
      "Batch 870,  loss: 3.2649839401245115\n",
      "Batch 875,  loss: 3.5939369201660156\n",
      "Batch 880,  loss: 3.403599166870117\n",
      "Batch 885,  loss: 3.8734643936157225\n",
      "Batch 890,  loss: 3.957162094116211\n",
      "Batch 895,  loss: 3.5647000312805175\n",
      "Batch 900,  loss: 3.7218400478363036\n",
      "Batch 905,  loss: 3.5809422969818114\n",
      "Batch 910,  loss: 3.9621233463287355\n",
      "Batch 915,  loss: 4.245767593383789\n",
      "Batch 920,  loss: 3.875639867782593\n",
      "Batch 925,  loss: 3.453925609588623\n",
      "Batch 930,  loss: 3.2430573463439942\n",
      "Batch 935,  loss: 3.5255470275878906\n",
      "Batch 940,  loss: 3.616062593460083\n",
      "Batch 945,  loss: 3.742495536804199\n",
      "Batch 950,  loss: 2.9023009300231934\n",
      "Batch 955,  loss: 3.5160935878753663\n",
      "Batch 960,  loss: 3.744653415679932\n",
      "Batch 965,  loss: 3.44266562461853\n",
      "Batch 970,  loss: 3.8820852279663085\n",
      "Batch 975,  loss: 3.473279619216919\n",
      "Batch 980,  loss: 3.691068410873413\n",
      "Batch 985,  loss: 3.0635436534881593\n",
      "Batch 990,  loss: 3.6494559764862062\n",
      "Batch 995,  loss: 3.751946210861206\n",
      "Batch 1000,  loss: 3.90372748374939\n",
      "Batch 1005,  loss: 3.584358501434326\n",
      "Batch 1010,  loss: 3.6133336067199706\n",
      "Batch 1015,  loss: 4.117367887496949\n",
      "Batch 1020,  loss: 2.9514315605163572\n",
      "Batch 1025,  loss: 3.6417941093444823\n",
      "Batch 1030,  loss: 3.1785340309143066\n",
      "Batch 1035,  loss: 4.239356517791748\n",
      "Batch 1040,  loss: 3.7278275012969972\n",
      "Batch 1045,  loss: 3.0808152675628664\n",
      "Batch 1050,  loss: 3.3332998752593994\n",
      "Batch 1055,  loss: 2.865609073638916\n",
      "Batch 1060,  loss: 3.6392313480377196\n",
      "Batch 1065,  loss: 3.5690608501434324\n",
      "Batch 1070,  loss: 4.0689338684082035\n",
      "Batch 1075,  loss: 3.5332340240478515\n",
      "Batch 1080,  loss: 3.499402904510498\n",
      "Batch 1085,  loss: 3.4211833000183107\n",
      "Batch 1090,  loss: 3.625780391693115\n",
      "Batch 1095,  loss: 4.035827255249023\n",
      "Batch 1100,  loss: 2.8137093067169188\n",
      "Batch 1105,  loss: 3.103518009185791\n",
      "Batch 1110,  loss: 3.4969183921813967\n",
      "Batch 1115,  loss: 3.747966003417969\n",
      "Batch 1120,  loss: 3.6887741088867188\n",
      "Batch 1125,  loss: 3.7299752235412598\n",
      "Batch 1130,  loss: 3.697701644897461\n",
      "Batch 1135,  loss: 3.8228562831878663\n",
      "Batch 1140,  loss: 3.0812735080718996\n",
      "Batch 1145,  loss: 3.663749122619629\n",
      "Batch 1150,  loss: 3.8600162506103515\n",
      "Batch 1155,  loss: 3.049442100524902\n",
      "Batch 1160,  loss: 3.562129592895508\n",
      "Batch 1165,  loss: 4.019914674758911\n",
      "Batch 1170,  loss: 3.3840609550476075\n",
      "Batch 1175,  loss: 3.7544388294219972\n",
      "Batch 1180,  loss: 4.539353513717652\n",
      "Batch 1185,  loss: 3.7622188568115233\n",
      "Batch 1190,  loss: 3.4451406002044678\n",
      "Batch 1195,  loss: 3.2755974769592284\n",
      "Batch 1200,  loss: 3.488096761703491\n",
      "Batch 1205,  loss: 3.336863613128662\n",
      "Batch 1210,  loss: 3.7883373260498048\n",
      "Batch 1215,  loss: 3.7205778121948243\n",
      "Batch 1220,  loss: 3.457225465774536\n",
      "Batch 1225,  loss: 3.932969331741333\n",
      "Batch 1230,  loss: 3.7644035816192627\n",
      "Batch 1235,  loss: 3.3778555393218994\n",
      "Batch 1240,  loss: 3.800372505187988\n",
      "Batch 1245,  loss: 3.043020820617676\n",
      "Batch 1250,  loss: 4.0076230525970455\n",
      "Batch 1255,  loss: 3.558959150314331\n",
      "Batch 1260,  loss: 3.7320531368255616\n",
      "Batch 1265,  loss: 3.5734580993652343\n",
      "Batch 1270,  loss: 4.008964586257934\n",
      "Batch 1275,  loss: 3.6420217037200926\n",
      "Batch 1280,  loss: 4.052926445007325\n",
      "Batch 1285,  loss: 3.8520291805267335\n",
      "Batch 1290,  loss: 3.644145059585571\n",
      "Batch 1295,  loss: 4.298918533325195\n",
      "LOSS train 4.298918533325195. Validation loss: 4.012860139079944 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 2:\n",
      "Batch 5,  loss: 3.965035390853882\n",
      "Batch 10,  loss: 3.7172311782836913\n",
      "Batch 15,  loss: 3.5945600509643554\n",
      "Batch 20,  loss: 3.6335411071777344\n",
      "Batch 25,  loss: 3.899594211578369\n",
      "Batch 30,  loss: 4.015700149536133\n",
      "Batch 35,  loss: 3.7074890613555906\n",
      "Batch 40,  loss: 3.062484550476074\n",
      "Batch 45,  loss: 3.6636883497238157\n",
      "Batch 50,  loss: 3.1955992698669435\n",
      "Batch 55,  loss: 3.25878963470459\n",
      "Batch 60,  loss: 3.399447727203369\n",
      "Batch 65,  loss: 3.485985851287842\n",
      "Batch 70,  loss: 3.4825150966644287\n",
      "Batch 75,  loss: 3.441678047180176\n",
      "Batch 80,  loss: 4.296514463424683\n",
      "Batch 85,  loss: 3.901566743850708\n",
      "Batch 90,  loss: 4.522095632553101\n",
      "Batch 95,  loss: 4.337727499008179\n",
      "Batch 100,  loss: 3.870680093765259\n",
      "Batch 105,  loss: 3.2376489639282227\n",
      "Batch 110,  loss: 3.48466796875\n",
      "Batch 115,  loss: 3.0464656829833983\n",
      "Batch 120,  loss: 3.216063117980957\n",
      "Batch 125,  loss: 3.491117000579834\n",
      "Batch 130,  loss: 3.7621529579162596\n",
      "Batch 135,  loss: 3.5558635234832763\n",
      "Batch 140,  loss: 3.137396955490112\n",
      "Batch 145,  loss: 3.6347750663757323\n",
      "Batch 150,  loss: 3.2147240161895754\n",
      "Batch 155,  loss: 3.0597978115081785\n",
      "Batch 160,  loss: 3.279937982559204\n",
      "Batch 165,  loss: 3.855871057510376\n",
      "Batch 170,  loss: 3.3724388599395754\n",
      "Batch 175,  loss: 4.045571184158325\n",
      "Batch 180,  loss: 4.136780309677124\n",
      "Batch 185,  loss: 3.4131320476531983\n",
      "Batch 190,  loss: 3.628493070602417\n",
      "Batch 195,  loss: 3.063409996032715\n",
      "Batch 200,  loss: 3.5009068489074706\n",
      "Batch 205,  loss: 4.136827278137207\n",
      "Batch 210,  loss: 4.270589208602905\n",
      "Batch 215,  loss: 3.7940954208374023\n",
      "Batch 220,  loss: 3.3721914768218992\n",
      "Batch 225,  loss: 3.3954458713531492\n",
      "Batch 230,  loss: 2.9720147609710694\n",
      "Batch 235,  loss: 3.470849847793579\n",
      "Batch 240,  loss: 4.140150117874145\n",
      "Batch 245,  loss: 4.0931848049163815\n",
      "Batch 250,  loss: 4.0022196769714355\n",
      "Batch 255,  loss: 3.6805532455444334\n",
      "Batch 260,  loss: 3.4523150444030763\n",
      "Batch 265,  loss: 3.825572299957275\n",
      "Batch 270,  loss: 3.2124495983123778\n",
      "Batch 275,  loss: 3.067117166519165\n",
      "Batch 280,  loss: 3.4842175960540773\n",
      "Batch 285,  loss: 3.352598524093628\n",
      "Batch 290,  loss: 3.9356075286865235\n",
      "Batch 295,  loss: 3.9022402286529543\n",
      "Batch 300,  loss: 3.8237133979797364\n",
      "Batch 305,  loss: 3.9155651569366454\n",
      "Batch 310,  loss: 5.42599048614502\n",
      "Batch 315,  loss: 3.3315972805023195\n",
      "Batch 320,  loss: 3.2762358665466307\n",
      "Batch 325,  loss: 2.8367241859436034\n",
      "Batch 330,  loss: 3.3770878314971924\n",
      "Batch 335,  loss: 3.4924251556396486\n",
      "Batch 340,  loss: 3.4924088954925536\n",
      "Batch 345,  loss: 3.561336326599121\n",
      "Batch 350,  loss: 3.423636865615845\n",
      "Batch 355,  loss: 4.154163265228272\n",
      "Batch 360,  loss: 3.365158796310425\n",
      "Batch 365,  loss: 3.04037823677063\n",
      "Batch 370,  loss: 4.289378213882446\n",
      "Batch 375,  loss: 3.2512269973754884\n",
      "Batch 380,  loss: 4.158080196380615\n",
      "Batch 385,  loss: 2.8720218181610107\n",
      "Batch 390,  loss: 3.7577653408050535\n",
      "Batch 395,  loss: 4.055922317504883\n",
      "Batch 400,  loss: 3.5655261039733888\n",
      "Batch 405,  loss: 3.883923864364624\n",
      "Batch 410,  loss: 3.809714365005493\n",
      "Batch 415,  loss: 3.4736349105834963\n",
      "Batch 420,  loss: 3.124623441696167\n",
      "Batch 425,  loss: 4.090284919738769\n",
      "Batch 430,  loss: 3.5462684631347656\n",
      "Batch 435,  loss: 4.084696054458618\n",
      "Batch 440,  loss: 4.987031745910644\n",
      "Batch 445,  loss: 3.9587018489837646\n",
      "Batch 450,  loss: 3.523685646057129\n",
      "Batch 455,  loss: 4.321807765960694\n",
      "Batch 460,  loss: 4.275237798690796\n",
      "Batch 465,  loss: 3.625844669342041\n",
      "Batch 470,  loss: 3.5829864978790282\n",
      "Batch 475,  loss: 3.116329526901245\n",
      "Batch 480,  loss: 3.261187124252319\n",
      "Batch 485,  loss: 3.612001419067383\n",
      "Batch 490,  loss: 4.215355491638183\n",
      "Batch 495,  loss: 3.3607017517089846\n",
      "Batch 500,  loss: 3.4366857528686525\n",
      "Batch 505,  loss: 3.3728609561920164\n",
      "Batch 510,  loss: 3.2478450775146483\n",
      "Batch 515,  loss: 3.7376770973205566\n",
      "Batch 520,  loss: 4.0782544136047365\n",
      "Batch 525,  loss: 3.8247079372406008\n",
      "Batch 530,  loss: 4.089589834213257\n",
      "Batch 535,  loss: 3.8216755867004393\n",
      "Batch 540,  loss: 3.13833692073822\n",
      "Batch 545,  loss: 3.2046737909317016\n",
      "Batch 550,  loss: 3.2036507606506346\n",
      "Batch 555,  loss: 3.7536237239837646\n",
      "Batch 560,  loss: 3.7141780853271484\n",
      "Batch 565,  loss: 3.6831790447235107\n",
      "Batch 570,  loss: 4.462315273284912\n",
      "Batch 575,  loss: 3.4218464851379395\n",
      "Batch 580,  loss: 3.6076603889465333\n",
      "Batch 585,  loss: 3.122167229652405\n",
      "Batch 590,  loss: 3.0793951511383058\n",
      "Batch 595,  loss: 3.526283359527588\n",
      "Batch 600,  loss: 3.1479812145233153\n",
      "Batch 605,  loss: 3.861415672302246\n",
      "Batch 610,  loss: 3.9807909965515136\n",
      "Batch 615,  loss: 4.327011680603027\n",
      "Batch 620,  loss: 3.6373878002166746\n",
      "Batch 625,  loss: 3.7788548946380613\n",
      "Batch 630,  loss: 3.68905086517334\n",
      "Batch 635,  loss: 3.2685741424560546\n",
      "Batch 640,  loss: 4.250298643112183\n",
      "Batch 645,  loss: 3.7441452026367186\n",
      "Batch 650,  loss: 3.7927809238433836\n",
      "Batch 655,  loss: 3.1325960874557497\n",
      "Batch 660,  loss: 3.692083311080933\n",
      "Batch 665,  loss: 3.980458879470825\n",
      "Batch 670,  loss: 3.0820637226104735\n",
      "Batch 675,  loss: 4.0598752975463865\n",
      "Batch 680,  loss: 3.428205728530884\n",
      "Batch 685,  loss: 3.3000797748565676\n",
      "Batch 690,  loss: 3.3610442161560057\n",
      "Batch 695,  loss: 3.2664278030395506\n",
      "Batch 700,  loss: 3.829250383377075\n",
      "Batch 705,  loss: 3.356729030609131\n",
      "Batch 710,  loss: 3.504621410369873\n",
      "Batch 715,  loss: 3.5626023769378663\n",
      "Batch 720,  loss: 3.5602442264556884\n",
      "Batch 725,  loss: 3.700907850265503\n",
      "Batch 730,  loss: 3.9935779094696047\n",
      "Batch 735,  loss: 3.8858567237854005\n",
      "Batch 740,  loss: 3.9190446853637697\n",
      "Batch 745,  loss: 3.3975814819335937\n",
      "Batch 750,  loss: 3.2696152687072755\n",
      "Batch 755,  loss: 3.121577787399292\n",
      "Batch 760,  loss: 3.839885187149048\n",
      "Batch 765,  loss: 3.5762380599975585\n",
      "Batch 770,  loss: 3.1851449012756348\n",
      "Batch 775,  loss: 3.1127804279327393\n",
      "Batch 780,  loss: 3.173966646194458\n",
      "Batch 785,  loss: 3.646751070022583\n",
      "Batch 790,  loss: 3.0716198921203612\n",
      "Batch 795,  loss: 3.496881055831909\n",
      "Batch 800,  loss: 3.471575212478638\n",
      "Batch 805,  loss: 3.203239154815674\n",
      "Batch 810,  loss: 3.8777762413024903\n",
      "Batch 815,  loss: 3.448150157928467\n",
      "Batch 820,  loss: 3.152804899215698\n",
      "Batch 825,  loss: 3.1924369812011717\n",
      "Batch 830,  loss: 3.462027740478516\n",
      "Batch 835,  loss: 3.688357925415039\n",
      "Batch 840,  loss: 3.189988613128662\n",
      "Batch 845,  loss: 3.0928247451782225\n",
      "Batch 850,  loss: 3.434685230255127\n",
      "Batch 855,  loss: 3.1460296630859377\n",
      "Batch 860,  loss: 4.246777009963989\n",
      "Batch 865,  loss: 3.6588671684265135\n",
      "Batch 870,  loss: 3.243282842636108\n",
      "Batch 875,  loss: 3.805599308013916\n",
      "Batch 880,  loss: 3.0291287422180178\n",
      "Batch 885,  loss: 4.121823406219482\n",
      "Batch 890,  loss: 3.5044507026672362\n",
      "Batch 895,  loss: 3.9711684703826906\n",
      "Batch 900,  loss: 3.21500563621521\n",
      "Batch 905,  loss: 3.2261157035827637\n",
      "Batch 910,  loss: 3.70849027633667\n",
      "Batch 915,  loss: 4.3804315567016605\n",
      "Batch 920,  loss: 3.1668018817901613\n",
      "Batch 925,  loss: 3.4703946113586426\n",
      "Batch 930,  loss: 3.4160558700561525\n",
      "Batch 935,  loss: 3.205501079559326\n",
      "Batch 940,  loss: 3.9137621402740477\n",
      "Batch 945,  loss: 4.100597524642945\n",
      "Batch 950,  loss: 4.109839296340942\n",
      "Batch 955,  loss: 3.577659273147583\n",
      "Batch 960,  loss: 3.2370559692382814\n",
      "Batch 965,  loss: 3.1797993183135986\n",
      "Batch 970,  loss: 3.4837623596191407\n",
      "Batch 975,  loss: 2.8983717441558836\n",
      "Batch 980,  loss: 3.28906626701355\n",
      "Batch 985,  loss: 3.161953258514404\n",
      "Batch 990,  loss: 2.6112178802490233\n",
      "Batch 995,  loss: 3.7687214851379394\n",
      "Batch 1000,  loss: 3.8250123977661135\n",
      "Batch 1005,  loss: 4.036586284637451\n",
      "Batch 1010,  loss: 3.764395332336426\n",
      "Batch 1015,  loss: 3.7045170307159423\n",
      "Batch 1020,  loss: 3.4517669677734375\n",
      "Batch 1025,  loss: 2.9369534969329836\n",
      "Batch 1030,  loss: 3.7669692993164063\n",
      "Batch 1035,  loss: 3.644957399368286\n",
      "Batch 1040,  loss: 3.71669340133667\n",
      "Batch 1045,  loss: 3.8993192672729493\n",
      "Batch 1050,  loss: 3.5816534996032714\n",
      "Batch 1055,  loss: 3.043576383590698\n",
      "Batch 1060,  loss: 3.1497488021850586\n",
      "Batch 1065,  loss: 3.600483751296997\n",
      "Batch 1070,  loss: 3.1115713596343992\n",
      "Batch 1075,  loss: 2.9913583755493165\n",
      "Batch 1080,  loss: 3.406831455230713\n",
      "Batch 1085,  loss: 3.6254961013793947\n",
      "Batch 1090,  loss: 3.143578624725342\n",
      "Batch 1095,  loss: 3.5594863414764406\n",
      "Batch 1100,  loss: 3.387545919418335\n",
      "Batch 1105,  loss: 3.1853336811065676\n",
      "Batch 1110,  loss: 3.719191789627075\n",
      "Batch 1115,  loss: 3.4999687910079955\n",
      "Batch 1120,  loss: 3.4337223052978514\n",
      "Batch 1125,  loss: 3.5938625574111938\n",
      "Batch 1130,  loss: 3.3421703338623048\n",
      "Batch 1135,  loss: 2.9218636035919188\n",
      "Batch 1140,  loss: 3.8688321113586426\n",
      "Batch 1145,  loss: 4.185659265518188\n",
      "Batch 1150,  loss: 3.507215642929077\n",
      "Batch 1155,  loss: 3.4735020637512206\n",
      "Batch 1160,  loss: 3.129809761047363\n",
      "Batch 1165,  loss: 3.1955074310302733\n",
      "Batch 1170,  loss: 3.555921125411987\n",
      "Batch 1175,  loss: 3.7921236991882323\n",
      "Batch 1180,  loss: 2.998327398300171\n",
      "Batch 1185,  loss: 3.8920719623565674\n",
      "Batch 1190,  loss: 3.870764970779419\n",
      "Batch 1195,  loss: 4.836696624755859\n",
      "Batch 1200,  loss: 4.1277995109558105\n",
      "Batch 1205,  loss: 2.7814833402633665\n",
      "Batch 1210,  loss: 3.766812801361084\n",
      "Batch 1215,  loss: 3.6355698108673096\n",
      "Batch 1220,  loss: 3.4839125633239747\n",
      "Batch 1225,  loss: 2.6453096866607666\n",
      "Batch 1230,  loss: 3.784494638442993\n",
      "Batch 1235,  loss: 4.150185680389404\n",
      "Batch 1240,  loss: 3.258406972885132\n",
      "Batch 1245,  loss: 2.614741563796997\n",
      "Batch 1250,  loss: 3.127386522293091\n",
      "Batch 1255,  loss: 3.22321195602417\n",
      "Batch 1260,  loss: 3.189327573776245\n",
      "Batch 1265,  loss: 3.4672314167022704\n",
      "Batch 1270,  loss: 3.315087175369263\n",
      "Batch 1275,  loss: 3.518082523345947\n",
      "Batch 1280,  loss: 3.140738296508789\n",
      "Batch 1285,  loss: 4.174123954772949\n",
      "Batch 1290,  loss: 3.295822286605835\n",
      "Batch 1295,  loss: 3.5014771938323976\n",
      "LOSS train 3.5014771938323976. Validation loss: 3.918316538066224 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 3:\n",
      "Batch 5,  loss: 3.8283299446105956\n",
      "Batch 10,  loss: 2.55200731754303\n",
      "Batch 15,  loss: 3.7177501201629637\n",
      "Batch 20,  loss: 3.9014678955078126\n",
      "Batch 25,  loss: 3.5264170169830322\n",
      "Batch 30,  loss: 3.151171636581421\n",
      "Batch 35,  loss: 3.6324435234069825\n",
      "Batch 40,  loss: 3.927047109603882\n",
      "Batch 45,  loss: 3.4495164155960083\n",
      "Batch 50,  loss: 2.585518503189087\n",
      "Batch 55,  loss: 3.864934873580933\n",
      "Batch 60,  loss: 2.8528268337249756\n",
      "Batch 65,  loss: 2.7209115505218504\n",
      "Batch 70,  loss: 2.78782377243042\n",
      "Batch 75,  loss: 3.6694279670715333\n",
      "Batch 80,  loss: 3.4844691276550295\n",
      "Batch 85,  loss: 3.8364264011383056\n",
      "Batch 90,  loss: 3.7603726387023926\n",
      "Batch 95,  loss: 3.688629674911499\n",
      "Batch 100,  loss: 3.8820651531219483\n",
      "Batch 105,  loss: 3.221479320526123\n",
      "Batch 110,  loss: 3.7289822578430174\n",
      "Batch 115,  loss: 3.6613378524780273\n",
      "Batch 120,  loss: 3.851080083847046\n",
      "Batch 125,  loss: 3.579173755645752\n",
      "Batch 130,  loss: 3.45585560798645\n",
      "Batch 135,  loss: 3.312054491043091\n",
      "Batch 140,  loss: 3.758914566040039\n",
      "Batch 145,  loss: 3.714361333847046\n",
      "Batch 150,  loss: 3.372269058227539\n",
      "Batch 155,  loss: 3.8481653690338136\n",
      "Batch 160,  loss: 3.695066976547241\n",
      "Batch 165,  loss: 3.6428518533706664\n",
      "Batch 170,  loss: 3.31020131111145\n",
      "Batch 175,  loss: 3.3876297950744627\n",
      "Batch 180,  loss: 3.6907975673675537\n",
      "Batch 185,  loss: 4.172700071334839\n",
      "Batch 190,  loss: 4.046872758865357\n",
      "Batch 195,  loss: 3.1375829696655275\n",
      "Batch 200,  loss: 3.60889310836792\n",
      "Batch 205,  loss: 3.182395505905151\n",
      "Batch 210,  loss: 3.444624185562134\n",
      "Batch 215,  loss: 3.644995021820068\n",
      "Batch 220,  loss: 3.158878517150879\n",
      "Batch 225,  loss: 3.4002990245819094\n",
      "Batch 230,  loss: 3.3844584465026855\n",
      "Batch 235,  loss: 3.717879819869995\n",
      "Batch 240,  loss: 4.153730487823486\n",
      "Batch 245,  loss: 3.879508876800537\n",
      "Batch 250,  loss: 3.060118246078491\n",
      "Batch 255,  loss: 3.221655797958374\n",
      "Batch 260,  loss: 3.4335905790328978\n",
      "Batch 265,  loss: 3.1755325317382814\n",
      "Batch 270,  loss: 3.787656879425049\n",
      "Batch 275,  loss: 3.2534995555877684\n",
      "Batch 280,  loss: 2.713346242904663\n",
      "Batch 285,  loss: 4.509919023513794\n",
      "Batch 290,  loss: 3.499656057357788\n",
      "Batch 295,  loss: 4.114468336105347\n",
      "Batch 300,  loss: 3.34126501083374\n",
      "Batch 305,  loss: 3.749335527420044\n",
      "Batch 310,  loss: 3.3123770236968992\n",
      "Batch 315,  loss: 3.3765828132629396\n",
      "Batch 320,  loss: 2.9196386337280273\n",
      "Batch 325,  loss: 3.616404914855957\n",
      "Batch 330,  loss: 3.3495271682739256\n",
      "Batch 335,  loss: 3.245921993255615\n",
      "Batch 340,  loss: 3.8253490924835205\n",
      "Batch 345,  loss: 3.1185724258422853\n",
      "Batch 350,  loss: 4.002550888061523\n",
      "Batch 355,  loss: 2.887328052520752\n",
      "Batch 360,  loss: 3.625073480606079\n",
      "Batch 365,  loss: 3.72633581161499\n",
      "Batch 370,  loss: 4.214076328277588\n",
      "Batch 375,  loss: 2.5073126316070558\n",
      "Batch 380,  loss: 3.2934027671813966\n",
      "Batch 385,  loss: 3.592409944534302\n",
      "Batch 390,  loss: 2.9973892688751222\n",
      "Batch 395,  loss: 3.717643117904663\n",
      "Batch 400,  loss: 3.663076066970825\n",
      "Batch 405,  loss: 3.6553361415863037\n",
      "Batch 410,  loss: 3.274348592758179\n",
      "Batch 415,  loss: 3.0912548065185548\n",
      "Batch 420,  loss: 3.370511507987976\n",
      "Batch 425,  loss: 4.049330139160157\n",
      "Batch 430,  loss: 3.553346347808838\n",
      "Batch 435,  loss: 3.1528728008270264\n",
      "Batch 440,  loss: 3.708977460861206\n",
      "Batch 445,  loss: 3.270298624038696\n",
      "Batch 450,  loss: 3.720121431350708\n",
      "Batch 455,  loss: 3.3107645988464354\n",
      "Batch 460,  loss: 4.131198501586914\n",
      "Batch 465,  loss: 3.284590244293213\n",
      "Batch 470,  loss: 3.3909807205200195\n",
      "Batch 475,  loss: 3.487607002258301\n",
      "Batch 480,  loss: 2.811763572692871\n",
      "Batch 485,  loss: 4.055489730834961\n",
      "Batch 490,  loss: 3.4662992477416994\n",
      "Batch 495,  loss: 3.861936664581299\n",
      "Batch 500,  loss: 3.7071506500244142\n",
      "Batch 505,  loss: 2.9287667751312254\n",
      "Batch 510,  loss: 3.136546993255615\n",
      "Batch 515,  loss: 3.422307777404785\n",
      "Batch 520,  loss: 3.831215476989746\n",
      "Batch 525,  loss: 3.457263946533203\n",
      "Batch 530,  loss: 3.62987961769104\n",
      "Batch 535,  loss: 3.3306907176971436\n",
      "Batch 540,  loss: 3.5662933826446532\n",
      "Batch 545,  loss: 3.4438055515289308\n",
      "Batch 550,  loss: 3.27011661529541\n",
      "Batch 555,  loss: 2.971289587020874\n",
      "Batch 560,  loss: 3.3219632625579836\n",
      "Batch 565,  loss: 3.483595275878906\n",
      "Batch 570,  loss: 3.81434280872345\n",
      "Batch 575,  loss: 3.2227451324462892\n",
      "Batch 580,  loss: 4.085060358047485\n",
      "Batch 585,  loss: 3.245686149597168\n",
      "Batch 590,  loss: 3.822457027435303\n",
      "Batch 595,  loss: 2.9882022380828857\n",
      "Batch 600,  loss: 4.003953838348389\n",
      "Batch 605,  loss: 3.1153960704803465\n",
      "Batch 610,  loss: 3.7226532459259034\n",
      "Batch 615,  loss: 3.449874258041382\n",
      "Batch 620,  loss: 3.8054012298583983\n",
      "Batch 625,  loss: 3.6665335655212403\n",
      "Batch 630,  loss: 3.3454092502593995\n",
      "Batch 635,  loss: 3.635718011856079\n",
      "Batch 640,  loss: 3.2622624397277833\n",
      "Batch 645,  loss: 3.6845781326293947\n",
      "Batch 650,  loss: 3.307678985595703\n",
      "Batch 655,  loss: 3.2908349990844727\n",
      "Batch 660,  loss: 3.3548718452453614\n",
      "Batch 665,  loss: 4.456720209121704\n",
      "Batch 670,  loss: 3.4926982879638673\n",
      "Batch 675,  loss: 3.4025530338287355\n",
      "Batch 680,  loss: 4.352410984039307\n",
      "Batch 685,  loss: 2.907538652420044\n",
      "Batch 690,  loss: 3.418247103691101\n",
      "Batch 695,  loss: 4.141729307174683\n",
      "Batch 700,  loss: 3.496595525741577\n",
      "Batch 705,  loss: 3.5692944526672363\n",
      "Batch 710,  loss: 3.5279541969299317\n",
      "Batch 715,  loss: 3.790748357772827\n",
      "Batch 720,  loss: 3.219467830657959\n",
      "Batch 725,  loss: 3.3966323852539064\n",
      "Batch 730,  loss: 3.703563022613525\n",
      "Batch 735,  loss: 2.99402060508728\n",
      "Batch 740,  loss: 3.533201313018799\n",
      "Batch 745,  loss: 4.080127239227295\n",
      "Batch 750,  loss: 3.2848418712615968\n",
      "Batch 755,  loss: 3.2753273487091064\n",
      "Batch 760,  loss: 3.2522090911865233\n",
      "Batch 765,  loss: 3.0980152606964113\n",
      "Batch 770,  loss: 3.35148401260376\n",
      "Batch 775,  loss: 3.68009557723999\n",
      "Batch 780,  loss: 3.293494939804077\n",
      "Batch 785,  loss: 4.436888980865478\n",
      "Batch 790,  loss: 3.013883924484253\n",
      "Batch 795,  loss: 3.058093309402466\n",
      "Batch 800,  loss: 3.001329469680786\n",
      "Batch 805,  loss: 3.6791082859039306\n",
      "Batch 810,  loss: 3.242910289764404\n",
      "Batch 815,  loss: 3.385199546813965\n",
      "Batch 820,  loss: 3.343001365661621\n",
      "Batch 825,  loss: 3.967598056793213\n",
      "Batch 830,  loss: 2.993863582611084\n",
      "Batch 835,  loss: 3.9265764236450194\n",
      "Batch 840,  loss: 3.1829500675201414\n",
      "Batch 845,  loss: 3.4372249603271485\n",
      "Batch 850,  loss: 3.7685495376586915\n",
      "Batch 855,  loss: 3.554017925262451\n",
      "Batch 860,  loss: 3.0980412483215334\n",
      "Batch 865,  loss: 3.537130117416382\n",
      "Batch 870,  loss: 3.43928804397583\n",
      "Batch 875,  loss: 3.311193752288818\n",
      "Batch 880,  loss: 3.758942222595215\n",
      "Batch 885,  loss: 3.8185051918029784\n",
      "Batch 890,  loss: 4.513597202301026\n",
      "Batch 895,  loss: 3.105828046798706\n",
      "Batch 900,  loss: 3.694280910491943\n",
      "Batch 905,  loss: 3.5632359981536865\n",
      "Batch 910,  loss: 3.730411958694458\n",
      "Batch 915,  loss: 3.5292834281921386\n",
      "Batch 920,  loss: 3.66480131149292\n",
      "Batch 925,  loss: 3.2490110874176024\n",
      "Batch 930,  loss: 3.5507843494415283\n",
      "Batch 935,  loss: 3.3588961124420167\n",
      "Batch 940,  loss: 3.979461145401001\n",
      "Batch 945,  loss: 3.366055154800415\n",
      "Batch 950,  loss: 3.5632858753204344\n",
      "Batch 955,  loss: 2.843179941177368\n",
      "Batch 960,  loss: 3.1433122634887694\n",
      "Batch 965,  loss: 3.4001856327056883\n",
      "Batch 970,  loss: 3.1077703475952148\n",
      "Batch 975,  loss: 3.799526882171631\n",
      "Batch 980,  loss: 2.562615728378296\n",
      "Batch 985,  loss: 4.146182346343994\n",
      "Batch 990,  loss: 2.84621319770813\n",
      "Batch 995,  loss: 3.2766811847686768\n",
      "Batch 1000,  loss: 3.4658836841583254\n",
      "Batch 1005,  loss: 3.9784790515899657\n",
      "Batch 1010,  loss: 3.439946413040161\n",
      "Batch 1015,  loss: 3.8484167575836183\n",
      "Batch 1020,  loss: 3.1886780738830565\n",
      "Batch 1025,  loss: 3.5383981227874757\n",
      "Batch 1030,  loss: 2.58134446144104\n",
      "Batch 1035,  loss: 3.5750909805297852\n",
      "Batch 1040,  loss: 3.588965082168579\n",
      "Batch 1045,  loss: 3.454416036605835\n",
      "Batch 1050,  loss: 3.37666335105896\n",
      "Batch 1055,  loss: 3.9968303203582765\n",
      "Batch 1060,  loss: 3.6425857543945312\n",
      "Batch 1065,  loss: 3.2498252391815186\n",
      "Batch 1070,  loss: 3.0469462394714357\n",
      "Batch 1075,  loss: 3.5248250484466555\n",
      "Batch 1080,  loss: 3.49892373085022\n",
      "Batch 1085,  loss: 2.9102396965026855\n",
      "Batch 1090,  loss: 3.475570058822632\n",
      "Batch 1095,  loss: 2.957048511505127\n",
      "Batch 1100,  loss: 3.4384297847747805\n",
      "Batch 1105,  loss: 3.2280688762664793\n",
      "Batch 1110,  loss: 4.151927375793457\n",
      "Batch 1115,  loss: 3.0565258026123048\n",
      "Batch 1120,  loss: 3.463781642913818\n",
      "Batch 1125,  loss: 3.4977643489837646\n",
      "Batch 1130,  loss: 3.2542332649230956\n",
      "Batch 1135,  loss: 3.253675603866577\n",
      "Batch 1140,  loss: 3.470503234863281\n",
      "Batch 1145,  loss: 3.574556827545166\n",
      "Batch 1150,  loss: 4.0181480884552006\n",
      "Batch 1155,  loss: 3.764834928512573\n",
      "Batch 1160,  loss: 3.0871973991394044\n",
      "Batch 1165,  loss: 2.8198972225189207\n",
      "Batch 1170,  loss: 3.1284512996673586\n",
      "Batch 1175,  loss: 3.250730848312378\n",
      "Batch 1180,  loss: 3.455534267425537\n",
      "Batch 1185,  loss: 3.7025238037109376\n",
      "Batch 1190,  loss: 3.553687858581543\n",
      "Batch 1195,  loss: 3.6312429428100588\n",
      "Batch 1200,  loss: 4.059209966659546\n",
      "Batch 1205,  loss: 3.169462060928345\n",
      "Batch 1210,  loss: 3.205913209915161\n",
      "Batch 1215,  loss: 3.4110524654388428\n",
      "Batch 1220,  loss: 2.7558377742767335\n",
      "Batch 1225,  loss: 3.546789026260376\n",
      "Batch 1230,  loss: 2.7619683742523193\n",
      "Batch 1235,  loss: 3.38298978805542\n",
      "Batch 1240,  loss: 3.340786504745483\n",
      "Batch 1245,  loss: 3.491180753707886\n",
      "Batch 1250,  loss: 3.2092534065246583\n",
      "Batch 1255,  loss: 3.967173957824707\n",
      "Batch 1260,  loss: 2.4189024686813356\n",
      "Batch 1265,  loss: 3.6672229766845703\n",
      "Batch 1270,  loss: 3.608534049987793\n",
      "Batch 1275,  loss: 2.6112730503082275\n",
      "Batch 1280,  loss: 3.5051409721374513\n",
      "Batch 1285,  loss: 3.0887362480163576\n",
      "Batch 1290,  loss: 3.9900341033935547\n",
      "Batch 1295,  loss: 3.5657272338867188\n",
      "LOSS train 3.5657272338867188. Validation loss: 3.794293250704046 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 4:\n",
      "Batch 5,  loss: 3.4305710792541504\n",
      "Batch 10,  loss: 3.184218740463257\n",
      "Batch 15,  loss: 2.86600604057312\n",
      "Batch 20,  loss: 3.4373499870300295\n",
      "Batch 25,  loss: 3.385778021812439\n",
      "Batch 30,  loss: 3.4087119102478027\n",
      "Batch 35,  loss: 3.7422104358673094\n",
      "Batch 40,  loss: 2.94933819770813\n",
      "Batch 45,  loss: 3.8322095394134523\n",
      "Batch 50,  loss: 3.411869525909424\n",
      "Batch 55,  loss: 3.4806164741516112\n",
      "Batch 60,  loss: 3.176497220993042\n",
      "Batch 65,  loss: 2.7886499166488647\n",
      "Batch 70,  loss: 3.643962097167969\n",
      "Batch 75,  loss: 3.6280911922454835\n",
      "Batch 80,  loss: 3.4057774543762207\n",
      "Batch 85,  loss: 3.7701854705810547\n",
      "Batch 90,  loss: 3.5820613861083985\n",
      "Batch 95,  loss: 3.570516490936279\n",
      "Batch 100,  loss: 3.3607396364212034\n",
      "Batch 105,  loss: 3.1539169788360595\n",
      "Batch 110,  loss: 2.8234172821044923\n",
      "Batch 115,  loss: 3.100635290145874\n",
      "Batch 120,  loss: 2.995194172859192\n",
      "Batch 125,  loss: 3.7284677982330323\n",
      "Batch 130,  loss: 3.1258684635162353\n",
      "Batch 135,  loss: 3.2209221363067626\n",
      "Batch 140,  loss: 4.057911062240601\n",
      "Batch 145,  loss: 3.394271230697632\n",
      "Batch 150,  loss: 2.957344961166382\n",
      "Batch 155,  loss: 3.0266561985015867\n",
      "Batch 160,  loss: 3.3923881530761717\n",
      "Batch 165,  loss: 3.2456406116485597\n",
      "Batch 170,  loss: 4.449045181274414\n",
      "Batch 175,  loss: 3.8348833084106446\n",
      "Batch 180,  loss: 4.654224681854248\n",
      "Batch 185,  loss: 2.7372264862060547\n",
      "Batch 190,  loss: 3.170728397369385\n",
      "Batch 195,  loss: 3.349108839035034\n",
      "Batch 200,  loss: 3.5287263870239256\n",
      "Batch 205,  loss: 2.9006394863128664\n",
      "Batch 210,  loss: 3.076695203781128\n",
      "Batch 215,  loss: 4.276893091201782\n",
      "Batch 220,  loss: 3.582638454437256\n",
      "Batch 225,  loss: 3.8588537216186523\n",
      "Batch 230,  loss: 2.6094271183013915\n",
      "Batch 235,  loss: 2.6878183126449584\n",
      "Batch 240,  loss: 3.8221900939941404\n",
      "Batch 245,  loss: 3.090826082229614\n",
      "Batch 250,  loss: 4.424558115005493\n",
      "Batch 255,  loss: 3.064436435699463\n",
      "Batch 260,  loss: 3.9347564220428466\n",
      "Batch 265,  loss: 2.995003080368042\n",
      "Batch 270,  loss: 3.3648664474487306\n",
      "Batch 275,  loss: 4.3795318603515625\n",
      "Batch 280,  loss: 3.5592941761016847\n",
      "Batch 285,  loss: 3.42161111831665\n",
      "Batch 290,  loss: 3.520012950897217\n",
      "Batch 295,  loss: 3.8285074710845945\n",
      "Batch 300,  loss: 3.4932257652282717\n",
      "Batch 305,  loss: 3.9958722591400146\n",
      "Batch 310,  loss: 3.512825918197632\n",
      "Batch 315,  loss: 3.1040859699249266\n",
      "Batch 320,  loss: 3.44214448928833\n",
      "Batch 325,  loss: 3.2053024768829346\n",
      "Batch 330,  loss: 3.1559080600738527\n",
      "Batch 335,  loss: 3.32677845954895\n",
      "Batch 340,  loss: 3.52163348197937\n",
      "Batch 345,  loss: 3.558890771865845\n",
      "Batch 350,  loss: 3.71872239112854\n",
      "Batch 355,  loss: 3.5440596103668214\n",
      "Batch 360,  loss: 3.4532421588897706\n",
      "Batch 365,  loss: 4.253906488418579\n",
      "Batch 370,  loss: 3.941441202163696\n",
      "Batch 375,  loss: 3.9267712593078614\n",
      "Batch 380,  loss: 3.8836569786071777\n",
      "Batch 385,  loss: 3.6495409965515138\n",
      "Batch 390,  loss: 3.09656023979187\n",
      "Batch 395,  loss: 3.6869810581207276\n",
      "Batch 400,  loss: 2.966953420639038\n",
      "Batch 405,  loss: 3.4473695755004883\n",
      "Batch 410,  loss: 3.733116292953491\n",
      "Batch 415,  loss: 2.777485179901123\n",
      "Batch 420,  loss: 3.4606833457946777\n",
      "Batch 425,  loss: 3.1399622917175294\n",
      "Batch 430,  loss: 2.832017683982849\n",
      "Batch 435,  loss: 3.1520935535430907\n",
      "Batch 440,  loss: 3.5486541748046876\n",
      "Batch 445,  loss: 3.6871838092803957\n",
      "Batch 450,  loss: 2.766440916061401\n",
      "Batch 455,  loss: 2.76691312789917\n",
      "Batch 460,  loss: 2.938114643096924\n",
      "Batch 465,  loss: 3.3236669063568116\n",
      "Batch 470,  loss: 4.077662181854248\n",
      "Batch 475,  loss: 3.1213377475738526\n",
      "Batch 480,  loss: 3.0877894878387453\n",
      "Batch 485,  loss: 3.8634706020355223\n",
      "Batch 490,  loss: 4.115920543670654\n",
      "Batch 495,  loss: 2.897828245162964\n",
      "Batch 500,  loss: 3.4008339405059815\n",
      "Batch 505,  loss: 3.7116745471954347\n",
      "Batch 510,  loss: 3.157627058029175\n",
      "Batch 515,  loss: 3.806169557571411\n",
      "Batch 520,  loss: 2.78015251159668\n",
      "Batch 525,  loss: 3.752989339828491\n",
      "Batch 530,  loss: 3.8848621368408205\n",
      "Batch 535,  loss: 1.9824889183044434\n",
      "Batch 540,  loss: 2.943816328048706\n",
      "Batch 545,  loss: 3.6810272216796873\n",
      "Batch 550,  loss: 2.555355095863342\n",
      "Batch 555,  loss: 3.778011417388916\n",
      "Batch 560,  loss: 3.651053619384766\n",
      "Batch 565,  loss: 3.1870221138000487\n",
      "Batch 570,  loss: 4.442411184310913\n",
      "Batch 575,  loss: 3.0770684719085692\n",
      "Batch 580,  loss: 2.8781785011291503\n",
      "Batch 585,  loss: 3.530463171005249\n",
      "Batch 590,  loss: 2.9523495197296143\n",
      "Batch 595,  loss: 3.9617921829223635\n",
      "Batch 600,  loss: 2.979557180404663\n",
      "Batch 605,  loss: 2.922558069229126\n",
      "Batch 610,  loss: 4.135624885559082\n",
      "Batch 615,  loss: 3.470058870315552\n",
      "Batch 620,  loss: 3.1753679752349853\n",
      "Batch 625,  loss: 2.954699993133545\n",
      "Batch 630,  loss: 3.2707850456237795\n",
      "Batch 635,  loss: 2.7828861951828\n",
      "Batch 640,  loss: 3.628623533248901\n",
      "Batch 645,  loss: 3.614516830444336\n",
      "Batch 650,  loss: 3.340056800842285\n",
      "Batch 655,  loss: 2.4589157342910766\n",
      "Batch 660,  loss: 3.8342856407165526\n",
      "Batch 665,  loss: 3.5533963680267333\n",
      "Batch 670,  loss: 3.3884023666381835\n",
      "Batch 675,  loss: 3.3090939998626707\n",
      "Batch 680,  loss: 3.406094551086426\n",
      "Batch 685,  loss: 3.3140555381774903\n",
      "Batch 690,  loss: 2.728080940246582\n",
      "Batch 695,  loss: 3.2871753215789794\n",
      "Batch 700,  loss: 3.533897590637207\n",
      "Batch 705,  loss: 4.012455940246582\n",
      "Batch 710,  loss: 3.020068645477295\n",
      "Batch 715,  loss: 3.395479941368103\n",
      "Batch 720,  loss: 2.9930028438568117\n",
      "Batch 725,  loss: 3.1814624309539794\n",
      "Batch 730,  loss: 3.2691662311553955\n",
      "Batch 735,  loss: 4.004577398300171\n",
      "Batch 740,  loss: 3.081715202331543\n",
      "Batch 745,  loss: 2.4125508785247805\n",
      "Batch 750,  loss: 3.5081470012664795\n",
      "Batch 755,  loss: 3.255141353607178\n",
      "Batch 760,  loss: 3.565032720565796\n",
      "Batch 765,  loss: 3.7986279010772703\n",
      "Batch 770,  loss: 3.1181852340698244\n",
      "Batch 775,  loss: 3.067604827880859\n",
      "Batch 780,  loss: 3.377998399734497\n",
      "Batch 785,  loss: 3.756076955795288\n",
      "Batch 790,  loss: 2.6413143634796143\n",
      "Batch 795,  loss: 3.9431171894073485\n",
      "Batch 800,  loss: 3.2237032413482667\n",
      "Batch 805,  loss: 3.2261542797088625\n",
      "Batch 810,  loss: 2.974251389503479\n",
      "Batch 815,  loss: 3.0451056003570556\n",
      "Batch 820,  loss: 3.6724990367889405\n",
      "Batch 825,  loss: 2.918784666061401\n",
      "Batch 830,  loss: 3.663352918624878\n",
      "Batch 835,  loss: 2.87880802154541\n",
      "Batch 840,  loss: 3.076555013656616\n",
      "Batch 845,  loss: 3.2884021282196043\n",
      "Batch 850,  loss: 2.822356128692627\n",
      "Batch 855,  loss: 3.273703956604004\n",
      "Batch 860,  loss: 3.010196495056152\n",
      "Batch 865,  loss: 2.845373582839966\n",
      "Batch 870,  loss: 3.3054454803466795\n",
      "Batch 875,  loss: 3.2501586198806764\n",
      "Batch 880,  loss: 3.5500222206115724\n",
      "Batch 885,  loss: 3.29022274017334\n",
      "Batch 890,  loss: 3.658285903930664\n",
      "Batch 895,  loss: 2.835951232910156\n",
      "Batch 900,  loss: 2.9736709117889406\n",
      "Batch 905,  loss: 3.941107749938965\n",
      "Batch 910,  loss: 3.1346431732177735\n",
      "Batch 915,  loss: 3.4795963764190674\n",
      "Batch 920,  loss: 3.4010757923126222\n",
      "Batch 925,  loss: 3.646833801269531\n",
      "Batch 930,  loss: 2.8496299743652345\n",
      "Batch 935,  loss: 3.3573209762573244\n",
      "Batch 940,  loss: 2.79091420173645\n",
      "Batch 945,  loss: 3.2755048274993896\n",
      "Batch 950,  loss: 3.8119935989379883\n",
      "Batch 955,  loss: 3.180048131942749\n",
      "Batch 960,  loss: 2.802543830871582\n",
      "Batch 965,  loss: 3.4286354064941404\n",
      "Batch 970,  loss: 3.4102384567260744\n",
      "Batch 975,  loss: 3.3864051342010497\n",
      "Batch 980,  loss: 3.8551440715789793\n",
      "Batch 985,  loss: 3.0554962158203125\n",
      "Batch 990,  loss: 3.740013861656189\n",
      "Batch 995,  loss: 4.08169207572937\n",
      "Batch 1000,  loss: 3.4357675075531007\n",
      "Batch 1005,  loss: 3.733400344848633\n",
      "Batch 1010,  loss: 2.688847041130066\n",
      "Batch 1015,  loss: 3.222530889511108\n",
      "Batch 1020,  loss: 3.432345485687256\n",
      "Batch 1025,  loss: 3.6950494289398192\n",
      "Batch 1030,  loss: 3.4269840240478517\n",
      "Batch 1035,  loss: 3.386805534362793\n",
      "Batch 1040,  loss: 2.670075273513794\n",
      "Batch 1045,  loss: 4.058691596984863\n",
      "Batch 1050,  loss: 3.064944696426392\n",
      "Batch 1055,  loss: 2.932084560394287\n",
      "Batch 1060,  loss: 4.061568689346314\n",
      "Batch 1065,  loss: 3.5809043407440186\n",
      "Batch 1070,  loss: 3.3204926013946534\n",
      "Batch 1075,  loss: 2.8814884662628173\n",
      "Batch 1080,  loss: 3.18542423248291\n",
      "Batch 1085,  loss: 2.925258827209473\n",
      "Batch 1090,  loss: 2.919901466369629\n",
      "Batch 1095,  loss: 2.4613341331481933\n",
      "Batch 1100,  loss: 2.5985423803329466\n",
      "Batch 1105,  loss: 3.2171247959136964\n",
      "Batch 1110,  loss: 4.480191421508789\n",
      "Batch 1115,  loss: 2.9514761447906492\n",
      "Batch 1120,  loss: 3.363234615325928\n",
      "Batch 1125,  loss: 3.0775206089019775\n",
      "Batch 1130,  loss: 2.642149543762207\n",
      "Batch 1135,  loss: 3.41477952003479\n",
      "Batch 1140,  loss: 3.074912738800049\n",
      "Batch 1145,  loss: 3.528711223602295\n",
      "Batch 1150,  loss: 3.440890741348267\n",
      "Batch 1155,  loss: 3.4567710876464846\n",
      "Batch 1160,  loss: 3.4392278671264647\n",
      "Batch 1165,  loss: 2.949458456039429\n",
      "Batch 1170,  loss: 3.537601900100708\n",
      "Batch 1175,  loss: 3.2692153453826904\n",
      "Batch 1180,  loss: 3.976101016998291\n",
      "Batch 1185,  loss: 2.809713101387024\n",
      "Batch 1190,  loss: 2.756603813171387\n",
      "Batch 1195,  loss: 3.6140819549560548\n",
      "Batch 1200,  loss: 3.298883080482483\n",
      "Batch 1205,  loss: 3.711281490325928\n",
      "Batch 1210,  loss: 3.05364933013916\n",
      "Batch 1215,  loss: 3.3225163221359253\n",
      "Batch 1220,  loss: 3.4720504760742186\n",
      "Batch 1225,  loss: 3.9478659629821777\n",
      "Batch 1230,  loss: 2.5962507724761963\n",
      "Batch 1235,  loss: 2.7840076684951782\n",
      "Batch 1240,  loss: 3.6885618686676027\n",
      "Batch 1245,  loss: 3.122533750534058\n",
      "Batch 1250,  loss: 3.077501821517944\n",
      "Batch 1255,  loss: 3.4410021781921385\n",
      "Batch 1260,  loss: 3.2392501831054688\n",
      "Batch 1265,  loss: 3.421590232849121\n",
      "Batch 1270,  loss: 4.1313080310821535\n",
      "Batch 1275,  loss: 3.56259822845459\n",
      "Batch 1280,  loss: 2.9353095054626466\n",
      "Batch 1285,  loss: 3.2358625411987303\n",
      "Batch 1290,  loss: 4.43702278137207\n",
      "Batch 1295,  loss: 3.889981508255005\n",
      "LOSS train 3.889981508255005. Validation loss: 3.8861312148195726 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 5:\n",
      "Batch 5,  loss: 3.073394536972046\n",
      "Batch 10,  loss: 3.33412880897522\n",
      "Batch 15,  loss: 3.5692708492279053\n",
      "Batch 20,  loss: 3.1998557090759276\n",
      "Batch 25,  loss: 2.9105711936950684\n",
      "Batch 30,  loss: 3.7095520973205565\n",
      "Batch 35,  loss: 3.759415626525879\n",
      "Batch 40,  loss: 3.103377914428711\n",
      "Batch 45,  loss: 3.0842662334442137\n",
      "Batch 50,  loss: 4.15455322265625\n",
      "Batch 55,  loss: 3.3288991928100584\n",
      "Batch 60,  loss: 3.0340609550476074\n",
      "Batch 65,  loss: 3.243521976470947\n",
      "Batch 70,  loss: 3.546342134475708\n",
      "Batch 75,  loss: 3.099262571334839\n",
      "Batch 80,  loss: 2.7576250076293944\n",
      "Batch 85,  loss: 2.9595013856887817\n",
      "Batch 90,  loss: 2.5764497995376585\n",
      "Batch 95,  loss: 3.1844645977020263\n",
      "Batch 100,  loss: 3.815335178375244\n",
      "Batch 105,  loss: 3.6823336601257326\n",
      "Batch 110,  loss: 3.074674105644226\n",
      "Batch 115,  loss: 3.6075193881988525\n",
      "Batch 120,  loss: 3.616090774536133\n",
      "Batch 125,  loss: 2.9711575269699098\n",
      "Batch 130,  loss: 3.4295256614685057\n",
      "Batch 135,  loss: 3.7002205848693848\n",
      "Batch 140,  loss: 2.9110528945922853\n",
      "Batch 145,  loss: 2.6031635284423826\n",
      "Batch 150,  loss: 4.150549983978271\n",
      "Batch 155,  loss: 3.093239450454712\n",
      "Batch 160,  loss: 3.025629758834839\n",
      "Batch 165,  loss: 3.7671189308166504\n",
      "Batch 170,  loss: 2.930822658538818\n",
      "Batch 175,  loss: 3.477879762649536\n",
      "Batch 180,  loss: 3.475514221191406\n",
      "Batch 185,  loss: 2.864998769760132\n",
      "Batch 190,  loss: 3.0264230251312254\n",
      "Batch 195,  loss: 3.953987216949463\n",
      "Batch 200,  loss: 3.0910717487335204\n",
      "Batch 205,  loss: 3.4663137435913085\n",
      "Batch 210,  loss: 2.9077750205993653\n",
      "Batch 215,  loss: 3.1842960834503176\n",
      "Batch 220,  loss: 3.2472692012786863\n",
      "Batch 225,  loss: 3.5265790939331056\n",
      "Batch 230,  loss: 3.7447524547576903\n",
      "Batch 235,  loss: 3.696369171142578\n",
      "Batch 240,  loss: 4.030326223373413\n",
      "Batch 245,  loss: 3.549132537841797\n",
      "Batch 250,  loss: 3.226227617263794\n",
      "Batch 255,  loss: 3.316883659362793\n",
      "Batch 260,  loss: 3.2777875900268554\n",
      "Batch 265,  loss: 3.6469537258148192\n",
      "Batch 270,  loss: 2.9122058629989622\n",
      "Batch 275,  loss: 3.021008110046387\n",
      "Batch 280,  loss: 4.021374654769898\n",
      "Batch 285,  loss: 3.3188883304595946\n",
      "Batch 290,  loss: 3.380802869796753\n",
      "Batch 295,  loss: 3.547448205947876\n",
      "Batch 300,  loss: 3.1389369487762453\n",
      "Batch 305,  loss: 3.415422058105469\n",
      "Batch 310,  loss: 3.0186811447143556\n",
      "Batch 315,  loss: 3.633303499221802\n",
      "Batch 320,  loss: 3.545732259750366\n",
      "Batch 325,  loss: 3.4789990901947023\n",
      "Batch 330,  loss: 3.5066630840301514\n",
      "Batch 335,  loss: 3.456205415725708\n",
      "Batch 340,  loss: 2.849328804016113\n",
      "Batch 345,  loss: 3.7462995529174803\n",
      "Batch 350,  loss: 2.767422008514404\n",
      "Batch 355,  loss: 3.886672306060791\n",
      "Batch 360,  loss: 3.3834033489227293\n",
      "Batch 365,  loss: 2.7259942531585692\n",
      "Batch 370,  loss: 2.7350687980651855\n",
      "Batch 375,  loss: 3.2255080699920655\n",
      "Batch 380,  loss: 2.9002986907958985\n",
      "Batch 385,  loss: 3.297759246826172\n",
      "Batch 390,  loss: 2.977960801124573\n",
      "Batch 395,  loss: 3.0029821395874023\n",
      "Batch 400,  loss: 3.198451352119446\n",
      "Batch 405,  loss: 3.6279822826385497\n",
      "Batch 410,  loss: 2.884230041503906\n",
      "Batch 415,  loss: 3.7179781436920165\n",
      "Batch 420,  loss: 3.524930143356323\n",
      "Batch 425,  loss: 3.126098966598511\n",
      "Batch 430,  loss: 3.1908578395843508\n",
      "Batch 435,  loss: 2.6171660900115965\n",
      "Batch 440,  loss: 3.7432412624359133\n",
      "Batch 445,  loss: 2.99577260017395\n",
      "Batch 450,  loss: 2.998306131362915\n",
      "Batch 455,  loss: 2.5553725242614744\n",
      "Batch 460,  loss: 3.377299499511719\n",
      "Batch 465,  loss: 3.9039728164672853\n",
      "Batch 470,  loss: 3.1198455810546877\n",
      "Batch 475,  loss: 2.724487543106079\n",
      "Batch 480,  loss: 3.5519761562347414\n",
      "Batch 485,  loss: 3.6124050617218018\n",
      "Batch 490,  loss: 3.4096370220184324\n",
      "Batch 495,  loss: 3.289020872116089\n",
      "Batch 500,  loss: 3.96440749168396\n",
      "Batch 505,  loss: 3.1789153099060057\n",
      "Batch 510,  loss: 3.2084706306457518\n",
      "Batch 515,  loss: 2.5925528526306154\n",
      "Batch 520,  loss: 3.0953393459320067\n",
      "Batch 525,  loss: 3.5253358840942384\n",
      "Batch 530,  loss: 3.3626593589782714\n",
      "Batch 535,  loss: 3.1523619174957274\n",
      "Batch 540,  loss: 3.1610196113586424\n",
      "Batch 545,  loss: 2.5510605812072753\n",
      "Batch 550,  loss: 3.3069682121276855\n",
      "Batch 555,  loss: 3.6496171951293945\n",
      "Batch 560,  loss: 3.230925703048706\n",
      "Batch 565,  loss: 3.4660033226013183\n",
      "Batch 570,  loss: 3.442899703979492\n",
      "Batch 575,  loss: 3.089389371871948\n",
      "Batch 580,  loss: 3.0545063495635985\n",
      "Batch 585,  loss: 3.0116252422332765\n",
      "Batch 590,  loss: 3.3560916900634767\n",
      "Batch 595,  loss: 3.1815499782562258\n",
      "Batch 600,  loss: 2.760928916931152\n",
      "Batch 605,  loss: 3.188599681854248\n",
      "Batch 610,  loss: 2.7253607749938964\n",
      "Batch 615,  loss: 3.42732572555542\n",
      "Batch 620,  loss: 2.9862646102905273\n",
      "Batch 625,  loss: 3.4802178382873534\n",
      "Batch 630,  loss: 3.3055850982666017\n",
      "Batch 635,  loss: 3.158562517166138\n",
      "Batch 640,  loss: 2.8382517814636232\n",
      "Batch 645,  loss: 2.8840911865234373\n",
      "Batch 650,  loss: 3.268973445892334\n",
      "Batch 655,  loss: 2.643941879272461\n",
      "Batch 660,  loss: 2.817254400253296\n",
      "Batch 665,  loss: 3.43939414024353\n",
      "Batch 670,  loss: 3.385644817352295\n",
      "Batch 675,  loss: 2.6712501525878904\n",
      "Batch 680,  loss: 3.1022990703582765\n",
      "Batch 685,  loss: 3.086966896057129\n",
      "Batch 690,  loss: 3.63828387260437\n",
      "Batch 695,  loss: 3.5280399322509766\n",
      "Batch 700,  loss: 2.523349976539612\n",
      "Batch 705,  loss: 3.222868251800537\n",
      "Batch 710,  loss: 3.0527138233184816\n",
      "Batch 715,  loss: 3.15372953414917\n",
      "Batch 720,  loss: 3.141750383377075\n",
      "Batch 725,  loss: 2.696659469604492\n",
      "Batch 730,  loss: 3.115555453300476\n",
      "Batch 735,  loss: 2.952101469039917\n",
      "Batch 740,  loss: 3.279767465591431\n",
      "Batch 745,  loss: 2.998359966278076\n",
      "Batch 750,  loss: 3.147520732879639\n",
      "Batch 755,  loss: 2.7697149991989134\n",
      "Batch 760,  loss: 2.53406982421875\n",
      "Batch 765,  loss: 2.7596328020095826\n",
      "Batch 770,  loss: 3.2499062061309814\n",
      "Batch 775,  loss: 3.1791385412216187\n",
      "Batch 780,  loss: 3.69092173576355\n",
      "Batch 785,  loss: 2.452384352684021\n",
      "Batch 790,  loss: 3.5182369709014893\n",
      "Batch 795,  loss: 3.2970768451690673\n",
      "Batch 800,  loss: 3.856353282928467\n",
      "Batch 805,  loss: 3.036754035949707\n",
      "Batch 810,  loss: 3.2897297859191896\n",
      "Batch 815,  loss: 2.781164026260376\n",
      "Batch 820,  loss: 2.9806838989257813\n",
      "Batch 825,  loss: 2.9518276691436767\n",
      "Batch 830,  loss: 3.200277256965637\n",
      "Batch 835,  loss: 3.3880496263504027\n",
      "Batch 840,  loss: 2.942474937438965\n",
      "Batch 845,  loss: 3.931848096847534\n",
      "Batch 850,  loss: 3.1489341259002686\n",
      "Batch 855,  loss: 3.5026547431945803\n",
      "Batch 860,  loss: 3.709756088256836\n",
      "Batch 865,  loss: 3.1556708335876467\n",
      "Batch 870,  loss: 3.9401625394821167\n",
      "Batch 875,  loss: 2.972783327102661\n",
      "Batch 880,  loss: 3.076435995101929\n",
      "Batch 885,  loss: 3.071104145050049\n",
      "Batch 890,  loss: 3.308783674240112\n",
      "Batch 895,  loss: 2.456973838806152\n",
      "Batch 900,  loss: 3.287424421310425\n",
      "Batch 905,  loss: 3.3838422775268553\n",
      "Batch 910,  loss: 3.0954652786254884\n",
      "Batch 915,  loss: 3.7673737049102782\n",
      "Batch 920,  loss: 3.5207715034484863\n",
      "Batch 925,  loss: 3.4142918586730957\n",
      "Batch 930,  loss: 4.5492720127105715\n",
      "Batch 935,  loss: 3.5432390213012694\n",
      "Batch 940,  loss: 3.423749399185181\n",
      "Batch 945,  loss: 3.688290596008301\n",
      "Batch 950,  loss: 3.249587869644165\n",
      "Batch 955,  loss: 3.981636571884155\n",
      "Batch 960,  loss: 3.2639309406280517\n",
      "Batch 965,  loss: 3.254071569442749\n",
      "Batch 970,  loss: 3.0196724414825438\n",
      "Batch 975,  loss: 3.517482042312622\n",
      "Batch 980,  loss: 3.112886381149292\n",
      "Batch 985,  loss: 3.320593500137329\n",
      "Batch 990,  loss: 2.842504930496216\n",
      "Batch 995,  loss: 3.2056044578552245\n",
      "Batch 1000,  loss: 3.0132306575775147\n",
      "Batch 1005,  loss: 3.1428294658660887\n",
      "Batch 1010,  loss: 2.9327347755432127\n",
      "Batch 1015,  loss: 3.22081618309021\n",
      "Batch 1020,  loss: 3.2757261753082276\n",
      "Batch 1025,  loss: 2.6821547985076903\n",
      "Batch 1030,  loss: 3.6270273685455323\n",
      "Batch 1035,  loss: 3.023262882232666\n",
      "Batch 1040,  loss: 3.5242810249328613\n",
      "Batch 1045,  loss: 2.9611886024475096\n",
      "Batch 1050,  loss: 3.1302292346954346\n",
      "Batch 1055,  loss: 3.1991928577423097\n",
      "Batch 1060,  loss: 3.3487935066223145\n",
      "Batch 1065,  loss: 2.8204504013061524\n",
      "Batch 1070,  loss: 3.8571706771850587\n",
      "Batch 1075,  loss: 3.1950013637542725\n",
      "Batch 1080,  loss: 2.937878894805908\n",
      "Batch 1085,  loss: 3.488685703277588\n",
      "Batch 1090,  loss: 3.7467435359954835\n",
      "Batch 1095,  loss: 3.111906909942627\n",
      "Batch 1100,  loss: 2.9336117267608643\n",
      "Batch 1105,  loss: 3.2009705543518066\n",
      "Batch 1110,  loss: 3.3559806823730467\n",
      "Batch 1115,  loss: 3.0780616760253907\n",
      "Batch 1120,  loss: 3.385965871810913\n",
      "Batch 1125,  loss: 3.5625062942504884\n",
      "Batch 1130,  loss: 3.514419174194336\n",
      "Batch 1135,  loss: 3.3022463798522947\n",
      "Batch 1140,  loss: 2.8856369733810423\n",
      "Batch 1145,  loss: 3.114537811279297\n",
      "Batch 1150,  loss: 3.61137318611145\n",
      "Batch 1155,  loss: 3.0591994285583497\n",
      "Batch 1160,  loss: 2.9990699768066404\n",
      "Batch 1165,  loss: 3.177217483520508\n",
      "Batch 1170,  loss: 2.5660439252853395\n",
      "Batch 1175,  loss: 2.7664578914642335\n",
      "Batch 1180,  loss: 3.5777071475982667\n",
      "Batch 1185,  loss: 3.5060534477233887\n",
      "Batch 1190,  loss: 3.6854531288146974\n",
      "Batch 1195,  loss: 2.541542387008667\n",
      "Batch 1200,  loss: 3.0467778205871583\n",
      "Batch 1205,  loss: 3.509874773025513\n",
      "Batch 1210,  loss: 3.085837459564209\n",
      "Batch 1215,  loss: 3.717288303375244\n",
      "Batch 1220,  loss: 3.3225894927978517\n",
      "Batch 1225,  loss: 3.369065856933594\n",
      "Batch 1230,  loss: 3.6596161842346193\n",
      "Batch 1235,  loss: 3.0187352657318116\n",
      "Batch 1240,  loss: 2.5253733158111573\n",
      "Batch 1245,  loss: 3.284597873687744\n",
      "Batch 1250,  loss: 2.8290693283081056\n",
      "Batch 1255,  loss: 3.02845139503479\n",
      "Batch 1260,  loss: 3.009792995452881\n",
      "Batch 1265,  loss: 3.5546519279479982\n",
      "Batch 1270,  loss: 3.6999834060668944\n",
      "Batch 1275,  loss: 3.0198060035705567\n",
      "Batch 1280,  loss: 2.8096478939056397\n",
      "Batch 1285,  loss: 3.482378530502319\n",
      "Batch 1290,  loss: 3.028816342353821\n",
      "Batch 1295,  loss: 2.99982271194458\n",
      "LOSS train 2.99982271194458. Validation loss: 3.7186423343640786 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 6:\n",
      "Batch 5,  loss: 2.660079574584961\n",
      "Batch 10,  loss: 2.9517733812332154\n",
      "Batch 15,  loss: 3.8131524085998536\n",
      "Batch 20,  loss: 3.521139144897461\n",
      "Batch 25,  loss: 3.4459637641906737\n",
      "Batch 30,  loss: 2.695987606048584\n",
      "Batch 35,  loss: 3.623943042755127\n",
      "Batch 40,  loss: 2.578025197982788\n",
      "Batch 45,  loss: 3.645129346847534\n",
      "Batch 50,  loss: 3.00238356590271\n",
      "Batch 55,  loss: 2.882885694503784\n",
      "Batch 60,  loss: 2.9089887142181396\n",
      "Batch 65,  loss: 3.4505555629730225\n",
      "Batch 70,  loss: 2.26239070892334\n",
      "Batch 75,  loss: 2.9082308530807497\n",
      "Batch 80,  loss: 2.830235815048218\n",
      "Batch 85,  loss: 3.302418518066406\n",
      "Batch 90,  loss: 3.0518298149108887\n",
      "Batch 95,  loss: 3.1841843843460085\n",
      "Batch 100,  loss: 2.3834254503250123\n",
      "Batch 105,  loss: 3.2589470386505126\n",
      "Batch 110,  loss: 3.0934214115142824\n",
      "Batch 115,  loss: 3.4937178611755373\n",
      "Batch 120,  loss: 3.4094276428222656\n",
      "Batch 125,  loss: 3.111960935592651\n",
      "Batch 130,  loss: 3.125726509094238\n",
      "Batch 135,  loss: 3.9383290290832518\n",
      "Batch 140,  loss: 2.434804606437683\n",
      "Batch 145,  loss: 3.60259690284729\n",
      "Batch 150,  loss: 3.4405121564865113\n",
      "Batch 155,  loss: 2.832974100112915\n",
      "Batch 160,  loss: 2.598824143409729\n",
      "Batch 165,  loss: 3.613320302963257\n",
      "Batch 170,  loss: 2.745974826812744\n",
      "Batch 175,  loss: 2.851099061965942\n",
      "Batch 180,  loss: 3.0822922229766845\n",
      "Batch 185,  loss: 3.100233030319214\n",
      "Batch 190,  loss: 3.9323949813842773\n",
      "Batch 195,  loss: 3.034875202178955\n",
      "Batch 200,  loss: 3.1344491243362427\n",
      "Batch 205,  loss: 2.9788421154022218\n",
      "Batch 210,  loss: 2.6531922817230225\n",
      "Batch 215,  loss: 3.2409384727478026\n",
      "Batch 220,  loss: 2.510842490196228\n",
      "Batch 225,  loss: 2.9951076745986938\n",
      "Batch 230,  loss: 2.8257890701293946\n",
      "Batch 235,  loss: 3.758781147003174\n",
      "Batch 240,  loss: 3.6693530082702637\n",
      "Batch 245,  loss: 2.8636218070983888\n",
      "Batch 250,  loss: 2.854103946685791\n",
      "Batch 255,  loss: 3.673727798461914\n",
      "Batch 260,  loss: 3.913690614700317\n",
      "Batch 265,  loss: 3.0201165199279787\n",
      "Batch 270,  loss: 3.186780834197998\n",
      "Batch 275,  loss: 2.8920446395874024\n",
      "Batch 280,  loss: 3.2508713722229006\n",
      "Batch 285,  loss: 3.230478811264038\n",
      "Batch 290,  loss: 2.890318202972412\n",
      "Batch 295,  loss: 3.393361139297485\n",
      "Batch 300,  loss: 3.366094398498535\n",
      "Batch 305,  loss: 2.946599316596985\n",
      "Batch 310,  loss: 3.102401328086853\n",
      "Batch 315,  loss: 2.743329668045044\n",
      "Batch 320,  loss: 2.8639283657073973\n",
      "Batch 325,  loss: 2.667188882827759\n",
      "Batch 330,  loss: 3.2756351470947265\n",
      "Batch 335,  loss: 3.1256606578826904\n",
      "Batch 340,  loss: 3.0955221176147463\n",
      "Batch 345,  loss: 2.756560909748077\n",
      "Batch 350,  loss: 3.31414532661438\n",
      "Batch 355,  loss: 2.8590138435363768\n",
      "Batch 360,  loss: 3.19008321762085\n",
      "Batch 365,  loss: 3.0536992073059084\n",
      "Batch 370,  loss: 3.1684386253356935\n",
      "Batch 375,  loss: 3.7129696369171143\n",
      "Batch 380,  loss: 3.0764232158660887\n",
      "Batch 385,  loss: 2.791361999511719\n",
      "Batch 390,  loss: 2.4462632656097414\n",
      "Batch 395,  loss: 2.6286993503570555\n",
      "Batch 400,  loss: 2.9251255989074707\n",
      "Batch 405,  loss: 4.003858041763306\n",
      "Batch 410,  loss: 3.0773014307022093\n",
      "Batch 415,  loss: 3.2140808582305906\n",
      "Batch 420,  loss: 2.7644328117370605\n",
      "Batch 425,  loss: 3.149530601501465\n",
      "Batch 430,  loss: 3.376991891860962\n",
      "Batch 435,  loss: 2.940549397468567\n",
      "Batch 440,  loss: 3.4415546894073485\n",
      "Batch 445,  loss: 2.7386566162109376\n",
      "Batch 450,  loss: 3.1102572441101075\n",
      "Batch 455,  loss: 2.6860074520111086\n",
      "Batch 460,  loss: 3.670465326309204\n",
      "Batch 465,  loss: 3.1360105514526366\n",
      "Batch 470,  loss: 2.987196350097656\n",
      "Batch 475,  loss: 2.9787121295928953\n",
      "Batch 480,  loss: 3.0480615139007567\n",
      "Batch 485,  loss: 2.8625508546829224\n",
      "Batch 490,  loss: 2.7439153671264647\n",
      "Batch 495,  loss: 3.1345335960388185\n",
      "Batch 500,  loss: 3.591063642501831\n",
      "Batch 505,  loss: 3.1569634914398192\n",
      "Batch 510,  loss: 3.409510087966919\n",
      "Batch 515,  loss: 2.9731149196624758\n",
      "Batch 520,  loss: 2.8802602291107178\n",
      "Batch 525,  loss: 3.153110647201538\n",
      "Batch 530,  loss: 3.102694606781006\n",
      "Batch 535,  loss: 2.9831780910491945\n",
      "Batch 540,  loss: 3.829102087020874\n",
      "Batch 545,  loss: 3.04149010181427\n",
      "Batch 550,  loss: 2.785134744644165\n",
      "Batch 555,  loss: 2.3934750080108644\n",
      "Batch 560,  loss: 3.8908196449279786\n",
      "Batch 565,  loss: 3.6220768451690675\n",
      "Batch 570,  loss: 2.9677561283111573\n",
      "Batch 575,  loss: 3.1958069801330566\n",
      "Batch 580,  loss: 3.0865699291229247\n",
      "Batch 585,  loss: 2.7722261428833006\n",
      "Batch 590,  loss: 3.6840536117553713\n",
      "Batch 595,  loss: 3.0625070571899413\n",
      "Batch 600,  loss: 2.9273416996002197\n",
      "Batch 605,  loss: 3.471725082397461\n",
      "Batch 610,  loss: 3.770340824127197\n",
      "Batch 615,  loss: 2.7415427923202516\n",
      "Batch 620,  loss: 2.566673231124878\n",
      "Batch 625,  loss: 3.5829250812530518\n",
      "Batch 630,  loss: 3.3023064613342283\n",
      "Batch 635,  loss: 3.1453043460845946\n",
      "Batch 640,  loss: 3.3163096427917482\n",
      "Batch 645,  loss: 3.8722638130187987\n",
      "Batch 650,  loss: 3.046462631225586\n",
      "Batch 655,  loss: 3.7924307346343995\n",
      "Batch 660,  loss: 3.3272799491882323\n",
      "Batch 665,  loss: 2.4767222166061402\n",
      "Batch 670,  loss: 2.8273253440856934\n",
      "Batch 675,  loss: 3.0370638847351072\n",
      "Batch 680,  loss: 2.958996391296387\n",
      "Batch 685,  loss: 2.650110626220703\n",
      "Batch 690,  loss: 3.162719202041626\n",
      "Batch 695,  loss: 3.2156317234039307\n",
      "Batch 700,  loss: 2.7251648187637327\n",
      "Batch 705,  loss: 3.213919973373413\n",
      "Batch 710,  loss: 2.841063642501831\n",
      "Batch 715,  loss: 2.545007038116455\n",
      "Batch 720,  loss: 3.799835538864136\n",
      "Batch 725,  loss: 3.8142780303955077\n",
      "Batch 730,  loss: 3.2121036529541014\n",
      "Batch 735,  loss: 3.0682857990264893\n",
      "Batch 740,  loss: 3.5763689041137696\n",
      "Batch 745,  loss: 3.4827218532562254\n",
      "Batch 750,  loss: 3.8069056034088136\n",
      "Batch 755,  loss: 3.875076675415039\n",
      "Batch 760,  loss: 3.0509817600250244\n",
      "Batch 765,  loss: 3.1180124282836914\n",
      "Batch 770,  loss: 2.7493975162506104\n",
      "Batch 775,  loss: 2.961192417144775\n",
      "Batch 780,  loss: 3.207800340652466\n",
      "Batch 785,  loss: 3.2211127281188965\n",
      "Batch 790,  loss: 3.2717000007629395\n",
      "Batch 795,  loss: 3.590541219711304\n",
      "Batch 800,  loss: 2.887584018707275\n",
      "Batch 805,  loss: 3.0506197929382326\n",
      "Batch 810,  loss: 3.332852363586426\n",
      "Batch 815,  loss: 3.5453704833984374\n",
      "Batch 820,  loss: 2.923653745651245\n",
      "Batch 825,  loss: 3.133302927017212\n",
      "Batch 830,  loss: 2.7600151538848876\n",
      "Batch 835,  loss: 3.5222361087799072\n",
      "Batch 840,  loss: 3.3898634910583496\n",
      "Batch 845,  loss: 3.2124589920043944\n",
      "Batch 850,  loss: 3.105993127822876\n",
      "Batch 855,  loss: 3.3122543811798097\n",
      "Batch 860,  loss: 3.7442885398864747\n",
      "Batch 865,  loss: 3.1216299057006838\n",
      "Batch 870,  loss: 3.045997428894043\n",
      "Batch 875,  loss: 3.2818650722503664\n",
      "Batch 880,  loss: 3.076533842086792\n",
      "Batch 885,  loss: 3.218508243560791\n",
      "Batch 890,  loss: 3.059375\n",
      "Batch 895,  loss: 2.8365020990371703\n",
      "Batch 900,  loss: 2.6677029848098757\n",
      "Batch 905,  loss: 2.178395223617554\n",
      "Batch 910,  loss: 2.6049270629882812\n",
      "Batch 915,  loss: 2.5387542247772217\n",
      "Batch 920,  loss: 3.174887466430664\n",
      "Batch 925,  loss: 2.88597993850708\n",
      "Batch 930,  loss: 3.0791807174682617\n",
      "Batch 935,  loss: 2.6673559665679933\n",
      "Batch 940,  loss: 2.9689737796783446\n",
      "Batch 945,  loss: 3.406318759918213\n",
      "Batch 950,  loss: 2.875947093963623\n",
      "Batch 955,  loss: 2.8404726505279543\n",
      "Batch 960,  loss: 2.797804522514343\n",
      "Batch 965,  loss: 2.6687843561172486\n",
      "Batch 970,  loss: 3.461019682884216\n",
      "Batch 975,  loss: 2.4023158311843873\n",
      "Batch 980,  loss: 2.7829684257507323\n",
      "Batch 985,  loss: 2.9053086280822753\n",
      "Batch 990,  loss: 3.5687609672546388\n",
      "Batch 995,  loss: 2.728896141052246\n",
      "Batch 1000,  loss: 3.008809781074524\n",
      "Batch 1005,  loss: 3.139687156677246\n",
      "Batch 1010,  loss: 3.785899829864502\n",
      "Batch 1015,  loss: 3.258919334411621\n",
      "Batch 1020,  loss: 3.4639193534851076\n",
      "Batch 1025,  loss: 2.7038105726242065\n",
      "Batch 1030,  loss: 3.1614043951034545\n",
      "Batch 1035,  loss: 3.6923969745635987\n",
      "Batch 1040,  loss: 3.0430954933166503\n",
      "Batch 1045,  loss: 3.1747878074645994\n",
      "Batch 1050,  loss: 3.793683624267578\n",
      "Batch 1055,  loss: 3.037645959854126\n",
      "Batch 1060,  loss: 3.2733152866363526\n",
      "Batch 1065,  loss: 2.97699875831604\n",
      "Batch 1070,  loss: 3.1105340480804444\n",
      "Batch 1075,  loss: 2.93446307182312\n",
      "Batch 1080,  loss: 3.309208297729492\n",
      "Batch 1085,  loss: 3.1445393562316895\n",
      "Batch 1090,  loss: 2.8034832954406737\n",
      "Batch 1095,  loss: 3.0276408195495605\n",
      "Batch 1100,  loss: 3.0048271656036376\n",
      "Batch 1105,  loss: 2.7166147232055664\n",
      "Batch 1110,  loss: 3.4915563106536864\n",
      "Batch 1115,  loss: 3.092130422592163\n",
      "Batch 1120,  loss: 3.208781051635742\n",
      "Batch 1125,  loss: 3.673366355895996\n",
      "Batch 1130,  loss: 3.3282004833221435\n",
      "Batch 1135,  loss: 2.774706220626831\n",
      "Batch 1140,  loss: 2.9903776168823244\n",
      "Batch 1145,  loss: 4.080284261703492\n",
      "Batch 1150,  loss: 2.9511845111846924\n",
      "Batch 1155,  loss: 2.6285596609115602\n",
      "Batch 1160,  loss: 2.812615442276001\n",
      "Batch 1165,  loss: 3.7044575691223143\n",
      "Batch 1170,  loss: 3.3057265758514403\n",
      "Batch 1175,  loss: 3.1192864894866945\n",
      "Batch 1180,  loss: 2.6606948375701904\n",
      "Batch 1185,  loss: 3.5764177799224854\n",
      "Batch 1190,  loss: 3.0402069091796875\n",
      "Batch 1195,  loss: 3.1376229763031005\n",
      "Batch 1200,  loss: 3.002780723571777\n",
      "Batch 1205,  loss: 3.259504699707031\n",
      "Batch 1210,  loss: 2.6933496952056886\n",
      "Batch 1215,  loss: 3.0217538118362426\n",
      "Batch 1220,  loss: 3.0517996311187745\n",
      "Batch 1225,  loss: 3.3049365520477294\n",
      "Batch 1230,  loss: 4.039267921447754\n",
      "Batch 1235,  loss: 3.300240898132324\n",
      "Batch 1240,  loss: 2.724713087081909\n",
      "Batch 1245,  loss: 2.316536378860474\n",
      "Batch 1250,  loss: 2.781767177581787\n",
      "Batch 1255,  loss: 2.8680224418640137\n",
      "Batch 1260,  loss: 2.7684420108795167\n",
      "Batch 1265,  loss: 3.4823691844940186\n",
      "Batch 1270,  loss: 3.399742603302002\n",
      "Batch 1275,  loss: 2.8200859308242796\n",
      "Batch 1280,  loss: 2.3273867845535277\n",
      "Batch 1285,  loss: 3.6937222480773926\n",
      "Batch 1290,  loss: 2.946062469482422\n",
      "Batch 1295,  loss: 3.5743423461914063\n",
      "LOSS train 3.5743423461914063. Validation loss: 3.4667581933171108 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 7:\n",
      "Batch 5,  loss: 3.450793981552124\n",
      "Batch 10,  loss: 3.1098952770233153\n",
      "Batch 15,  loss: 2.5160358428955076\n",
      "Batch 20,  loss: 3.5634750366210937\n",
      "Batch 25,  loss: 3.4911626815795898\n",
      "Batch 30,  loss: 2.8610471725463866\n",
      "Batch 35,  loss: 2.6202489852905275\n",
      "Batch 40,  loss: 3.1862321853637696\n",
      "Batch 45,  loss: 2.7949344158172607\n",
      "Batch 50,  loss: 3.3090731620788576\n",
      "Batch 55,  loss: 3.6109062671661376\n",
      "Batch 60,  loss: 2.496111440658569\n",
      "Batch 65,  loss: 3.4059771060943604\n",
      "Batch 70,  loss: 2.9646699905395506\n",
      "Batch 75,  loss: 2.558820295333862\n",
      "Batch 80,  loss: 2.6339104175567627\n",
      "Batch 85,  loss: 3.0727939128875734\n",
      "Batch 90,  loss: 3.0765037536621094\n",
      "Batch 95,  loss: 2.5594533920288085\n",
      "Batch 100,  loss: 2.833062696456909\n",
      "Batch 105,  loss: 2.6795588970184325\n",
      "Batch 110,  loss: 2.784758138656616\n",
      "Batch 115,  loss: 2.9174643993377685\n",
      "Batch 120,  loss: 3.0470168590545654\n",
      "Batch 125,  loss: 3.3510720252990724\n",
      "Batch 130,  loss: 2.9214200735092164\n",
      "Batch 135,  loss: 2.490326976776123\n",
      "Batch 140,  loss: 3.065060329437256\n",
      "Batch 145,  loss: 2.875151491165161\n",
      "Batch 150,  loss: 3.46040563583374\n",
      "Batch 155,  loss: 3.0267192840576174\n",
      "Batch 160,  loss: 3.2135652542114257\n",
      "Batch 165,  loss: 3.2130055904388426\n",
      "Batch 170,  loss: 3.0158809423446655\n",
      "Batch 175,  loss: 3.688284683227539\n",
      "Batch 180,  loss: 3.00154185295105\n",
      "Batch 185,  loss: 3.0275208950042725\n",
      "Batch 190,  loss: 2.7875938415527344\n",
      "Batch 195,  loss: 3.356858968734741\n",
      "Batch 200,  loss: 3.433308458328247\n",
      "Batch 205,  loss: 2.9821009635925293\n",
      "Batch 210,  loss: 2.96087327003479\n",
      "Batch 215,  loss: 3.2891361236572267\n",
      "Batch 220,  loss: 3.007338523864746\n",
      "Batch 225,  loss: 2.652900457382202\n",
      "Batch 230,  loss: 3.0128714561462404\n",
      "Batch 235,  loss: 2.768447208404541\n",
      "Batch 240,  loss: 2.9012719631195067\n",
      "Batch 245,  loss: 3.2000396966934206\n",
      "Batch 250,  loss: 2.63107545375824\n",
      "Batch 255,  loss: 2.6817455768585203\n",
      "Batch 260,  loss: 2.569906210899353\n",
      "Batch 265,  loss: 3.2196006774902344\n",
      "Batch 270,  loss: 3.1451292991638184\n",
      "Batch 275,  loss: 2.869294834136963\n",
      "Batch 280,  loss: 3.1927818775177004\n",
      "Batch 285,  loss: 3.068748378753662\n",
      "Batch 290,  loss: 3.177190589904785\n",
      "Batch 295,  loss: 2.7854548931121825\n",
      "Batch 300,  loss: 2.8921041011810305\n",
      "Batch 305,  loss: 2.7964982986450195\n",
      "Batch 310,  loss: 3.631012487411499\n",
      "Batch 315,  loss: 3.20768141746521\n",
      "Batch 320,  loss: 3.532976531982422\n",
      "Batch 325,  loss: 2.832708644866943\n",
      "Batch 330,  loss: 2.552727556228638\n",
      "Batch 335,  loss: 3.262187194824219\n",
      "Batch 340,  loss: 3.021253490447998\n",
      "Batch 345,  loss: 2.429951238632202\n",
      "Batch 350,  loss: 2.901287889480591\n",
      "Batch 355,  loss: 3.062671184539795\n",
      "Batch 360,  loss: 2.9300181388854982\n",
      "Batch 365,  loss: 2.5169270277023315\n",
      "Batch 370,  loss: 3.017970848083496\n",
      "Batch 375,  loss: 2.32745099067688\n",
      "Batch 380,  loss: 3.5522900581359864\n",
      "Batch 385,  loss: 3.0544825077056883\n",
      "Batch 390,  loss: 3.7084208011627195\n",
      "Batch 395,  loss: 2.806567120552063\n",
      "Batch 400,  loss: 3.797232818603516\n",
      "Batch 405,  loss: 3.083635425567627\n",
      "Batch 410,  loss: 3.010979986190796\n",
      "Batch 415,  loss: 2.830218958854675\n",
      "Batch 420,  loss: 2.6131930351257324\n",
      "Batch 425,  loss: 2.89562668800354\n",
      "Batch 430,  loss: 2.8320956230163574\n",
      "Batch 435,  loss: 2.490570545196533\n",
      "Batch 440,  loss: 2.242446756362915\n",
      "Batch 445,  loss: 2.812263822555542\n",
      "Batch 450,  loss: 2.920327568054199\n",
      "Batch 455,  loss: 2.8180418491363524\n",
      "Batch 460,  loss: 3.1531644582748415\n",
      "Batch 465,  loss: 2.9391847133636473\n",
      "Batch 470,  loss: 2.9878821849822996\n",
      "Batch 475,  loss: 2.9341904878616334\n",
      "Batch 480,  loss: 2.6860657930374146\n",
      "Batch 485,  loss: 3.360746908187866\n",
      "Batch 490,  loss: 2.7335612773895264\n",
      "Batch 495,  loss: 3.231587028503418\n",
      "Batch 500,  loss: 2.1798200368881226\n",
      "Batch 505,  loss: 2.409493398666382\n",
      "Batch 510,  loss: 3.4488927841186525\n",
      "Batch 515,  loss: 2.514228582382202\n",
      "Batch 520,  loss: 2.787448835372925\n",
      "Batch 525,  loss: 3.0931721687316895\n",
      "Batch 530,  loss: 3.1760657787323\n",
      "Batch 535,  loss: 3.103261423110962\n",
      "Batch 540,  loss: 2.6881918907165527\n",
      "Batch 545,  loss: 3.0886549949645996\n",
      "Batch 550,  loss: 2.917200469970703\n",
      "Batch 555,  loss: 2.406981182098389\n",
      "Batch 560,  loss: 3.2639462471008303\n",
      "Batch 565,  loss: 2.82776141166687\n",
      "Batch 570,  loss: 3.334776425361633\n",
      "Batch 575,  loss: 3.384114456176758\n",
      "Batch 580,  loss: 3.3343145847320557\n",
      "Batch 585,  loss: 3.4267157077789308\n",
      "Batch 590,  loss: 3.2181702136993406\n",
      "Batch 595,  loss: 3.167305517196655\n",
      "Batch 600,  loss: 2.8877296447753906\n",
      "Batch 605,  loss: 2.58800950050354\n",
      "Batch 610,  loss: 3.5329320430755615\n",
      "Batch 615,  loss: 2.487498378753662\n",
      "Batch 620,  loss: 3.008283567428589\n",
      "Batch 625,  loss: 2.9544608354568482\n",
      "Batch 630,  loss: 2.4557075977325438\n",
      "Batch 635,  loss: 2.7753575801849366\n",
      "Batch 640,  loss: 2.4658085346221923\n",
      "Batch 645,  loss: 2.938721179962158\n",
      "Batch 650,  loss: 3.0562225818634032\n",
      "Batch 655,  loss: 3.157960033416748\n",
      "Batch 660,  loss: 3.058852958679199\n",
      "Batch 665,  loss: 3.209338331222534\n",
      "Batch 670,  loss: 3.1093099117279053\n",
      "Batch 675,  loss: 3.453202486038208\n",
      "Batch 680,  loss: 3.0178093910217285\n",
      "Batch 685,  loss: 3.0819896697998046\n",
      "Batch 690,  loss: 2.8939464569091795\n",
      "Batch 695,  loss: 3.1867240190505983\n",
      "Batch 700,  loss: 3.4165611267089844\n",
      "Batch 705,  loss: 3.1037943363189697\n",
      "Batch 710,  loss: 2.898998498916626\n",
      "Batch 715,  loss: 3.026614809036255\n",
      "Batch 720,  loss: 3.3227555751800537\n",
      "Batch 725,  loss: 2.6621524810791017\n",
      "Batch 730,  loss: 3.2331685543060305\n",
      "Batch 735,  loss: 3.1048014163970947\n",
      "Batch 740,  loss: 3.2916205883026124\n",
      "Batch 745,  loss: 2.409921073913574\n",
      "Batch 750,  loss: 3.5902427196502686\n",
      "Batch 755,  loss: 3.1422947883605956\n",
      "Batch 760,  loss: 3.0222588062286375\n",
      "Batch 765,  loss: 3.6707098960876463\n",
      "Batch 770,  loss: 3.5315024852752686\n",
      "Batch 775,  loss: 3.099514627456665\n",
      "Batch 780,  loss: 3.1014450550079347\n",
      "Batch 785,  loss: 2.498456525802612\n",
      "Batch 790,  loss: 2.3318548679351805\n",
      "Batch 795,  loss: 2.6330159664154054\n",
      "Batch 800,  loss: 2.6288058757781982\n",
      "Batch 805,  loss: 2.8570051431655883\n",
      "Batch 810,  loss: 3.3030585289001464\n",
      "Batch 815,  loss: 3.7928070068359374\n",
      "Batch 820,  loss: 2.2256956815719606\n",
      "Batch 825,  loss: 2.7612999439239503\n",
      "Batch 830,  loss: 2.099830651283264\n",
      "Batch 835,  loss: 3.0999847888946532\n",
      "Batch 840,  loss: 3.1103471755981444\n",
      "Batch 845,  loss: 2.522596073150635\n",
      "Batch 850,  loss: 3.257910680770874\n",
      "Batch 855,  loss: 2.9135893821716308\n",
      "Batch 860,  loss: 3.0236653327941894\n",
      "Batch 865,  loss: 3.2005786895751953\n",
      "Batch 870,  loss: 2.990415573120117\n",
      "Batch 875,  loss: 3.55073938369751\n",
      "Batch 880,  loss: 2.5396657466888426\n",
      "Batch 885,  loss: 2.82288498878479\n",
      "Batch 890,  loss: 2.5693927526474\n",
      "Batch 895,  loss: 3.362301540374756\n",
      "Batch 900,  loss: 3.304505157470703\n",
      "Batch 905,  loss: 2.8740012645721436\n",
      "Batch 910,  loss: 2.863412284851074\n",
      "Batch 915,  loss: 2.507707190513611\n",
      "Batch 920,  loss: 3.2243727684020995\n",
      "Batch 925,  loss: 3.2971049308776856\n",
      "Batch 930,  loss: 4.010537004470825\n",
      "Batch 935,  loss: 3.9257179260253907\n",
      "Batch 940,  loss: 3.3237702369689943\n",
      "Batch 945,  loss: 3.010279655456543\n",
      "Batch 950,  loss: 3.1475883960723876\n",
      "Batch 955,  loss: 2.9096207141876222\n",
      "Batch 960,  loss: 2.758312940597534\n",
      "Batch 965,  loss: 2.7192523956298826\n",
      "Batch 970,  loss: 3.3256563186645507\n",
      "Batch 975,  loss: 2.850221586227417\n",
      "Batch 980,  loss: 2.432130479812622\n",
      "Batch 985,  loss: 3.127953863143921\n",
      "Batch 990,  loss: 3.1447087049484255\n",
      "Batch 995,  loss: 3.159796142578125\n",
      "Batch 1000,  loss: 3.413859987258911\n",
      "Batch 1005,  loss: 2.3365294456481935\n",
      "Batch 1010,  loss: 2.5541227579116823\n",
      "Batch 1015,  loss: 3.3264434814453123\n",
      "Batch 1020,  loss: 2.5303300619125366\n",
      "Batch 1025,  loss: 2.8025391578674315\n",
      "Batch 1030,  loss: 3.053831171989441\n",
      "Batch 1035,  loss: 3.0326752185821535\n",
      "Batch 1040,  loss: 3.0778727769851684\n",
      "Batch 1045,  loss: 3.11625714302063\n",
      "Batch 1050,  loss: 2.718802642822266\n",
      "Batch 1055,  loss: 3.356836366653442\n",
      "Batch 1060,  loss: 2.758647346496582\n",
      "Batch 1065,  loss: 3.197486400604248\n",
      "Batch 1070,  loss: 2.8609885215759276\n",
      "Batch 1075,  loss: 3.0984092235565184\n",
      "Batch 1080,  loss: 3.3822415828704835\n",
      "Batch 1085,  loss: 3.114815855026245\n",
      "Batch 1090,  loss: 2.83155779838562\n",
      "Batch 1095,  loss: 2.6805604457855225\n",
      "Batch 1100,  loss: 2.981309676170349\n",
      "Batch 1105,  loss: 3.052943801879883\n",
      "Batch 1110,  loss: 2.806515026092529\n",
      "Batch 1115,  loss: 2.817952108383179\n",
      "Batch 1120,  loss: 2.5439468145370485\n",
      "Batch 1125,  loss: 2.8142489433288573\n",
      "Batch 1130,  loss: 3.309408950805664\n",
      "Batch 1135,  loss: 2.7671386003494263\n",
      "Batch 1140,  loss: 2.75433189868927\n",
      "Batch 1145,  loss: 2.8514066696166993\n",
      "Batch 1150,  loss: 2.8987705230712892\n",
      "Batch 1155,  loss: 2.6505794525146484\n",
      "Batch 1160,  loss: 3.0080461502075195\n",
      "Batch 1165,  loss: 3.0074965000152587\n",
      "Batch 1170,  loss: 3.316686677932739\n",
      "Batch 1175,  loss: 3.0043591499328612\n",
      "Batch 1180,  loss: 2.8791990280151367\n",
      "Batch 1185,  loss: 3.0589101791381834\n",
      "Batch 1190,  loss: 3.5356501579284667\n",
      "Batch 1195,  loss: 2.919694018363953\n",
      "Batch 1200,  loss: 3.2380202054977416\n",
      "Batch 1205,  loss: 3.284356784820557\n",
      "Batch 1210,  loss: 3.181467342376709\n",
      "Batch 1215,  loss: 2.629029369354248\n",
      "Batch 1220,  loss: 3.2785012245178224\n",
      "Batch 1225,  loss: 3.1541165828704836\n",
      "Batch 1230,  loss: 3.4389431476593018\n",
      "Batch 1235,  loss: 2.9897791385650634\n",
      "Batch 1240,  loss: 2.909968948364258\n",
      "Batch 1245,  loss: 3.249716281890869\n",
      "Batch 1250,  loss: 3.244793939590454\n",
      "Batch 1255,  loss: 2.7646913051605226\n",
      "Batch 1260,  loss: 2.7911931037902833\n",
      "Batch 1265,  loss: 2.715281677246094\n",
      "Batch 1270,  loss: 2.75786669254303\n",
      "Batch 1275,  loss: 3.2201271772384645\n",
      "Batch 1280,  loss: 2.898384428024292\n",
      "Batch 1285,  loss: 3.653419256210327\n",
      "Batch 1290,  loss: 2.7787192344665526\n",
      "Batch 1295,  loss: 3.22949275970459\n",
      "LOSS train 3.22949275970459. Validation loss: 3.368998522911635 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 8:\n",
      "Batch 5,  loss: 2.747598648071289\n",
      "Batch 10,  loss: 2.619898200035095\n",
      "Batch 15,  loss: 3.1922388553619383\n",
      "Batch 20,  loss: 2.621208643913269\n",
      "Batch 25,  loss: 3.0014456510543823\n",
      "Batch 30,  loss: 2.8332125186920165\n",
      "Batch 35,  loss: 2.57690372467041\n",
      "Batch 40,  loss: 2.4188046932220457\n",
      "Batch 45,  loss: 3.461763882637024\n",
      "Batch 50,  loss: 2.567412090301514\n",
      "Batch 55,  loss: 3.18663911819458\n",
      "Batch 60,  loss: 2.91651611328125\n",
      "Batch 65,  loss: 2.96767635345459\n",
      "Batch 70,  loss: 2.990209531784058\n",
      "Batch 75,  loss: 3.0101583003997803\n",
      "Batch 80,  loss: 2.7457810401916505\n",
      "Batch 85,  loss: 3.2484652519226076\n",
      "Batch 90,  loss: 2.721658182144165\n",
      "Batch 95,  loss: 2.99902024269104\n",
      "Batch 100,  loss: 2.680074453353882\n",
      "Batch 105,  loss: 2.9831372261047364\n",
      "Batch 110,  loss: 3.1115490436553954\n",
      "Batch 115,  loss: 2.775600957870483\n",
      "Batch 120,  loss: 2.7570768356323243\n",
      "Batch 125,  loss: 2.4116308212280275\n",
      "Batch 130,  loss: 2.540333795547485\n",
      "Batch 135,  loss: 2.5699195861816406\n",
      "Batch 140,  loss: 2.5273599147796633\n",
      "Batch 145,  loss: 2.5118274688720703\n",
      "Batch 150,  loss: 3.0347330570220947\n",
      "Batch 155,  loss: 2.779423952102661\n",
      "Batch 160,  loss: 3.436497449874878\n",
      "Batch 165,  loss: 2.9347430229187013\n",
      "Batch 170,  loss: 3.389615535736084\n",
      "Batch 175,  loss: 2.5447704076766966\n",
      "Batch 180,  loss: 2.5522619009017946\n",
      "Batch 185,  loss: 3.1386459827423097\n",
      "Batch 190,  loss: 2.2287460803985595\n",
      "Batch 195,  loss: 2.89591908454895\n",
      "Batch 200,  loss: 3.1397287845611572\n",
      "Batch 205,  loss: 3.355504035949707\n",
      "Batch 210,  loss: 2.8559340476989745\n",
      "Batch 215,  loss: 2.800164556503296\n",
      "Batch 220,  loss: 2.9740689754486085\n",
      "Batch 225,  loss: 2.8399776458740233\n",
      "Batch 230,  loss: 3.0159643411636354\n",
      "Batch 235,  loss: 2.8363640785217283\n",
      "Batch 240,  loss: 2.433992385864258\n",
      "Batch 245,  loss: 3.018341827392578\n",
      "Batch 250,  loss: 2.5770360469818114\n",
      "Batch 255,  loss: 2.7061836242675783\n",
      "Batch 260,  loss: 2.9428564071655274\n",
      "Batch 265,  loss: 2.6412533283233643\n",
      "Batch 270,  loss: 2.899449110031128\n",
      "Batch 275,  loss: 2.042348313331604\n",
      "Batch 280,  loss: 2.9687469959259034\n",
      "Batch 285,  loss: 3.017215871810913\n",
      "Batch 290,  loss: 3.2146809101104736\n",
      "Batch 295,  loss: 2.6687793731689453\n",
      "Batch 300,  loss: 2.8104717254638674\n",
      "Batch 305,  loss: 3.037914848327637\n",
      "Batch 310,  loss: 3.1460214614868165\n",
      "Batch 315,  loss: 2.765883779525757\n",
      "Batch 320,  loss: 2.940811014175415\n",
      "Batch 325,  loss: 3.036875581741333\n",
      "Batch 330,  loss: 2.6837196350097656\n",
      "Batch 335,  loss: 2.9322084903717043\n",
      "Batch 340,  loss: 2.8561892986297606\n",
      "Batch 345,  loss: 2.887532353401184\n",
      "Batch 350,  loss: 3.642987823486328\n",
      "Batch 355,  loss: 3.3773174047470094\n",
      "Batch 360,  loss: 2.7416670322418213\n",
      "Batch 365,  loss: 3.4072800636291505\n",
      "Batch 370,  loss: 2.920709800720215\n",
      "Batch 375,  loss: 2.730903720855713\n",
      "Batch 380,  loss: 2.728771781921387\n",
      "Batch 385,  loss: 2.487391138076782\n",
      "Batch 390,  loss: 2.5301802158355713\n",
      "Batch 395,  loss: 2.218289613723755\n",
      "Batch 400,  loss: 2.8803045749664307\n",
      "Batch 405,  loss: 3.1042597770690916\n",
      "Batch 410,  loss: 2.3830838203430176\n",
      "Batch 415,  loss: 2.5351702213287353\n",
      "Batch 420,  loss: 2.8889575481414793\n",
      "Batch 425,  loss: 3.706657600402832\n",
      "Batch 430,  loss: 2.291807699203491\n",
      "Batch 435,  loss: 3.3975024223327637\n",
      "Batch 440,  loss: 2.6500211477279665\n",
      "Batch 445,  loss: 3.307579278945923\n",
      "Batch 450,  loss: 2.869822788238525\n",
      "Batch 455,  loss: 2.778523826599121\n",
      "Batch 460,  loss: 2.139771032333374\n",
      "Batch 465,  loss: 3.232404899597168\n",
      "Batch 470,  loss: 2.5903321266174317\n",
      "Batch 475,  loss: 3.344330596923828\n",
      "Batch 480,  loss: 2.7123002767562867\n",
      "Batch 485,  loss: 3.3620324611663817\n",
      "Batch 490,  loss: 3.016707181930542\n",
      "Batch 495,  loss: 2.164213514328003\n",
      "Batch 500,  loss: 2.69061279296875\n",
      "Batch 505,  loss: 2.8267916440963745\n",
      "Batch 510,  loss: 2.9434316635131834\n",
      "Batch 515,  loss: 2.568685293197632\n",
      "Batch 520,  loss: 3.016386365890503\n",
      "Batch 525,  loss: 2.8097336292266846\n",
      "Batch 530,  loss: 3.2278053760528564\n",
      "Batch 535,  loss: 2.6284525632858275\n",
      "Batch 540,  loss: 3.0500415325164796\n",
      "Batch 545,  loss: 2.9941429615020754\n",
      "Batch 550,  loss: 3.1574398994445803\n",
      "Batch 555,  loss: 2.2233734130859375\n",
      "Batch 560,  loss: 3.0378693342208862\n",
      "Batch 565,  loss: 2.8626636981964113\n",
      "Batch 570,  loss: 3.2057622194290163\n",
      "Batch 575,  loss: 3.350660228729248\n",
      "Batch 580,  loss: 3.265428638458252\n",
      "Batch 585,  loss: 2.983109378814697\n",
      "Batch 590,  loss: 2.278900671005249\n",
      "Batch 595,  loss: 2.5562424659729004\n",
      "Batch 600,  loss: 2.1212429761886598\n",
      "Batch 605,  loss: 3.5730841159820557\n",
      "Batch 610,  loss: 3.1182884216308593\n",
      "Batch 615,  loss: 3.2113006353378295\n",
      "Batch 620,  loss: 2.672988700866699\n",
      "Batch 625,  loss: 2.9735729694366455\n",
      "Batch 630,  loss: 3.0759634494781496\n",
      "Batch 635,  loss: 2.3998301029205322\n",
      "Batch 640,  loss: 3.264857292175293\n",
      "Batch 645,  loss: 2.8205095291137696\n",
      "Batch 650,  loss: 3.345767879486084\n",
      "Batch 655,  loss: 2.6570287227630613\n",
      "Batch 660,  loss: 3.159743595123291\n",
      "Batch 665,  loss: 3.048450803756714\n",
      "Batch 670,  loss: 2.646169137954712\n",
      "Batch 675,  loss: 3.0241115570068358\n",
      "Batch 680,  loss: 3.3991212368011476\n",
      "Batch 685,  loss: 2.824879002571106\n",
      "Batch 690,  loss: 2.7983443975448608\n",
      "Batch 695,  loss: 2.6976020336151123\n",
      "Batch 700,  loss: 3.4045863151550293\n",
      "Batch 705,  loss: 2.9718934535980224\n",
      "Batch 710,  loss: 2.844689202308655\n",
      "Batch 715,  loss: 2.7244801998138426\n",
      "Batch 720,  loss: 2.8571308612823487\n",
      "Batch 725,  loss: 2.2958176374435424\n",
      "Batch 730,  loss: 3.354277563095093\n",
      "Batch 735,  loss: 2.8528367280960083\n",
      "Batch 740,  loss: 3.2013463020324706\n",
      "Batch 745,  loss: 2.908777284622192\n",
      "Batch 750,  loss: 2.663913679122925\n",
      "Batch 755,  loss: 2.6600231170654296\n",
      "Batch 760,  loss: 2.896548318862915\n",
      "Batch 765,  loss: 2.7732589721679686\n",
      "Batch 770,  loss: 2.7220067977905273\n",
      "Batch 775,  loss: 2.8249558448791503\n",
      "Batch 780,  loss: 3.9137032508850096\n",
      "Batch 785,  loss: 2.9423603057861327\n",
      "Batch 790,  loss: 3.029425573348999\n",
      "Batch 795,  loss: 3.168753957748413\n",
      "Batch 800,  loss: 3.180076742172241\n",
      "Batch 805,  loss: 2.8590737342834474\n",
      "Batch 810,  loss: 3.145577096939087\n",
      "Batch 815,  loss: 2.91261739730835\n",
      "Batch 820,  loss: 2.8364608764648436\n",
      "Batch 825,  loss: 3.0351850032806396\n",
      "Batch 830,  loss: 3.4768287181854247\n",
      "Batch 835,  loss: 3.0017985343933105\n",
      "Batch 840,  loss: 2.7074912548065186\n",
      "Batch 845,  loss: 2.847577524185181\n",
      "Batch 850,  loss: 3.4568008899688722\n",
      "Batch 855,  loss: 2.6768887519836424\n",
      "Batch 860,  loss: 2.9014875411987306\n",
      "Batch 865,  loss: 3.0384090185165404\n",
      "Batch 870,  loss: 2.707833409309387\n",
      "Batch 875,  loss: 3.0631362915039064\n",
      "Batch 880,  loss: 2.433756422996521\n",
      "Batch 885,  loss: 3.2473095893859862\n",
      "Batch 890,  loss: 2.2497140407562255\n",
      "Batch 895,  loss: 3.254238820075989\n",
      "Batch 900,  loss: 2.754822015762329\n",
      "Batch 905,  loss: 2.589873433113098\n",
      "Batch 910,  loss: 3.7126954078674315\n",
      "Batch 915,  loss: 2.6218250036239623\n",
      "Batch 920,  loss: 2.780537223815918\n",
      "Batch 925,  loss: 3.3006865978240967\n",
      "Batch 930,  loss: 2.979156732559204\n",
      "Batch 935,  loss: 2.537422442436218\n",
      "Batch 940,  loss: 2.899615430831909\n",
      "Batch 945,  loss: 2.9951847553253175\n",
      "Batch 950,  loss: 2.750762963294983\n",
      "Batch 955,  loss: 2.896596574783325\n",
      "Batch 960,  loss: 3.0558922290802\n",
      "Batch 965,  loss: 2.9897833347320555\n",
      "Batch 970,  loss: 3.2592507362365724\n",
      "Batch 975,  loss: 2.5321643352508545\n",
      "Batch 980,  loss: 2.7483526468276978\n",
      "Batch 985,  loss: 3.135881853103638\n",
      "Batch 990,  loss: 3.029379653930664\n",
      "Batch 995,  loss: 2.588415431976318\n",
      "Batch 1000,  loss: 3.366526412963867\n",
      "Batch 1005,  loss: 3.2506547451019285\n",
      "Batch 1010,  loss: 2.430102753639221\n",
      "Batch 1015,  loss: 2.5314542055130005\n",
      "Batch 1020,  loss: 2.4579102039337157\n",
      "Batch 1025,  loss: 2.201001000404358\n",
      "Batch 1030,  loss: 2.969904327392578\n",
      "Batch 1035,  loss: 3.170984077453613\n",
      "Batch 1040,  loss: 2.308951663970947\n",
      "Batch 1045,  loss: 2.865529251098633\n",
      "Batch 1050,  loss: 2.5055498123168944\n",
      "Batch 1055,  loss: 3.1858335971832275\n",
      "Batch 1060,  loss: 2.7633774280548096\n",
      "Batch 1065,  loss: 2.847237491607666\n",
      "Batch 1070,  loss: 3.1256386756896974\n",
      "Batch 1075,  loss: 2.602537441253662\n",
      "Batch 1080,  loss: 3.064458799362183\n",
      "Batch 1085,  loss: 2.4304744005203247\n",
      "Batch 1090,  loss: 2.824919557571411\n",
      "Batch 1095,  loss: 3.4460781097412108\n",
      "Batch 1100,  loss: 2.9447129726409913\n",
      "Batch 1105,  loss: 3.0966896533966066\n",
      "Batch 1110,  loss: 2.859635257720947\n",
      "Batch 1115,  loss: 3.017853546142578\n",
      "Batch 1120,  loss: 2.474406909942627\n",
      "Batch 1125,  loss: 2.6018529653549196\n",
      "Batch 1130,  loss: 2.58148193359375\n",
      "Batch 1135,  loss: 2.542235851287842\n",
      "Batch 1140,  loss: 3.1650455474853514\n",
      "Batch 1145,  loss: 3.0867342948913574\n",
      "Batch 1150,  loss: 2.9444284439086914\n",
      "Batch 1155,  loss: 2.94511342048645\n",
      "Batch 1160,  loss: 2.6613547801971436\n",
      "Batch 1165,  loss: 3.8311421394348146\n",
      "Batch 1170,  loss: 2.4716256141662596\n",
      "Batch 1175,  loss: 3.1536845922470094\n",
      "Batch 1180,  loss: 2.3619948863983153\n",
      "Batch 1185,  loss: 2.906729555130005\n",
      "Batch 1190,  loss: 2.4622292518615723\n",
      "Batch 1195,  loss: 2.75018253326416\n",
      "Batch 1200,  loss: 3.654483127593994\n",
      "Batch 1205,  loss: 3.0000660896301268\n",
      "Batch 1210,  loss: 2.8173618793487547\n",
      "Batch 1215,  loss: 2.6554078102111816\n",
      "Batch 1220,  loss: 2.5508034944534304\n",
      "Batch 1225,  loss: 2.6507261276245115\n",
      "Batch 1230,  loss: 2.4780344724655152\n",
      "Batch 1235,  loss: 2.5388840198516847\n",
      "Batch 1240,  loss: 2.2350465297698974\n",
      "Batch 1245,  loss: 3.065157580375671\n",
      "Batch 1250,  loss: 2.6601943492889406\n",
      "Batch 1255,  loss: 2.947235679626465\n",
      "Batch 1260,  loss: 2.7112684726715086\n",
      "Batch 1265,  loss: 2.783258152008057\n",
      "Batch 1270,  loss: 2.93238525390625\n",
      "Batch 1275,  loss: 2.588082265853882\n",
      "Batch 1280,  loss: 2.354698967933655\n",
      "Batch 1285,  loss: 2.3189234256744387\n",
      "Batch 1290,  loss: 3.0056963920593263\n",
      "Batch 1295,  loss: 3.4059130191802978\n",
      "LOSS train 3.4059130191802978. Validation loss: 3.190575417456717 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 9:\n",
      "Batch 5,  loss: 2.661166954040527\n",
      "Batch 10,  loss: 2.4991700410842896\n",
      "Batch 15,  loss: 3.1207559585571287\n",
      "Batch 20,  loss: 3.1064223289489745\n",
      "Batch 25,  loss: 2.729037547111511\n",
      "Batch 30,  loss: 3.538347578048706\n",
      "Batch 35,  loss: 2.6726011514663695\n",
      "Batch 40,  loss: 2.4088696241378784\n",
      "Batch 45,  loss: 3.3757326126098635\n",
      "Batch 50,  loss: 3.2317986488342285\n",
      "Batch 55,  loss: 2.836826276779175\n",
      "Batch 60,  loss: 2.3757100820541384\n",
      "Batch 65,  loss: 2.932238483428955\n",
      "Batch 70,  loss: 2.7160213470458983\n",
      "Batch 75,  loss: 2.285096549987793\n",
      "Batch 80,  loss: 2.9031791210174562\n",
      "Batch 85,  loss: 2.5354521751403807\n",
      "Batch 90,  loss: 3.022907018661499\n",
      "Batch 95,  loss: 2.2274596214294435\n",
      "Batch 100,  loss: 2.838290643692017\n",
      "Batch 105,  loss: 2.7860516786575316\n",
      "Batch 110,  loss: 2.5166791915893554\n",
      "Batch 115,  loss: 2.5825556755065917\n",
      "Batch 120,  loss: 2.3903204441070556\n",
      "Batch 125,  loss: 3.395453596115112\n",
      "Batch 130,  loss: 2.225483226776123\n",
      "Batch 135,  loss: 3.034630012512207\n",
      "Batch 140,  loss: 3.1835747241973875\n",
      "Batch 145,  loss: 3.562967395782471\n",
      "Batch 150,  loss: 2.696428370475769\n",
      "Batch 155,  loss: 2.5144774436950685\n",
      "Batch 160,  loss: 2.6148290157318117\n",
      "Batch 165,  loss: 3.122553586959839\n",
      "Batch 170,  loss: 2.8558922529220583\n",
      "Batch 175,  loss: 2.8940732002258303\n",
      "Batch 180,  loss: 2.9525831937789917\n",
      "Batch 185,  loss: 2.2989802837371824\n",
      "Batch 190,  loss: 3.279209518432617\n",
      "Batch 195,  loss: 2.6402673721313477\n",
      "Batch 200,  loss: 3.396777105331421\n",
      "Batch 205,  loss: 2.20550651550293\n",
      "Batch 210,  loss: 2.1001317739486693\n",
      "Batch 215,  loss: 2.757480764389038\n",
      "Batch 220,  loss: 2.6514643669128417\n",
      "Batch 225,  loss: 3.7718692779541017\n",
      "Batch 230,  loss: 2.677866744995117\n",
      "Batch 235,  loss: 3.188372039794922\n",
      "Batch 240,  loss: 2.4321675300598145\n",
      "Batch 245,  loss: 2.8330796718597413\n",
      "Batch 250,  loss: 2.972561073303223\n",
      "Batch 255,  loss: 2.4437198877334594\n",
      "Batch 260,  loss: 3.144889497756958\n",
      "Batch 265,  loss: 2.7770755767822264\n",
      "Batch 270,  loss: 3.0995686054229736\n",
      "Batch 275,  loss: 2.104223108291626\n",
      "Batch 280,  loss: 2.9665197849273683\n",
      "Batch 285,  loss: 2.8331369876861574\n",
      "Batch 290,  loss: 2.8105077743530273\n",
      "Batch 295,  loss: 2.500385546684265\n",
      "Batch 300,  loss: 3.3427483558654787\n",
      "Batch 305,  loss: 3.059081959724426\n",
      "Batch 310,  loss: 3.2609686851501465\n",
      "Batch 315,  loss: 2.21754310131073\n",
      "Batch 320,  loss: 2.749609851837158\n",
      "Batch 325,  loss: 2.3673715829849242\n",
      "Batch 330,  loss: 2.751967191696167\n",
      "Batch 335,  loss: 3.2799907684326173\n",
      "Batch 340,  loss: 2.990834045410156\n",
      "Batch 345,  loss: 2.5919514656066895\n",
      "Batch 350,  loss: 2.819647455215454\n",
      "Batch 355,  loss: 3.2672003746032714\n",
      "Batch 360,  loss: 2.679828977584839\n",
      "Batch 365,  loss: 2.972375774383545\n",
      "Batch 370,  loss: 2.4241320848464967\n",
      "Batch 375,  loss: 2.3073442935943604\n",
      "Batch 380,  loss: 3.3479441165924073\n",
      "Batch 385,  loss: 3.219947671890259\n",
      "Batch 390,  loss: 3.35479736328125\n",
      "Batch 395,  loss: 2.8763214111328126\n",
      "Batch 400,  loss: 2.911719560623169\n",
      "Batch 405,  loss: 2.701331377029419\n",
      "Batch 410,  loss: 2.8187129497528076\n",
      "Batch 415,  loss: 3.033461570739746\n",
      "Batch 420,  loss: 2.527500534057617\n",
      "Batch 425,  loss: 2.096900129318237\n",
      "Batch 430,  loss: 2.797964096069336\n",
      "Batch 435,  loss: 2.660553050041199\n",
      "Batch 440,  loss: 2.2913859844207765\n",
      "Batch 445,  loss: 2.4639023780822753\n",
      "Batch 450,  loss: 2.7636145830154417\n",
      "Batch 455,  loss: 3.836978626251221\n",
      "Batch 460,  loss: 3.0704847812652587\n",
      "Batch 465,  loss: 2.29547917842865\n",
      "Batch 470,  loss: 3.15269615650177\n",
      "Batch 475,  loss: 2.371272659301758\n",
      "Batch 480,  loss: 2.136942148208618\n",
      "Batch 485,  loss: 3.1243651866912843\n",
      "Batch 490,  loss: 2.2727802753448487\n",
      "Batch 495,  loss: 2.8434381008148195\n",
      "Batch 500,  loss: 2.1745465755462647\n",
      "Batch 505,  loss: 2.7978896379470823\n",
      "Batch 510,  loss: 2.38542263507843\n",
      "Batch 515,  loss: 2.8642831802368165\n",
      "Batch 520,  loss: 2.2382226467132567\n",
      "Batch 525,  loss: 2.7811517477035523\n",
      "Batch 530,  loss: 3.0331663608551027\n",
      "Batch 535,  loss: 2.3047712087631225\n",
      "Batch 540,  loss: 2.6520146131515503\n",
      "Batch 545,  loss: 2.883443737030029\n",
      "Batch 550,  loss: 2.326874256134033\n",
      "Batch 555,  loss: 2.540791702270508\n",
      "Batch 560,  loss: 3.336763286590576\n",
      "Batch 565,  loss: 2.7452502727508543\n",
      "Batch 570,  loss: 3.112591791152954\n",
      "Batch 575,  loss: 2.8611893177032472\n",
      "Batch 580,  loss: 2.7468705654144285\n",
      "Batch 585,  loss: 2.9462032318115234\n",
      "Batch 590,  loss: 2.72991361618042\n",
      "Batch 595,  loss: 2.6598694801330565\n",
      "Batch 600,  loss: 2.759762191772461\n",
      "Batch 605,  loss: 2.7599981307983397\n",
      "Batch 610,  loss: 3.55625262260437\n",
      "Batch 615,  loss: 3.0240792751312258\n",
      "Batch 620,  loss: 2.6565184593200684\n",
      "Batch 625,  loss: 3.3458454847335815\n",
      "Batch 630,  loss: 2.2941614389419556\n",
      "Batch 635,  loss: 2.4509032487869264\n",
      "Batch 640,  loss: 2.4654415607452393\n",
      "Batch 645,  loss: 2.7263858795166014\n",
      "Batch 650,  loss: 2.476082921028137\n",
      "Batch 655,  loss: 3.234922432899475\n",
      "Batch 660,  loss: 2.5399564504623413\n",
      "Batch 665,  loss: 2.7978477239608766\n",
      "Batch 670,  loss: 2.7401363849639893\n",
      "Batch 675,  loss: 2.493148922920227\n",
      "Batch 680,  loss: 2.3588335037231447\n",
      "Batch 685,  loss: 3.1159313678741456\n",
      "Batch 690,  loss: 2.692666840553284\n",
      "Batch 695,  loss: 2.2794589042663573\n",
      "Batch 700,  loss: 3.250480842590332\n",
      "Batch 705,  loss: 2.9583322525024416\n",
      "Batch 710,  loss: 2.5216768264770506\n",
      "Batch 715,  loss: 3.4043649196624757\n",
      "Batch 720,  loss: 3.1507981300354\n",
      "Batch 725,  loss: 2.3451982498168946\n",
      "Batch 730,  loss: 2.8592610359191895\n",
      "Batch 735,  loss: 2.6175088405609133\n",
      "Batch 740,  loss: 2.58641996383667\n",
      "Batch 745,  loss: 3.2146815776824953\n",
      "Batch 750,  loss: 2.7926655292510985\n",
      "Batch 755,  loss: 2.679252815246582\n",
      "Batch 760,  loss: 2.2345992803573607\n",
      "Batch 765,  loss: 2.507510757446289\n",
      "Batch 770,  loss: 2.792032337188721\n",
      "Batch 775,  loss: 2.4810290813446043\n",
      "Batch 780,  loss: 3.497495174407959\n",
      "Batch 785,  loss: 3.3636992454528807\n",
      "Batch 790,  loss: 2.7918951511383057\n",
      "Batch 795,  loss: 2.6120011806488037\n",
      "Batch 800,  loss: 2.360725736618042\n",
      "Batch 805,  loss: 3.0002636671066285\n",
      "Batch 810,  loss: 2.4827551364898683\n",
      "Batch 815,  loss: 2.6803621292114257\n",
      "Batch 820,  loss: 2.7369088172912597\n",
      "Batch 825,  loss: 2.4519933223724366\n",
      "Batch 830,  loss: 2.5679940700531008\n",
      "Batch 835,  loss: 3.4409422874450684\n",
      "Batch 840,  loss: 2.4637823820114138\n",
      "Batch 845,  loss: 2.0714327812194826\n",
      "Batch 850,  loss: 2.777754783630371\n",
      "Batch 855,  loss: 3.0933535575866697\n",
      "Batch 860,  loss: 2.8514966487884523\n",
      "Batch 865,  loss: 3.7256725311279295\n",
      "Batch 870,  loss: 2.437273693084717\n",
      "Batch 875,  loss: 2.1583548545837403\n",
      "Batch 880,  loss: 2.796385717391968\n",
      "Batch 885,  loss: 2.8751415252685546\n",
      "Batch 890,  loss: 2.2774794340133666\n",
      "Batch 895,  loss: 2.01254825592041\n",
      "Batch 900,  loss: 2.3657716274261475\n",
      "Batch 905,  loss: 2.3348264694213867\n",
      "Batch 910,  loss: 2.798534059524536\n",
      "Batch 915,  loss: 2.2740569829940798\n",
      "Batch 920,  loss: 2.140546369552612\n",
      "Batch 925,  loss: 3.5646792888641357\n",
      "Batch 930,  loss: 3.1170304298400877\n",
      "Batch 935,  loss: 2.2756413221359253\n",
      "Batch 940,  loss: 2.182624101638794\n",
      "Batch 945,  loss: 2.421256923675537\n",
      "Batch 950,  loss: 2.792181468009949\n",
      "Batch 955,  loss: 2.718586730957031\n",
      "Batch 960,  loss: 2.777914810180664\n",
      "Batch 965,  loss: 2.916424608230591\n",
      "Batch 970,  loss: 2.9827716827392576\n",
      "Batch 975,  loss: 3.0689472198486327\n",
      "Batch 980,  loss: 2.3645745277404786\n",
      "Batch 985,  loss: 2.390295219421387\n",
      "Batch 990,  loss: 2.6774837493896486\n",
      "Batch 995,  loss: 2.7833745002746584\n",
      "Batch 1000,  loss: 2.47664053440094\n",
      "Batch 1005,  loss: 3.080323338508606\n",
      "Batch 1010,  loss: 3.259004831314087\n",
      "Batch 1015,  loss: 2.934800863265991\n",
      "Batch 1020,  loss: 3.2636770725250246\n",
      "Batch 1025,  loss: 2.444248390197754\n",
      "Batch 1030,  loss: 2.7193540573120116\n",
      "Batch 1035,  loss: 2.990412950515747\n",
      "Batch 1040,  loss: 2.870354700088501\n",
      "Batch 1045,  loss: 3.102651500701904\n",
      "Batch 1050,  loss: 2.7083075046539307\n",
      "Batch 1055,  loss: 2.8519598484039306\n",
      "Batch 1060,  loss: 2.5243891000747682\n",
      "Batch 1065,  loss: 3.110082244873047\n",
      "Batch 1070,  loss: 2.3005674839019776\n",
      "Batch 1075,  loss: 2.658788776397705\n",
      "Batch 1080,  loss: 2.9065361499786375\n",
      "Batch 1085,  loss: 2.552636003494263\n",
      "Batch 1090,  loss: 3.073115921020508\n",
      "Batch 1095,  loss: 2.4557416677474975\n",
      "Batch 1100,  loss: 2.597004437446594\n",
      "Batch 1105,  loss: 3.20514714717865\n",
      "Batch 1110,  loss: 3.0075839042663572\n",
      "Batch 1115,  loss: 3.2696531295776365\n",
      "Batch 1120,  loss: 2.309243154525757\n",
      "Batch 1125,  loss: 3.075795793533325\n",
      "Batch 1130,  loss: 2.5851590156555178\n",
      "Batch 1135,  loss: 3.052503776550293\n",
      "Batch 1140,  loss: 2.26177921295166\n",
      "Batch 1145,  loss: 2.2321733713150023\n",
      "Batch 1150,  loss: 2.6560954332351683\n",
      "Batch 1155,  loss: 2.191863465309143\n",
      "Batch 1160,  loss: 2.897587013244629\n",
      "Batch 1165,  loss: 2.3334200620651244\n",
      "Batch 1170,  loss: 2.511801528930664\n",
      "Batch 1175,  loss: 3.4255483150482178\n",
      "Batch 1180,  loss: 2.68464081287384\n",
      "Batch 1185,  loss: 2.462143087387085\n",
      "Batch 1190,  loss: 2.497572350502014\n",
      "Batch 1195,  loss: 2.6808426856994627\n",
      "Batch 1200,  loss: 2.0761146068573\n",
      "Batch 1205,  loss: 2.396605110168457\n",
      "Batch 1210,  loss: 2.6835514068603517\n",
      "Batch 1215,  loss: 2.5459439277648928\n",
      "Batch 1220,  loss: 2.558099889755249\n",
      "Batch 1225,  loss: 3.3736788272857665\n",
      "Batch 1230,  loss: 2.2811311960220335\n",
      "Batch 1235,  loss: 2.6928350210189818\n",
      "Batch 1240,  loss: 2.4504849910736084\n",
      "Batch 1245,  loss: 2.860634183883667\n",
      "Batch 1250,  loss: 2.406381607055664\n",
      "Batch 1255,  loss: 2.83892936706543\n",
      "Batch 1260,  loss: 3.224053716659546\n",
      "Batch 1265,  loss: 2.4302107810974123\n",
      "Batch 1270,  loss: 2.200508689880371\n",
      "Batch 1275,  loss: 2.463630962371826\n",
      "Batch 1280,  loss: 2.8339943170547484\n",
      "Batch 1285,  loss: 2.9745185375213623\n",
      "Batch 1290,  loss: 2.916264867782593\n",
      "Batch 1295,  loss: 2.443480348587036\n",
      "LOSS train 2.443480348587036. Validation loss: 3.0789963139979926 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 10:\n",
      "Batch 5,  loss: 3.087956094741821\n",
      "Batch 10,  loss: 2.7951149702072144\n",
      "Batch 15,  loss: 2.8258394241333007\n",
      "Batch 20,  loss: 3.2383711338043213\n",
      "Batch 25,  loss: 2.3761124134063722\n",
      "Batch 30,  loss: 2.6758236408233644\n",
      "Batch 35,  loss: 2.4814048290252684\n",
      "Batch 40,  loss: 3.057877016067505\n",
      "Batch 45,  loss: 2.772374963760376\n",
      "Batch 50,  loss: 2.8236402034759522\n",
      "Batch 55,  loss: 2.6925694227218626\n",
      "Batch 60,  loss: 2.6085598945617674\n",
      "Batch 65,  loss: 3.4274966955184936\n",
      "Batch 70,  loss: 2.1376572608947755\n",
      "Batch 75,  loss: 2.391203761100769\n",
      "Batch 80,  loss: 2.715149736404419\n",
      "Batch 85,  loss: 2.9391524314880373\n",
      "Batch 90,  loss: 3.235391139984131\n",
      "Batch 95,  loss: 2.7051042556762694\n",
      "Batch 100,  loss: 2.667751502990723\n",
      "Batch 105,  loss: 3.6367230892181395\n",
      "Batch 110,  loss: 2.504043960571289\n",
      "Batch 115,  loss: 2.4089349508285522\n",
      "Batch 120,  loss: 2.5514646768569946\n",
      "Batch 125,  loss: 2.5279427051544188\n",
      "Batch 130,  loss: 2.170768165588379\n",
      "Batch 135,  loss: 2.6836216926574705\n",
      "Batch 140,  loss: 2.4886219024658205\n",
      "Batch 145,  loss: 2.366238903999329\n",
      "Batch 150,  loss: 2.6799225568771363\n",
      "Batch 155,  loss: 2.586923599243164\n",
      "Batch 160,  loss: 2.443415403366089\n",
      "Batch 165,  loss: 2.608642578125\n",
      "Batch 170,  loss: 3.453358840942383\n",
      "Batch 175,  loss: 2.654110050201416\n",
      "Batch 180,  loss: 2.3701465129852295\n",
      "Batch 185,  loss: 2.589856767654419\n",
      "Batch 190,  loss: 2.5092074871063232\n",
      "Batch 195,  loss: 2.387879824638367\n",
      "Batch 200,  loss: 2.936729145050049\n",
      "Batch 205,  loss: 1.9891091346740724\n",
      "Batch 210,  loss: 2.309304404258728\n",
      "Batch 215,  loss: 2.790029001235962\n",
      "Batch 220,  loss: 2.464755392074585\n",
      "Batch 225,  loss: 2.2953189611434937\n",
      "Batch 230,  loss: 2.612141966819763\n",
      "Batch 235,  loss: 2.774402928352356\n",
      "Batch 240,  loss: 2.626423192024231\n",
      "Batch 245,  loss: 3.0537892818450927\n",
      "Batch 250,  loss: 3.0862566471099853\n",
      "Batch 255,  loss: 2.425078845024109\n",
      "Batch 260,  loss: 3.4509304046630858\n",
      "Batch 265,  loss: 2.9512021064758303\n",
      "Batch 270,  loss: 3.1955196857452393\n",
      "Batch 275,  loss: 2.8264558792114256\n",
      "Batch 280,  loss: 2.3206819772720335\n",
      "Batch 285,  loss: 2.5906580448150636\n",
      "Batch 290,  loss: 2.303558921813965\n",
      "Batch 295,  loss: 2.524627447128296\n",
      "Batch 300,  loss: 3.292362833023071\n",
      "Batch 305,  loss: 2.4034632205963136\n",
      "Batch 310,  loss: 2.6659836769104004\n",
      "Batch 315,  loss: 2.8476089954376222\n",
      "Batch 320,  loss: 2.214936876296997\n",
      "Batch 325,  loss: 2.218228077888489\n",
      "Batch 330,  loss: 2.082805800437927\n",
      "Batch 335,  loss: 3.0546600818634033\n",
      "Batch 340,  loss: 3.3132445335388185\n",
      "Batch 345,  loss: 3.5857446670532225\n",
      "Batch 350,  loss: 2.2355311155319213\n",
      "Batch 355,  loss: 2.244061231613159\n",
      "Batch 360,  loss: 3.571727180480957\n",
      "Batch 365,  loss: 2.4012856483459473\n",
      "Batch 370,  loss: 2.3794750213623046\n",
      "Batch 375,  loss: 2.9524628162384032\n",
      "Batch 380,  loss: 2.4581151008605957\n",
      "Batch 385,  loss: 2.647310209274292\n",
      "Batch 390,  loss: 2.9758629322052004\n",
      "Batch 395,  loss: 3.247859573364258\n",
      "Batch 400,  loss: 2.5989936351776124\n",
      "Batch 405,  loss: 2.557418465614319\n",
      "Batch 410,  loss: 3.283122110366821\n",
      "Batch 415,  loss: 2.3001646995544434\n",
      "Batch 420,  loss: 3.2058620929718016\n",
      "Batch 425,  loss: 2.780691123008728\n",
      "Batch 430,  loss: 2.822988986968994\n",
      "Batch 435,  loss: 2.621617889404297\n",
      "Batch 440,  loss: 2.8522947788238526\n",
      "Batch 445,  loss: 2.698649597167969\n",
      "Batch 450,  loss: 2.6170007228851317\n",
      "Batch 455,  loss: 3.0318486213684084\n",
      "Batch 460,  loss: 2.411709189414978\n",
      "Batch 465,  loss: 2.5656266689300535\n",
      "Batch 470,  loss: 2.748034644126892\n",
      "Batch 475,  loss: 3.3231693744659423\n",
      "Batch 480,  loss: 2.9092777729034425\n",
      "Batch 485,  loss: 2.6587496757507325\n",
      "Batch 490,  loss: 2.667629337310791\n",
      "Batch 495,  loss: 2.2926465034484864\n",
      "Batch 500,  loss: 2.408888339996338\n",
      "Batch 505,  loss: 2.8505056619644167\n",
      "Batch 510,  loss: 2.6995195150375366\n",
      "Batch 515,  loss: 2.597285270690918\n",
      "Batch 520,  loss: 2.3886144161224365\n",
      "Batch 525,  loss: 2.936225175857544\n",
      "Batch 530,  loss: 2.8146317481994627\n",
      "Batch 535,  loss: 2.7369202613830566\n",
      "Batch 540,  loss: 2.107043242454529\n",
      "Batch 545,  loss: 2.1434919595718385\n",
      "Batch 550,  loss: 2.6738450050354006\n",
      "Batch 555,  loss: 1.9664068698883057\n",
      "Batch 560,  loss: 2.492909240722656\n",
      "Batch 565,  loss: 2.2794958353042603\n",
      "Batch 570,  loss: 2.356789493560791\n",
      "Batch 575,  loss: 2.616749310493469\n",
      "Batch 580,  loss: 2.5877935886383057\n",
      "Batch 585,  loss: 2.708456206321716\n",
      "Batch 590,  loss: 1.8640565156936646\n",
      "Batch 595,  loss: 2.6423176765441894\n",
      "Batch 600,  loss: 2.4353177070617678\n",
      "Batch 605,  loss: 2.8903075218200684\n",
      "Batch 610,  loss: 2.660614037513733\n",
      "Batch 615,  loss: 2.790909194946289\n",
      "Batch 620,  loss: 2.805789756774902\n",
      "Batch 625,  loss: 2.6957655906677247\n",
      "Batch 630,  loss: 2.696638584136963\n",
      "Batch 635,  loss: 2.616673135757446\n",
      "Batch 640,  loss: 2.534321403503418\n",
      "Batch 645,  loss: 3.4335922241210937\n",
      "Batch 650,  loss: 3.065753173828125\n",
      "Batch 655,  loss: 2.4204076290130616\n",
      "Batch 660,  loss: 3.26757550239563\n",
      "Batch 665,  loss: 2.4507817149162294\n",
      "Batch 670,  loss: 2.3135330438613892\n",
      "Batch 675,  loss: 3.3070350170135496\n",
      "Batch 680,  loss: 2.5630390644073486\n",
      "Batch 685,  loss: 2.5859015703201296\n",
      "Batch 690,  loss: 2.5784628868103026\n",
      "Batch 695,  loss: 3.2069895267486572\n",
      "Batch 700,  loss: 2.577892208099365\n",
      "Batch 705,  loss: 2.355081868171692\n",
      "Batch 710,  loss: 3.198785972595215\n",
      "Batch 715,  loss: 2.1739193201065063\n",
      "Batch 720,  loss: 2.8819973468780518\n",
      "Batch 725,  loss: 2.6253992080688477\n",
      "Batch 730,  loss: 2.389758253097534\n",
      "Batch 735,  loss: 2.833599328994751\n",
      "Batch 740,  loss: 2.4559590101242064\n",
      "Batch 745,  loss: 2.2061615705490114\n",
      "Batch 750,  loss: 2.523731327056885\n",
      "Batch 755,  loss: 2.259449291229248\n",
      "Batch 760,  loss: 2.327333092689514\n",
      "Batch 765,  loss: 2.9976101398468016\n",
      "Batch 770,  loss: 2.549411749839783\n",
      "Batch 775,  loss: 2.498258352279663\n",
      "Batch 780,  loss: 2.6057730674743653\n",
      "Batch 785,  loss: 2.781346845626831\n",
      "Batch 790,  loss: 2.5368985652923586\n",
      "Batch 795,  loss: 2.7228471994400025\n",
      "Batch 800,  loss: 2.78574652671814\n",
      "Batch 805,  loss: 3.186920738220215\n",
      "Batch 810,  loss: 2.893458938598633\n",
      "Batch 815,  loss: 2.6742200374603273\n",
      "Batch 820,  loss: 2.1523521184921264\n",
      "Batch 825,  loss: 2.2201650142669678\n",
      "Batch 830,  loss: 2.9856457710266113\n",
      "Batch 835,  loss: 2.751072549819946\n",
      "Batch 840,  loss: 2.4024389028549193\n",
      "Batch 845,  loss: 3.411863374710083\n",
      "Batch 850,  loss: 2.534213733673096\n",
      "Batch 855,  loss: 2.3915188550949096\n",
      "Batch 860,  loss: 2.6788799285888674\n",
      "Batch 865,  loss: 3.1302239418029787\n",
      "Batch 870,  loss: 2.275948429107666\n",
      "Batch 875,  loss: 2.5413289785385134\n",
      "Batch 880,  loss: 2.315234088897705\n",
      "Batch 885,  loss: 2.5735855102539062\n",
      "Batch 890,  loss: 2.746467447280884\n",
      "Batch 895,  loss: 2.480713057518005\n",
      "Batch 900,  loss: 2.8647482872009276\n",
      "Batch 905,  loss: 3.3646034955978394\n",
      "Batch 910,  loss: 2.024111032485962\n",
      "Batch 915,  loss: 2.4226392269134522\n",
      "Batch 920,  loss: 2.734028697013855\n",
      "Batch 925,  loss: 2.0728511810302734\n",
      "Batch 930,  loss: 2.946561813354492\n",
      "Batch 935,  loss: 2.3003572463989257\n",
      "Batch 940,  loss: 2.4762642621994018\n",
      "Batch 945,  loss: 2.6826727390289307\n",
      "Batch 950,  loss: 3.0253819465637206\n",
      "Batch 955,  loss: 1.958887529373169\n",
      "Batch 960,  loss: 2.752858304977417\n",
      "Batch 965,  loss: 2.398973798751831\n",
      "Batch 970,  loss: 2.7191844463348387\n",
      "Batch 975,  loss: 2.0435561895370484\n",
      "Batch 980,  loss: 2.2026339530944825\n",
      "Batch 985,  loss: 2.7322716236114504\n",
      "Batch 990,  loss: 3.0262943744659423\n",
      "Batch 995,  loss: 2.5362585306167604\n",
      "Batch 1000,  loss: 2.5760047912597654\n",
      "Batch 1005,  loss: 2.6376861572265624\n",
      "Batch 1010,  loss: 2.4284745693206786\n",
      "Batch 1015,  loss: 2.2499176502227782\n",
      "Batch 1020,  loss: 2.826517653465271\n",
      "Batch 1025,  loss: 3.0298229694366454\n",
      "Batch 1030,  loss: 2.150686502456665\n",
      "Batch 1035,  loss: 2.9531476497650146\n",
      "Batch 1040,  loss: 2.458315062522888\n",
      "Batch 1045,  loss: 2.8210216522216798\n",
      "Batch 1050,  loss: 2.7109519958496096\n",
      "Batch 1055,  loss: 2.2937244176864624\n",
      "Batch 1060,  loss: 2.6191794157028196\n",
      "Batch 1065,  loss: 2.844627523422241\n",
      "Batch 1070,  loss: 2.7698469161987305\n",
      "Batch 1075,  loss: 2.923280429840088\n",
      "Batch 1080,  loss: 2.3043980598449707\n",
      "Batch 1085,  loss: 2.795674443244934\n",
      "Batch 1090,  loss: 2.412050652503967\n",
      "Batch 1095,  loss: 2.6127154350280763\n",
      "Batch 1100,  loss: 2.270405340194702\n",
      "Batch 1105,  loss: 2.3492173671722414\n",
      "Batch 1110,  loss: 2.40925874710083\n",
      "Batch 1115,  loss: 2.213163208961487\n",
      "Batch 1120,  loss: 2.848900556564331\n",
      "Batch 1125,  loss: 1.9790627479553222\n",
      "Batch 1130,  loss: 1.856108021736145\n",
      "Batch 1135,  loss: 2.2973704099655152\n",
      "Batch 1140,  loss: 2.613634777069092\n",
      "Batch 1145,  loss: 2.9492682933807375\n",
      "Batch 1150,  loss: 2.7226967811584473\n",
      "Batch 1155,  loss: 2.436292362213135\n",
      "Batch 1160,  loss: 2.486682105064392\n",
      "Batch 1165,  loss: 2.421155405044556\n",
      "Batch 1170,  loss: 1.880574607849121\n",
      "Batch 1175,  loss: 2.8030048847198485\n",
      "Batch 1180,  loss: 2.929616928100586\n",
      "Batch 1185,  loss: 2.4187865018844605\n",
      "Batch 1190,  loss: 2.493346643447876\n",
      "Batch 1195,  loss: 2.1327118396759035\n",
      "Batch 1200,  loss: 2.664256191253662\n",
      "Batch 1205,  loss: 1.9929394245147705\n",
      "Batch 1210,  loss: 2.458353590965271\n",
      "Batch 1215,  loss: 3.0858155250549317\n",
      "Batch 1220,  loss: 2.9520657777786257\n",
      "Batch 1225,  loss: 2.4474388122558595\n",
      "Batch 1230,  loss: 2.106087160110474\n",
      "Batch 1235,  loss: 2.684592914581299\n",
      "Batch 1240,  loss: 2.5072346687316895\n",
      "Batch 1245,  loss: 2.454452562332153\n",
      "Batch 1250,  loss: 2.2369221687316894\n",
      "Batch 1255,  loss: 3.5601573467254637\n",
      "Batch 1260,  loss: 2.630312991142273\n",
      "Batch 1265,  loss: 2.2327786445617677\n",
      "Batch 1270,  loss: 2.5152655363082888\n",
      "Batch 1275,  loss: 2.6529797554016112\n",
      "Batch 1280,  loss: 2.7310359716415404\n",
      "Batch 1285,  loss: 2.3190562248229982\n",
      "Batch 1290,  loss: 2.6383589029312136\n",
      "Batch 1295,  loss: 2.168233048915863\n",
      "LOSS train 2.168233048915863. Validation loss: 2.9511887127188623 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 11:\n",
      "Batch 5,  loss: 2.2833035469055174\n",
      "Batch 10,  loss: 2.7178174018859864\n",
      "Batch 15,  loss: 2.108999514579773\n",
      "Batch 20,  loss: 2.085903453826904\n",
      "Batch 25,  loss: 3.068559455871582\n",
      "Batch 30,  loss: 2.2666470050811767\n",
      "Batch 35,  loss: 2.992690658569336\n",
      "Batch 40,  loss: 2.6143392086029054\n",
      "Batch 45,  loss: 2.644672155380249\n",
      "Batch 50,  loss: 3.414007806777954\n",
      "Batch 55,  loss: 3.1752060413360597\n",
      "Batch 60,  loss: 2.992415976524353\n",
      "Batch 65,  loss: 2.756918120384216\n",
      "Batch 70,  loss: 2.151013445854187\n",
      "Batch 75,  loss: 3.06001992225647\n",
      "Batch 80,  loss: 2.7045744895935058\n",
      "Batch 85,  loss: 2.7041285037994385\n",
      "Batch 90,  loss: 3.2971248865127563\n",
      "Batch 95,  loss: 2.618523931503296\n",
      "Batch 100,  loss: 2.2089266061782835\n",
      "Batch 105,  loss: 2.055660057067871\n",
      "Batch 110,  loss: 2.2619908332824705\n",
      "Batch 115,  loss: 2.5981196641921995\n",
      "Batch 120,  loss: 2.283536696434021\n",
      "Batch 125,  loss: 1.9022029876708983\n",
      "Batch 130,  loss: 2.5455296993255616\n",
      "Batch 135,  loss: 2.152825665473938\n",
      "Batch 140,  loss: 2.3944990396499635\n",
      "Batch 145,  loss: 2.44678373336792\n",
      "Batch 150,  loss: 2.357390260696411\n",
      "Batch 155,  loss: 2.75242600440979\n",
      "Batch 160,  loss: 2.277959680557251\n",
      "Batch 165,  loss: 2.0909252643585203\n",
      "Batch 170,  loss: 2.1097537517547607\n",
      "Batch 175,  loss: 2.6817553758621218\n",
      "Batch 180,  loss: 2.523954224586487\n",
      "Batch 185,  loss: 2.8470184326171877\n",
      "Batch 190,  loss: 3.267149066925049\n",
      "Batch 195,  loss: 2.255898857116699\n",
      "Batch 200,  loss: 3.3390156269073485\n",
      "Batch 205,  loss: 2.167647624015808\n",
      "Batch 210,  loss: 2.299919033050537\n",
      "Batch 215,  loss: 2.1892467975616454\n",
      "Batch 220,  loss: 2.8914162635803224\n",
      "Batch 225,  loss: 2.930466556549072\n",
      "Batch 230,  loss: 2.473384141921997\n",
      "Batch 235,  loss: 2.5222816944122313\n",
      "Batch 240,  loss: 2.905161952972412\n",
      "Batch 245,  loss: 2.1293999671936037\n",
      "Batch 250,  loss: 2.7918830633163454\n",
      "Batch 255,  loss: 2.538534736633301\n",
      "Batch 260,  loss: 2.562972903251648\n",
      "Batch 265,  loss: 2.4437501192092896\n",
      "Batch 270,  loss: 3.0407230377197267\n",
      "Batch 275,  loss: 2.2973073720932007\n",
      "Batch 280,  loss: 2.0728068590164184\n",
      "Batch 285,  loss: 2.669478988647461\n",
      "Batch 290,  loss: 2.679622030258179\n",
      "Batch 295,  loss: 2.19491970539093\n",
      "Batch 300,  loss: 2.5139792919158936\n",
      "Batch 305,  loss: 2.8866834163665773\n",
      "Batch 310,  loss: 2.108526039123535\n",
      "Batch 315,  loss: 2.099504828453064\n",
      "Batch 320,  loss: 3.369484567642212\n",
      "Batch 325,  loss: 2.781091904640198\n",
      "Batch 330,  loss: 2.2361237525939943\n",
      "Batch 335,  loss: 2.6953547477722166\n",
      "Batch 340,  loss: 2.6007723569869996\n",
      "Batch 345,  loss: 2.5048492431640623\n",
      "Batch 350,  loss: 2.683498477935791\n",
      "Batch 355,  loss: 2.3612903118133546\n",
      "Batch 360,  loss: 2.586384725570679\n",
      "Batch 365,  loss: 2.8517432689666746\n",
      "Batch 370,  loss: 2.9444090366363525\n",
      "Batch 375,  loss: 2.5187736988067626\n",
      "Batch 380,  loss: 2.2635420083999636\n",
      "Batch 385,  loss: 1.97402184009552\n",
      "Batch 390,  loss: 2.6483893156051637\n",
      "Batch 395,  loss: 2.471072626113892\n",
      "Batch 400,  loss: 3.183383512496948\n",
      "Batch 405,  loss: 2.4018647193908693\n",
      "Batch 410,  loss: 2.4768256664276125\n",
      "Batch 415,  loss: 2.531013584136963\n",
      "Batch 420,  loss: 2.5721179246902466\n",
      "Batch 425,  loss: 3.0685136795043944\n",
      "Batch 430,  loss: 2.5803897619247436\n",
      "Batch 435,  loss: 2.2868521928787233\n",
      "Batch 440,  loss: 2.2844407081604006\n",
      "Batch 445,  loss: 1.6918803930282593\n",
      "Batch 450,  loss: 2.548976755142212\n",
      "Batch 455,  loss: 3.069533610343933\n",
      "Batch 460,  loss: 2.5239586353302004\n",
      "Batch 465,  loss: 2.643085169792175\n",
      "Batch 470,  loss: 2.4043493986129763\n",
      "Batch 475,  loss: 2.7623386859893797\n",
      "Batch 480,  loss: 2.425671195983887\n",
      "Batch 485,  loss: 2.2712446451187134\n",
      "Batch 490,  loss: 2.7091658115386963\n",
      "Batch 495,  loss: 2.23294358253479\n",
      "Batch 500,  loss: 2.5326725482940673\n",
      "Batch 505,  loss: 2.2198164463043213\n",
      "Batch 510,  loss: 2.927058792114258\n",
      "Batch 515,  loss: 3.0788650512695312\n",
      "Batch 520,  loss: 2.593807315826416\n",
      "Batch 525,  loss: 2.416648769378662\n",
      "Batch 530,  loss: 2.472401475906372\n",
      "Batch 535,  loss: 2.2175193309783934\n",
      "Batch 540,  loss: 2.0177281141281127\n",
      "Batch 545,  loss: 2.274633026123047\n",
      "Batch 550,  loss: 2.392279529571533\n",
      "Batch 555,  loss: 2.477793502807617\n",
      "Batch 560,  loss: 2.329121232032776\n",
      "Batch 565,  loss: 2.810424327850342\n",
      "Batch 570,  loss: 3.2210774421691895\n",
      "Batch 575,  loss: 2.8077627420425415\n",
      "Batch 580,  loss: 2.6420515537261964\n",
      "Batch 585,  loss: 2.3288931369781496\n",
      "Batch 590,  loss: 3.0676980018615723\n",
      "Batch 595,  loss: 1.9835335969924928\n",
      "Batch 600,  loss: 2.57146635055542\n",
      "Batch 605,  loss: 3.1302022457122805\n",
      "Batch 610,  loss: 2.1077361822128298\n",
      "Batch 615,  loss: 1.9710682868957519\n",
      "Batch 620,  loss: 2.7056019783020018\n",
      "Batch 625,  loss: 2.3501473903656005\n",
      "Batch 630,  loss: 2.529823350906372\n",
      "Batch 635,  loss: 2.5888174057006834\n",
      "Batch 640,  loss: 2.3508280754089355\n",
      "Batch 645,  loss: 2.4419174194335938\n",
      "Batch 650,  loss: 2.6566543102264406\n",
      "Batch 655,  loss: 2.7374329566955566\n",
      "Batch 660,  loss: 2.3643322467803953\n",
      "Batch 665,  loss: 2.7493967533111574\n",
      "Batch 670,  loss: 2.583447289466858\n",
      "Batch 675,  loss: 2.141544961929321\n",
      "Batch 680,  loss: 3.0902332782745363\n",
      "Batch 685,  loss: 2.40057635307312\n",
      "Batch 690,  loss: 2.6202048301696776\n",
      "Batch 695,  loss: 2.1700467586517336\n",
      "Batch 700,  loss: 2.7032639026641845\n",
      "Batch 705,  loss: 2.5896793842315673\n",
      "Batch 710,  loss: 2.463329315185547\n",
      "Batch 715,  loss: 2.3330955266952516\n",
      "Batch 720,  loss: 2.3767996311187742\n",
      "Batch 725,  loss: 2.464750146865845\n",
      "Batch 730,  loss: 2.3205323696136473\n",
      "Batch 735,  loss: 1.8510794162750244\n",
      "Batch 740,  loss: 2.9414942264556885\n",
      "Batch 745,  loss: 2.245123338699341\n",
      "Batch 750,  loss: 2.657108736038208\n",
      "Batch 755,  loss: 2.323988676071167\n",
      "Batch 760,  loss: 2.4661438941955565\n",
      "Batch 765,  loss: 2.1742425680160524\n",
      "Batch 770,  loss: 2.460242676734924\n",
      "Batch 775,  loss: 2.4180683135986327\n",
      "Batch 780,  loss: 1.9266759395599364\n",
      "Batch 785,  loss: 2.3470157861709593\n",
      "Batch 790,  loss: 2.5372018337249758\n",
      "Batch 795,  loss: 2.519078588485718\n",
      "Batch 800,  loss: 2.1319342613220216\n",
      "Batch 805,  loss: 2.378841257095337\n",
      "Batch 810,  loss: 2.8678457736968994\n",
      "Batch 815,  loss: 3.155359220504761\n",
      "Batch 820,  loss: 2.5676774263381956\n",
      "Batch 825,  loss: 2.7060092449188233\n",
      "Batch 830,  loss: 2.498448038101196\n",
      "Batch 835,  loss: 2.4742729663848877\n",
      "Batch 840,  loss: 2.010647773742676\n",
      "Batch 845,  loss: 2.481794309616089\n",
      "Batch 850,  loss: 2.754858112335205\n",
      "Batch 855,  loss: 2.4012728691101075\n",
      "Batch 860,  loss: 2.553652215003967\n",
      "Batch 865,  loss: 2.1760427713394166\n",
      "Batch 870,  loss: 2.279112148284912\n",
      "Batch 875,  loss: 2.2247018814086914\n",
      "Batch 880,  loss: 2.7201295852661134\n",
      "Batch 885,  loss: 2.682850384712219\n",
      "Batch 890,  loss: 2.836077356338501\n",
      "Batch 895,  loss: 2.5869583606719972\n",
      "Batch 900,  loss: 2.773292064666748\n",
      "Batch 905,  loss: 2.12822949886322\n",
      "Batch 910,  loss: 2.5525954246520994\n",
      "Batch 915,  loss: 2.4641552686691286\n",
      "Batch 920,  loss: 2.1679582595825195\n",
      "Batch 925,  loss: 2.823983383178711\n",
      "Batch 930,  loss: 2.2834171533584593\n",
      "Batch 935,  loss: 2.651612377166748\n",
      "Batch 940,  loss: 3.0346367835998533\n",
      "Batch 945,  loss: 2.3844324350357056\n",
      "Batch 950,  loss: 2.577812671661377\n",
      "Batch 955,  loss: 2.376699113845825\n",
      "Batch 960,  loss: 2.5345343112945558\n",
      "Batch 965,  loss: 2.737032151222229\n",
      "Batch 970,  loss: 2.194827938079834\n",
      "Batch 975,  loss: 2.344896125793457\n",
      "Batch 980,  loss: 1.755259346961975\n",
      "Batch 985,  loss: 2.3650838851928713\n",
      "Batch 990,  loss: 1.634561824798584\n",
      "Batch 995,  loss: 2.4792835235595705\n",
      "Batch 1000,  loss: 2.12290198802948\n",
      "Batch 1005,  loss: 2.7458537817001343\n",
      "Batch 1010,  loss: 2.877551960945129\n",
      "Batch 1015,  loss: 2.947891044616699\n",
      "Batch 1020,  loss: 2.514287328720093\n",
      "Batch 1025,  loss: 2.3817989587783814\n",
      "Batch 1030,  loss: 1.9376190900802612\n",
      "Batch 1035,  loss: 2.4815452098846436\n",
      "Batch 1040,  loss: 2.33338623046875\n",
      "Batch 1045,  loss: 2.886330413818359\n",
      "Batch 1050,  loss: 2.3896344661712647\n",
      "Batch 1055,  loss: 2.216283416748047\n",
      "Batch 1060,  loss: 2.526094913482666\n",
      "Batch 1065,  loss: 2.704355311393738\n",
      "Batch 1070,  loss: 3.0895429611206056\n",
      "Batch 1075,  loss: 2.613392448425293\n",
      "Batch 1080,  loss: 2.1432960271835326\n",
      "Batch 1085,  loss: 2.7139135122299196\n",
      "Batch 1090,  loss: 2.283919858932495\n",
      "Batch 1095,  loss: 2.622190165519714\n",
      "Batch 1100,  loss: 2.4864283561706544\n",
      "Batch 1105,  loss: 2.614454507827759\n",
      "Batch 1110,  loss: 2.6210288524627687\n",
      "Batch 1115,  loss: 2.0793310403823853\n",
      "Batch 1120,  loss: 2.303356170654297\n",
      "Batch 1125,  loss: 2.757451963424683\n",
      "Batch 1130,  loss: 2.5994726181030274\n",
      "Batch 1135,  loss: 2.428588032722473\n",
      "Batch 1140,  loss: 2.784630632400513\n",
      "Batch 1145,  loss: 2.167777919769287\n",
      "Batch 1150,  loss: 2.49830322265625\n",
      "Batch 1155,  loss: 2.6226906299591066\n",
      "Batch 1160,  loss: 3.014356517791748\n",
      "Batch 1165,  loss: 2.7962194681167603\n",
      "Batch 1170,  loss: 2.4346518993377684\n",
      "Batch 1175,  loss: 2.2717896461486817\n",
      "Batch 1180,  loss: 2.546760654449463\n",
      "Batch 1185,  loss: 2.887239933013916\n",
      "Batch 1190,  loss: 2.162722134590149\n",
      "Batch 1195,  loss: 2.5723991870880125\n",
      "Batch 1200,  loss: 2.1978423595428467\n",
      "Batch 1205,  loss: 2.441184163093567\n",
      "Batch 1210,  loss: 2.4261725187301635\n",
      "Batch 1215,  loss: 2.1971840381622316\n",
      "Batch 1220,  loss: 2.6754192113876343\n",
      "Batch 1225,  loss: 2.400939440727234\n",
      "Batch 1230,  loss: 2.277161979675293\n",
      "Batch 1235,  loss: 2.0368876218795777\n",
      "Batch 1240,  loss: 2.54211311340332\n",
      "Batch 1245,  loss: 2.3668591737747193\n",
      "Batch 1250,  loss: 2.247591519355774\n",
      "Batch 1255,  loss: 2.914771795272827\n",
      "Batch 1260,  loss: 2.592234992980957\n",
      "Batch 1265,  loss: 2.4639094829559327\n",
      "Batch 1270,  loss: 2.647403335571289\n",
      "Batch 1275,  loss: 2.84396071434021\n",
      "Batch 1280,  loss: 2.4432843685150147\n",
      "Batch 1285,  loss: 3.0830307483673094\n",
      "Batch 1290,  loss: 2.143445134162903\n",
      "Batch 1295,  loss: 2.1290938377380373\n",
      "LOSS train 2.1290938377380373. Validation loss: 2.863597223804229 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 12:\n",
      "Batch 5,  loss: 2.3809216976165772\n",
      "Batch 10,  loss: 2.5478437423706053\n",
      "Batch 15,  loss: 2.1574557304382322\n",
      "Batch 20,  loss: 2.2969781398773192\n",
      "Batch 25,  loss: 2.609815216064453\n",
      "Batch 30,  loss: 2.563824510574341\n",
      "Batch 35,  loss: 2.478981685638428\n",
      "Batch 40,  loss: 2.4408483028411867\n",
      "Batch 45,  loss: 2.5439576625823976\n",
      "Batch 50,  loss: 3.479211187362671\n",
      "Batch 55,  loss: 2.1385765075683594\n",
      "Batch 60,  loss: 2.4570932388305664\n",
      "Batch 65,  loss: 1.896297526359558\n",
      "Batch 70,  loss: 1.984034013748169\n",
      "Batch 75,  loss: 2.2073100090026854\n",
      "Batch 80,  loss: 2.3414914131164553\n",
      "Batch 85,  loss: 1.6192814588546753\n",
      "Batch 90,  loss: 2.8080276012420655\n",
      "Batch 95,  loss: 2.6968634605407713\n",
      "Batch 100,  loss: 2.367789888381958\n",
      "Batch 105,  loss: 2.1963253736495973\n",
      "Batch 110,  loss: 2.6394798278808596\n",
      "Batch 115,  loss: 2.540808391571045\n",
      "Batch 120,  loss: 2.376411557197571\n",
      "Batch 125,  loss: 2.4524587631225585\n",
      "Batch 130,  loss: 1.850144124031067\n",
      "Batch 135,  loss: 2.400664234161377\n",
      "Batch 140,  loss: 1.9800344467163087\n",
      "Batch 145,  loss: 2.2111042976379394\n",
      "Batch 150,  loss: 2.3383375644683837\n",
      "Batch 155,  loss: 2.2169853925704954\n",
      "Batch 160,  loss: 2.809047269821167\n",
      "Batch 165,  loss: 2.2194263458251955\n",
      "Batch 170,  loss: 2.810234308242798\n",
      "Batch 175,  loss: 2.488841199874878\n",
      "Batch 180,  loss: 2.8752168655395507\n",
      "Batch 185,  loss: 2.3262608528137205\n",
      "Batch 190,  loss: 2.854340934753418\n",
      "Batch 195,  loss: 1.9803759336471558\n",
      "Batch 200,  loss: 2.816830062866211\n",
      "Batch 205,  loss: 2.9906968593597414\n",
      "Batch 210,  loss: 2.6033763885498047\n",
      "Batch 215,  loss: 3.25540976524353\n",
      "Batch 220,  loss: 2.3910317420959473\n",
      "Batch 225,  loss: 2.862240362167358\n",
      "Batch 230,  loss: 2.9373111724853516\n",
      "Batch 235,  loss: 2.513789701461792\n",
      "Batch 240,  loss: 2.3590659379959105\n",
      "Batch 245,  loss: 2.7441806554794312\n",
      "Batch 250,  loss: 2.344282054901123\n",
      "Batch 255,  loss: 2.157497930526733\n",
      "Batch 260,  loss: 2.6619779109954833\n",
      "Batch 265,  loss: 2.4023311138153076\n",
      "Batch 270,  loss: 2.6280834674835205\n",
      "Batch 275,  loss: 1.7141274094581604\n",
      "Batch 280,  loss: 2.626687240600586\n",
      "Batch 285,  loss: 2.5077065944671633\n",
      "Batch 290,  loss: 2.2924890518188477\n",
      "Batch 295,  loss: 2.2723608732223513\n",
      "Batch 300,  loss: 2.6482945919036864\n",
      "Batch 305,  loss: 2.3303279876708984\n",
      "Batch 310,  loss: 2.568730282783508\n",
      "Batch 315,  loss: 2.743568515777588\n",
      "Batch 320,  loss: 3.3759581089019775\n",
      "Batch 325,  loss: 2.5727912902832033\n",
      "Batch 330,  loss: 2.5464492559432985\n",
      "Batch 335,  loss: 1.7968370914459229\n",
      "Batch 340,  loss: 2.609493088722229\n",
      "Batch 345,  loss: 2.5016502380371093\n",
      "Batch 350,  loss: 2.1495739698410032\n",
      "Batch 355,  loss: 2.282516431808472\n",
      "Batch 360,  loss: 1.917131519317627\n",
      "Batch 365,  loss: 2.4066344261169434\n",
      "Batch 370,  loss: 2.704449939727783\n",
      "Batch 375,  loss: 2.2930593252182008\n",
      "Batch 380,  loss: 2.394086241722107\n",
      "Batch 385,  loss: 2.200510931015015\n",
      "Batch 390,  loss: 2.433540201187134\n",
      "Batch 395,  loss: 2.928435182571411\n",
      "Batch 400,  loss: 2.248559761047363\n",
      "Batch 405,  loss: 2.9283421993255616\n",
      "Batch 410,  loss: 2.053486633300781\n",
      "Batch 415,  loss: 2.3235032081604006\n",
      "Batch 420,  loss: 2.4647926807403566\n",
      "Batch 425,  loss: 2.4752994537353517\n",
      "Batch 430,  loss: 2.409204292297363\n",
      "Batch 435,  loss: 2.3110721826553347\n",
      "Batch 440,  loss: 2.3423304080963137\n",
      "Batch 445,  loss: 2.525267744064331\n",
      "Batch 450,  loss: 2.6435957193374633\n",
      "Batch 455,  loss: 1.9910657167434693\n",
      "Batch 460,  loss: 2.6200254440307615\n",
      "Batch 465,  loss: 2.5961559295654295\n",
      "Batch 470,  loss: 2.180070626735687\n",
      "Batch 475,  loss: 2.07858407497406\n",
      "Batch 480,  loss: 2.5161165237426757\n",
      "Batch 485,  loss: 2.3184762239456176\n",
      "Batch 490,  loss: 2.1230052709579468\n",
      "Batch 495,  loss: 2.0973913192749025\n",
      "Batch 500,  loss: 2.3080665111541747\n",
      "Batch 505,  loss: 2.079788255691528\n",
      "Batch 510,  loss: 2.3929689168930053\n",
      "Batch 515,  loss: 1.9505371570587158\n",
      "Batch 520,  loss: 2.459720420837402\n",
      "Batch 525,  loss: 2.124379873275757\n",
      "Batch 530,  loss: 2.8837900161743164\n",
      "Batch 535,  loss: 2.2045145988464356\n",
      "Batch 540,  loss: 2.4530478000640867\n",
      "Batch 545,  loss: 1.7114722728729248\n",
      "Batch 550,  loss: 2.586463737487793\n",
      "Batch 555,  loss: 2.385307478904724\n",
      "Batch 560,  loss: 2.2319908857345583\n",
      "Batch 565,  loss: 2.180831789970398\n",
      "Batch 570,  loss: 2.3758858680725097\n",
      "Batch 575,  loss: 1.8356589317321776\n",
      "Batch 580,  loss: 2.499550294876099\n",
      "Batch 585,  loss: 2.805218553543091\n",
      "Batch 590,  loss: 3.1416633129119873\n",
      "Batch 595,  loss: 2.559770107269287\n",
      "Batch 600,  loss: 2.7593574047088625\n",
      "Batch 605,  loss: 2.23190598487854\n",
      "Batch 610,  loss: 2.5834095001220705\n",
      "Batch 615,  loss: 2.2211554765701296\n",
      "Batch 620,  loss: 2.0949952363967896\n",
      "Batch 625,  loss: 2.1743518352508544\n",
      "Batch 630,  loss: 2.419942593574524\n",
      "Batch 635,  loss: 2.7187731742858885\n",
      "Batch 640,  loss: 1.733036184310913\n",
      "Batch 645,  loss: 2.355309247970581\n",
      "Batch 650,  loss: 3.069626069068909\n",
      "Batch 655,  loss: 2.115637516975403\n",
      "Batch 660,  loss: 2.411432647705078\n",
      "Batch 665,  loss: 2.2566306591033936\n",
      "Batch 670,  loss: 2.556950330734253\n",
      "Batch 675,  loss: 2.3618174314498903\n",
      "Batch 680,  loss: 2.084564781188965\n",
      "Batch 685,  loss: 2.478669023513794\n",
      "Batch 690,  loss: 1.8681426286697387\n",
      "Batch 695,  loss: 2.8314693927764893\n",
      "Batch 700,  loss: 2.0250277519226074\n",
      "Batch 705,  loss: 2.8607085227966307\n",
      "Batch 710,  loss: 3.115625333786011\n",
      "Batch 715,  loss: 2.267290449142456\n",
      "Batch 720,  loss: 2.3915157318115234\n",
      "Batch 725,  loss: 2.6024896621704103\n",
      "Batch 730,  loss: 2.5160701274871826\n",
      "Batch 735,  loss: 2.4807002544403076\n",
      "Batch 740,  loss: 2.313730335235596\n",
      "Batch 745,  loss: 1.628767466545105\n",
      "Batch 750,  loss: 1.7493376016616822\n",
      "Batch 755,  loss: 2.63073616027832\n",
      "Batch 760,  loss: 2.4109960556030274\n",
      "Batch 765,  loss: 2.074998641014099\n",
      "Batch 770,  loss: 2.1160131454467774\n",
      "Batch 775,  loss: 3.213665819168091\n",
      "Batch 780,  loss: 2.2956616401672365\n",
      "Batch 785,  loss: 1.937612271308899\n",
      "Batch 790,  loss: 2.1996970415115356\n",
      "Batch 795,  loss: 2.3106125593185425\n",
      "Batch 800,  loss: 2.9042513847351072\n",
      "Batch 805,  loss: 2.521561098098755\n",
      "Batch 810,  loss: 2.4990910291671753\n",
      "Batch 815,  loss: 2.58493058681488\n",
      "Batch 820,  loss: 2.508734107017517\n",
      "Batch 825,  loss: 2.1870009660720826\n",
      "Batch 830,  loss: 2.5627989530563355\n",
      "Batch 835,  loss: 1.8966589212417602\n",
      "Batch 840,  loss: 2.4111480474472047\n",
      "Batch 845,  loss: 2.501464509963989\n",
      "Batch 850,  loss: 2.1621933698654177\n",
      "Batch 855,  loss: 2.594115877151489\n",
      "Batch 860,  loss: 2.485229730606079\n",
      "Batch 865,  loss: 2.0475507974624634\n",
      "Batch 870,  loss: 2.10146644115448\n",
      "Batch 875,  loss: 2.165498876571655\n",
      "Batch 880,  loss: 2.536913537979126\n",
      "Batch 885,  loss: 2.3899006843566895\n",
      "Batch 890,  loss: 2.862236166000366\n",
      "Batch 895,  loss: 2.6060760021209717\n",
      "Batch 900,  loss: 2.9298614025115968\n",
      "Batch 905,  loss: 2.3606078147888185\n",
      "Batch 910,  loss: 2.040197658538818\n",
      "Batch 915,  loss: 2.8045960903167724\n",
      "Batch 920,  loss: 2.4611218690872194\n",
      "Batch 925,  loss: 2.5223764896392824\n",
      "Batch 930,  loss: 2.291284465789795\n",
      "Batch 935,  loss: 2.869268703460693\n",
      "Batch 940,  loss: 2.7047502279281614\n",
      "Batch 945,  loss: 2.1200686931610107\n",
      "Batch 950,  loss: 2.967213344573975\n",
      "Batch 955,  loss: 1.9380892038345336\n",
      "Batch 960,  loss: 2.7660874366760253\n",
      "Batch 965,  loss: 2.054408884048462\n",
      "Batch 970,  loss: 2.26963472366333\n",
      "Batch 975,  loss: 2.2008524417877195\n",
      "Batch 980,  loss: 2.548850584030151\n",
      "Batch 985,  loss: 1.7799659967422485\n",
      "Batch 990,  loss: 2.2585622549057005\n",
      "Batch 995,  loss: 2.2604918479919434\n",
      "Batch 1000,  loss: 2.398119640350342\n",
      "Batch 1005,  loss: 2.3129032135009764\n",
      "Batch 1010,  loss: 2.2785475969314577\n",
      "Batch 1015,  loss: 2.4319169998168944\n",
      "Batch 1020,  loss: 2.7429648637771606\n",
      "Batch 1025,  loss: 2.0917414903640745\n",
      "Batch 1030,  loss: 2.4657400369644167\n",
      "Batch 1035,  loss: 2.7897621631622314\n",
      "Batch 1040,  loss: 1.9620014429092407\n",
      "Batch 1045,  loss: 2.2159557819366453\n",
      "Batch 1050,  loss: 2.0485373735427856\n",
      "Batch 1055,  loss: 2.235807180404663\n",
      "Batch 1060,  loss: 1.5572889804840089\n",
      "Batch 1065,  loss: 1.8303142786026\n",
      "Batch 1070,  loss: 2.465098190307617\n",
      "Batch 1075,  loss: 2.866804313659668\n",
      "Batch 1080,  loss: 1.7700535535812378\n",
      "Batch 1085,  loss: 2.9833221435546875\n",
      "Batch 1090,  loss: 1.8445273399353028\n",
      "Batch 1095,  loss: 2.6769125699996947\n",
      "Batch 1100,  loss: 2.5544878244400024\n",
      "Batch 1105,  loss: 2.105868101119995\n",
      "Batch 1110,  loss: 2.357134485244751\n",
      "Batch 1115,  loss: 2.241532301902771\n",
      "Batch 1120,  loss: 2.809867286682129\n",
      "Batch 1125,  loss: 2.030665922164917\n",
      "Batch 1130,  loss: 2.0173571825027468\n",
      "Batch 1135,  loss: 2.660205769538879\n",
      "Batch 1140,  loss: 2.2753432273864744\n",
      "Batch 1145,  loss: 2.7279706954956056\n",
      "Batch 1150,  loss: 2.5843432426452635\n",
      "Batch 1155,  loss: 2.4603841304779053\n",
      "Batch 1160,  loss: 2.43948655128479\n",
      "Batch 1165,  loss: 2.2768634080886843\n",
      "Batch 1170,  loss: 2.687752437591553\n",
      "Batch 1175,  loss: 2.4428691387176515\n",
      "Batch 1180,  loss: 3.091659116744995\n",
      "Batch 1185,  loss: 2.4194034576416015\n",
      "Batch 1190,  loss: 2.0362693071365356\n",
      "Batch 1195,  loss: 2.4604918003082275\n",
      "Batch 1200,  loss: 2.276352643966675\n",
      "Batch 1205,  loss: 2.3266470193862916\n",
      "Batch 1210,  loss: 2.1557582139968874\n",
      "Batch 1215,  loss: 2.29512882232666\n",
      "Batch 1220,  loss: 2.8820096492767333\n",
      "Batch 1225,  loss: 2.7952021598815917\n",
      "Batch 1230,  loss: 2.315780687332153\n",
      "Batch 1235,  loss: 2.2990673780441284\n",
      "Batch 1240,  loss: 1.8993446350097656\n",
      "Batch 1245,  loss: 2.4801326513290407\n",
      "Batch 1250,  loss: 1.9209276676177978\n",
      "Batch 1255,  loss: 2.4291622638702393\n",
      "Batch 1260,  loss: 2.738846015930176\n",
      "Batch 1265,  loss: 2.9829061031341553\n",
      "Batch 1270,  loss: 2.3221298694610595\n",
      "Batch 1275,  loss: 3.0513697147369383\n",
      "Batch 1280,  loss: 2.6145932197570803\n",
      "Batch 1285,  loss: 2.730613899230957\n",
      "Batch 1290,  loss: 1.4848597288131713\n",
      "Batch 1295,  loss: 2.6757153034210206\n",
      "LOSS train 2.6757153034210206. Validation loss: 2.7957386097974246 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 13:\n",
      "Batch 5,  loss: 2.366088056564331\n",
      "Batch 10,  loss: 2.2650667428970337\n",
      "Batch 15,  loss: 2.4083902835845947\n",
      "Batch 20,  loss: 2.456961822509766\n",
      "Batch 25,  loss: 2.3619672298431396\n",
      "Batch 30,  loss: 2.2109509468078614\n",
      "Batch 35,  loss: 2.756686305999756\n",
      "Batch 40,  loss: 2.683721971511841\n",
      "Batch 45,  loss: 2.017933654785156\n",
      "Batch 50,  loss: 2.225811743736267\n",
      "Batch 55,  loss: 2.401368522644043\n",
      "Batch 60,  loss: 2.04297559261322\n",
      "Batch 65,  loss: 2.4825013637542725\n",
      "Batch 70,  loss: 2.2580984830856323\n",
      "Batch 75,  loss: 2.4712141275405886\n",
      "Batch 80,  loss: 2.2431333541870115\n",
      "Batch 85,  loss: 2.104130744934082\n",
      "Batch 90,  loss: 2.045108366012573\n",
      "Batch 95,  loss: 2.308212065696716\n",
      "Batch 100,  loss: 2.472577714920044\n",
      "Batch 105,  loss: 2.3018269538879395\n",
      "Batch 110,  loss: 2.5280205965042115\n",
      "Batch 115,  loss: 2.173863172531128\n",
      "Batch 120,  loss: 2.5087323904037477\n",
      "Batch 125,  loss: 2.3196099281311033\n",
      "Batch 130,  loss: 1.8647120475769043\n",
      "Batch 135,  loss: 1.8645906686782836\n",
      "Batch 140,  loss: 2.6115008354187013\n",
      "Batch 145,  loss: 2.636546802520752\n",
      "Batch 150,  loss: 2.426730918884277\n",
      "Batch 155,  loss: 2.009644794464111\n",
      "Batch 160,  loss: 2.18021297454834\n",
      "Batch 165,  loss: 2.604385161399841\n",
      "Batch 170,  loss: 2.6964135646820067\n",
      "Batch 175,  loss: 2.355530023574829\n",
      "Batch 180,  loss: 2.475346326828003\n",
      "Batch 185,  loss: 2.7420730113983156\n",
      "Batch 190,  loss: 2.3657532691955567\n",
      "Batch 195,  loss: 2.188479948043823\n",
      "Batch 200,  loss: 2.3021054744720457\n",
      "Batch 205,  loss: 2.367524003982544\n",
      "Batch 210,  loss: 2.3009044408798216\n",
      "Batch 215,  loss: 2.397815132141113\n",
      "Batch 220,  loss: 2.46287317276001\n",
      "Batch 225,  loss: 2.5553844928741456\n",
      "Batch 230,  loss: 2.3336541652679443\n",
      "Batch 235,  loss: 3.134008693695068\n",
      "Batch 240,  loss: 2.296331024169922\n",
      "Batch 245,  loss: 2.601580500602722\n",
      "Batch 250,  loss: 1.8995127201080322\n",
      "Batch 255,  loss: 2.377458095550537\n",
      "Batch 260,  loss: 1.8327308654785157\n",
      "Batch 265,  loss: 2.4260258436203004\n",
      "Batch 270,  loss: 2.462038850784302\n",
      "Batch 275,  loss: 1.9096096515655518\n",
      "Batch 280,  loss: 2.6219808578491213\n",
      "Batch 285,  loss: 2.473787784576416\n",
      "Batch 290,  loss: 2.272913408279419\n",
      "Batch 295,  loss: 2.1480589151382445\n",
      "Batch 300,  loss: 2.817541408538818\n",
      "Batch 305,  loss: 2.817391204833984\n",
      "Batch 310,  loss: 2.0116113662719726\n",
      "Batch 315,  loss: 2.0703122138977053\n",
      "Batch 320,  loss: 2.3745684146881105\n",
      "Batch 325,  loss: 2.706034278869629\n",
      "Batch 330,  loss: 2.144511008262634\n",
      "Batch 335,  loss: 2.3964381217956543\n",
      "Batch 340,  loss: 2.497178316116333\n",
      "Batch 345,  loss: 2.5933066606521606\n",
      "Batch 350,  loss: 2.157699370384216\n",
      "Batch 355,  loss: 2.4316471099853514\n",
      "Batch 360,  loss: 2.317289113998413\n",
      "Batch 365,  loss: 2.0339553356170654\n",
      "Batch 370,  loss: 2.15229172706604\n",
      "Batch 375,  loss: 2.6180162906646727\n",
      "Batch 380,  loss: 2.4427704572677613\n",
      "Batch 385,  loss: 2.3154584884643556\n",
      "Batch 390,  loss: 2.3798707008361815\n",
      "Batch 395,  loss: 2.387101411819458\n",
      "Batch 400,  loss: 1.8073378801345825\n",
      "Batch 405,  loss: 2.6211350917816163\n",
      "Batch 410,  loss: 2.1862138748168944\n",
      "Batch 415,  loss: 2.362305498123169\n",
      "Batch 420,  loss: 2.731325149536133\n",
      "Batch 425,  loss: 2.0851035833358766\n",
      "Batch 430,  loss: 2.297856330871582\n",
      "Batch 435,  loss: 1.9339387893676758\n",
      "Batch 440,  loss: 2.1690498113632204\n",
      "Batch 445,  loss: 2.1304656267166138\n",
      "Batch 450,  loss: 2.249073028564453\n",
      "Batch 455,  loss: 2.0046083450317385\n",
      "Batch 460,  loss: 2.246417927742004\n",
      "Batch 465,  loss: 2.210401010513306\n",
      "Batch 470,  loss: 2.363853669166565\n",
      "Batch 475,  loss: 2.8030542373657226\n",
      "Batch 480,  loss: 2.3632857322692873\n",
      "Batch 485,  loss: 2.546141338348389\n",
      "Batch 490,  loss: 2.524887752532959\n",
      "Batch 495,  loss: 2.5556214094161986\n",
      "Batch 500,  loss: 2.4961478471755982\n",
      "Batch 505,  loss: 2.3163868188858032\n",
      "Batch 510,  loss: 2.245320701599121\n",
      "Batch 515,  loss: 2.3600035667419434\n",
      "Batch 520,  loss: 2.2655677318573\n",
      "Batch 525,  loss: 2.2855025053024294\n",
      "Batch 530,  loss: 2.439017724990845\n",
      "Batch 535,  loss: 2.5853435039520263\n",
      "Batch 540,  loss: 1.7966378211975098\n",
      "Batch 545,  loss: 2.579511880874634\n",
      "Batch 550,  loss: 2.5394194602966307\n",
      "Batch 555,  loss: 2.198767590522766\n",
      "Batch 560,  loss: 2.2196874618530273\n",
      "Batch 565,  loss: 2.686189365386963\n",
      "Batch 570,  loss: 2.0714104890823366\n",
      "Batch 575,  loss: 2.2835010290145874\n",
      "Batch 580,  loss: 2.1191956281661986\n",
      "Batch 585,  loss: 2.437586212158203\n",
      "Batch 590,  loss: 1.9317222356796264\n",
      "Batch 595,  loss: 1.8555096864700318\n",
      "Batch 600,  loss: 1.942229199409485\n",
      "Batch 605,  loss: 2.374735188484192\n",
      "Batch 610,  loss: 2.0776654720306396\n",
      "Batch 615,  loss: 2.751517486572266\n",
      "Batch 620,  loss: 2.713714098930359\n",
      "Batch 625,  loss: 2.018002486228943\n",
      "Batch 630,  loss: 2.2674959659576417\n",
      "Batch 635,  loss: 2.5988341331481934\n",
      "Batch 640,  loss: 2.3510164499282835\n",
      "Batch 645,  loss: 2.634799909591675\n",
      "Batch 650,  loss: 2.337734413146973\n",
      "Batch 655,  loss: 1.9424904346466065\n",
      "Batch 660,  loss: 1.7104840993881225\n",
      "Batch 665,  loss: 2.482967400550842\n",
      "Batch 670,  loss: 2.314637041091919\n",
      "Batch 675,  loss: 2.0454315662384035\n",
      "Batch 680,  loss: 2.9566134214401245\n",
      "Batch 685,  loss: 2.1571876287460325\n",
      "Batch 690,  loss: 2.4362621545791625\n",
      "Batch 695,  loss: 2.569004774093628\n",
      "Batch 700,  loss: 2.866232442855835\n",
      "Batch 705,  loss: 2.5198136806488036\n",
      "Batch 710,  loss: 2.6055896043777467\n",
      "Batch 715,  loss: 2.577048397064209\n",
      "Batch 720,  loss: 2.108470845222473\n",
      "Batch 725,  loss: 2.693170928955078\n",
      "Batch 730,  loss: 1.9571510791778564\n",
      "Batch 735,  loss: 2.1090267419815065\n",
      "Batch 740,  loss: 2.0677509784698485\n",
      "Batch 745,  loss: 2.359145402908325\n",
      "Batch 750,  loss: 2.127568006515503\n",
      "Batch 755,  loss: 2.5638269662857054\n",
      "Batch 760,  loss: 2.0593154430389404\n",
      "Batch 765,  loss: 1.981447958946228\n",
      "Batch 770,  loss: 2.110683488845825\n",
      "Batch 775,  loss: 2.478460168838501\n",
      "Batch 780,  loss: 1.6324908256530761\n",
      "Batch 785,  loss: 2.036967396736145\n",
      "Batch 790,  loss: 2.592806839942932\n",
      "Batch 795,  loss: 2.240189456939697\n",
      "Batch 800,  loss: 2.0661199569702147\n",
      "Batch 805,  loss: 2.3467936277389527\n",
      "Batch 810,  loss: 2.531059002876282\n",
      "Batch 815,  loss: 1.9294092893600463\n",
      "Batch 820,  loss: 2.419231724739075\n",
      "Batch 825,  loss: 2.396177053451538\n",
      "Batch 830,  loss: 2.8253549575805663\n",
      "Batch 835,  loss: 2.0261799097061157\n",
      "Batch 840,  loss: 2.5472340822219848\n",
      "Batch 845,  loss: 2.119952654838562\n",
      "Batch 850,  loss: 2.294894480705261\n",
      "Batch 855,  loss: 2.1127061367034914\n",
      "Batch 860,  loss: 1.8999722242355346\n",
      "Batch 865,  loss: 2.270363116264343\n",
      "Batch 870,  loss: 2.151577281951904\n",
      "Batch 875,  loss: 1.9373945951461793\n",
      "Batch 880,  loss: 2.0141690254211424\n",
      "Batch 885,  loss: 1.8088547229766845\n",
      "Batch 890,  loss: 2.788496398925781\n",
      "Batch 895,  loss: 2.3463046550750732\n",
      "Batch 900,  loss: 2.271235132217407\n",
      "Batch 905,  loss: 2.3185178518295286\n",
      "Batch 910,  loss: 2.3073674201965333\n",
      "Batch 915,  loss: 2.4227900981903074\n",
      "Batch 920,  loss: 2.5425365924835206\n",
      "Batch 925,  loss: 2.1067018032073976\n",
      "Batch 930,  loss: 2.291364002227783\n",
      "Batch 935,  loss: 2.33971848487854\n",
      "Batch 940,  loss: 2.0142654180526733\n",
      "Batch 945,  loss: 2.001876783370972\n",
      "Batch 950,  loss: 1.802737545967102\n",
      "Batch 955,  loss: 2.222830867767334\n",
      "Batch 960,  loss: 2.468431234359741\n",
      "Batch 965,  loss: 2.1243783950805666\n",
      "Batch 970,  loss: 2.1076632738113403\n",
      "Batch 975,  loss: 2.1340609312057497\n",
      "Batch 980,  loss: 2.860848379135132\n",
      "Batch 985,  loss: 2.1928184986114503\n",
      "Batch 990,  loss: 2.158256196975708\n",
      "Batch 995,  loss: 2.5246678829193114\n",
      "Batch 1000,  loss: 2.146577334403992\n",
      "Batch 1005,  loss: 2.4318555355072022\n",
      "Batch 1010,  loss: 2.1875941276550295\n",
      "Batch 1015,  loss: 2.052184557914734\n",
      "Batch 1020,  loss: 2.931886339187622\n",
      "Batch 1025,  loss: 2.3298790216445924\n",
      "Batch 1030,  loss: 2.412704420089722\n",
      "Batch 1035,  loss: 2.5755541801452635\n",
      "Batch 1040,  loss: 2.528337812423706\n",
      "Batch 1045,  loss: 2.0762459993362428\n",
      "Batch 1050,  loss: 2.2136730670928957\n",
      "Batch 1055,  loss: 2.3304634571075438\n",
      "Batch 1060,  loss: 1.7983680963516235\n",
      "Batch 1065,  loss: 2.4025126695632935\n",
      "Batch 1070,  loss: 2.4249542713165284\n",
      "Batch 1075,  loss: 1.9791342735290527\n",
      "Batch 1080,  loss: 3.046069383621216\n",
      "Batch 1085,  loss: 2.4263123989105226\n",
      "Batch 1090,  loss: 2.06378231048584\n",
      "Batch 1095,  loss: 2.790112113952637\n",
      "Batch 1100,  loss: 2.157787322998047\n",
      "Batch 1105,  loss: 2.8513277530670167\n",
      "Batch 1110,  loss: 2.5816235542297363\n",
      "Batch 1115,  loss: 1.9366097927093506\n",
      "Batch 1120,  loss: 2.495807719230652\n",
      "Batch 1125,  loss: 2.2008639335632325\n",
      "Batch 1130,  loss: 1.7848634243011474\n",
      "Batch 1135,  loss: 2.17147843837738\n",
      "Batch 1140,  loss: 1.909514570236206\n",
      "Batch 1145,  loss: 2.0368605613708497\n",
      "Batch 1150,  loss: 2.771707630157471\n",
      "Batch 1155,  loss: 2.7877405881881714\n",
      "Batch 1160,  loss: 2.0299007415771486\n",
      "Batch 1165,  loss: 1.9544545412063599\n",
      "Batch 1170,  loss: 1.9113723754882812\n",
      "Batch 1175,  loss: 1.7873546838760377\n",
      "Batch 1180,  loss: 2.171761679649353\n",
      "Batch 1185,  loss: 2.20947802066803\n",
      "Batch 1190,  loss: 2.6788755893707275\n",
      "Batch 1195,  loss: 2.1827561616897584\n",
      "Batch 1200,  loss: 2.821815013885498\n",
      "Batch 1205,  loss: 2.155913734436035\n",
      "Batch 1210,  loss: 2.4997962474823\n",
      "Batch 1215,  loss: 1.891101884841919\n",
      "Batch 1220,  loss: 2.0703784465789794\n",
      "Batch 1225,  loss: 2.182434105873108\n",
      "Batch 1230,  loss: 2.3659178733825685\n",
      "Batch 1235,  loss: 1.9575040578842162\n",
      "Batch 1240,  loss: 1.8745212793350219\n",
      "Batch 1245,  loss: 1.9638177633285523\n",
      "Batch 1250,  loss: 2.653562307357788\n",
      "Batch 1255,  loss: 2.0251233339309693\n",
      "Batch 1260,  loss: 1.8641866207122804\n",
      "Batch 1265,  loss: 1.9509848356246948\n",
      "Batch 1270,  loss: 2.2923127174377442\n",
      "Batch 1275,  loss: 2.311990737915039\n",
      "Batch 1280,  loss: 2.295112895965576\n",
      "Batch 1285,  loss: 1.7007715940475463\n",
      "Batch 1290,  loss: 2.6496601581573485\n",
      "Batch 1295,  loss: 2.9335885524749754\n",
      "LOSS train 2.9335885524749754. Validation loss: 2.7396639622823353 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 14:\n",
      "Batch 5,  loss: 2.450076150894165\n",
      "Batch 10,  loss: 2.8239342212677\n",
      "Batch 15,  loss: 2.492284631729126\n",
      "Batch 20,  loss: 2.352285885810852\n",
      "Batch 25,  loss: 2.4109780550003053\n",
      "Batch 30,  loss: 1.9816312551498414\n",
      "Batch 35,  loss: 2.5617117404937746\n",
      "Batch 40,  loss: 1.9909279584884643\n",
      "Batch 45,  loss: 2.1622763156890867\n",
      "Batch 50,  loss: 2.1937420845031737\n",
      "Batch 55,  loss: 2.575134611129761\n",
      "Batch 60,  loss: 2.155960726737976\n",
      "Batch 65,  loss: 2.4041515588760376\n",
      "Batch 70,  loss: 2.6370468378067016\n",
      "Batch 75,  loss: 2.4746801376342775\n",
      "Batch 80,  loss: 1.6326691865921021\n",
      "Batch 85,  loss: 2.165371894836426\n",
      "Batch 90,  loss: 1.8044351577758788\n",
      "Batch 95,  loss: 2.4236066818237303\n",
      "Batch 100,  loss: 2.076123261451721\n",
      "Batch 105,  loss: 2.1215033531188965\n",
      "Batch 110,  loss: 2.2609195709228516\n",
      "Batch 115,  loss: 2.308918261528015\n",
      "Batch 120,  loss: 2.5000330924987795\n",
      "Batch 125,  loss: 2.061128854751587\n",
      "Batch 130,  loss: 2.211667466163635\n",
      "Batch 135,  loss: 2.2260481834411623\n",
      "Batch 140,  loss: 2.543969821929932\n",
      "Batch 145,  loss: 1.903745460510254\n",
      "Batch 150,  loss: 2.5657779455184935\n",
      "Batch 155,  loss: 2.110849928855896\n",
      "Batch 160,  loss: 2.428318452835083\n",
      "Batch 165,  loss: 1.9484508037567139\n",
      "Batch 170,  loss: 1.6877287864685058\n",
      "Batch 175,  loss: 2.1020196199417116\n",
      "Batch 180,  loss: 2.0048892736434936\n",
      "Batch 185,  loss: 2.250250005722046\n",
      "Batch 190,  loss: 2.368135356903076\n",
      "Batch 195,  loss: 2.34505410194397\n",
      "Batch 200,  loss: 2.0773376703262327\n",
      "Batch 205,  loss: 2.4752111434936523\n",
      "Batch 210,  loss: 1.9320760011672973\n",
      "Batch 215,  loss: 1.8879037857055665\n",
      "Batch 220,  loss: 2.144428181648254\n",
      "Batch 225,  loss: 2.457287836074829\n",
      "Batch 230,  loss: 2.257921051979065\n",
      "Batch 235,  loss: 1.77947998046875\n",
      "Batch 240,  loss: 2.5938257217407226\n",
      "Batch 245,  loss: 2.160730981826782\n",
      "Batch 250,  loss: 2.477419114112854\n",
      "Batch 255,  loss: 2.164760398864746\n",
      "Batch 260,  loss: 2.497441291809082\n",
      "Batch 265,  loss: 2.0870906352996825\n",
      "Batch 270,  loss: 1.9898228645324707\n",
      "Batch 275,  loss: 2.2832560539245605\n",
      "Batch 280,  loss: 2.2383406162261963\n",
      "Batch 285,  loss: 2.669885587692261\n",
      "Batch 290,  loss: 1.9568691968917846\n",
      "Batch 295,  loss: 2.384678030014038\n",
      "Batch 300,  loss: 2.1816797256469727\n",
      "Batch 305,  loss: 2.1868136882781983\n",
      "Batch 310,  loss: 2.031565284729004\n",
      "Batch 315,  loss: 2.339625692367554\n",
      "Batch 320,  loss: 2.081849455833435\n",
      "Batch 325,  loss: 2.3063485383987428\n",
      "Batch 330,  loss: 1.938184928894043\n",
      "Batch 335,  loss: 2.432447338104248\n",
      "Batch 340,  loss: 1.910527729988098\n",
      "Batch 345,  loss: 2.850683307647705\n",
      "Batch 350,  loss: 2.3880818843841554\n",
      "Batch 355,  loss: 1.763269031047821\n",
      "Batch 360,  loss: 2.636534094810486\n",
      "Batch 365,  loss: 2.4015805959701537\n",
      "Batch 370,  loss: 2.339907646179199\n",
      "Batch 375,  loss: 2.0325814962387083\n",
      "Batch 380,  loss: 1.8980108737945556\n",
      "Batch 385,  loss: 1.8130013942718506\n",
      "Batch 390,  loss: 2.116105556488037\n",
      "Batch 395,  loss: 2.0637869358062746\n",
      "Batch 400,  loss: 2.013485074043274\n",
      "Batch 405,  loss: 1.9784701108932494\n",
      "Batch 410,  loss: 1.860945701599121\n",
      "Batch 415,  loss: 1.7869070768356323\n",
      "Batch 420,  loss: 2.1734643936157227\n",
      "Batch 425,  loss: 2.0451398611068727\n",
      "Batch 430,  loss: 2.639576292037964\n",
      "Batch 435,  loss: 2.450051021575928\n",
      "Batch 440,  loss: 2.441484522819519\n",
      "Batch 445,  loss: 2.5485990047454834\n",
      "Batch 450,  loss: 2.2406869411468504\n",
      "Batch 455,  loss: 2.2108556747436525\n",
      "Batch 460,  loss: 2.3314068794250487\n",
      "Batch 465,  loss: 2.438278388977051\n",
      "Batch 470,  loss: 1.6140696048736571\n",
      "Batch 475,  loss: 2.3393632888793947\n",
      "Batch 480,  loss: 2.905774998664856\n",
      "Batch 485,  loss: 1.7615795850753784\n",
      "Batch 490,  loss: 2.105857563018799\n",
      "Batch 495,  loss: 2.390922260284424\n",
      "Batch 500,  loss: 2.512089395523071\n",
      "Batch 505,  loss: 2.2126123189926146\n",
      "Batch 510,  loss: 1.9704896211624146\n",
      "Batch 515,  loss: 2.483957123756409\n",
      "Batch 520,  loss: 2.0337082862854006\n",
      "Batch 525,  loss: 1.799027109146118\n",
      "Batch 530,  loss: 2.715394067764282\n",
      "Batch 535,  loss: 1.9545037746429443\n",
      "Batch 540,  loss: 1.81810564994812\n",
      "Batch 545,  loss: 2.1983625888824463\n",
      "Batch 550,  loss: 1.9527952432632447\n",
      "Batch 555,  loss: 2.5429247617721558\n",
      "Batch 560,  loss: 1.9032591104507446\n",
      "Batch 565,  loss: 2.486749219894409\n",
      "Batch 570,  loss: 2.494983696937561\n",
      "Batch 575,  loss: 2.2621772050857545\n",
      "Batch 580,  loss: 2.5310545921325684\n",
      "Batch 585,  loss: 2.6471266746520996\n",
      "Batch 590,  loss: 2.431603121757507\n",
      "Batch 595,  loss: 2.3441786766052246\n",
      "Batch 600,  loss: 2.01829195022583\n",
      "Batch 605,  loss: 2.3213767766952516\n",
      "Batch 610,  loss: 2.4012998580932616\n",
      "Batch 615,  loss: 1.7892853021621704\n",
      "Batch 620,  loss: 2.0595471620559693\n",
      "Batch 625,  loss: 2.1754766702651978\n",
      "Batch 630,  loss: 2.1193180799484255\n",
      "Batch 635,  loss: 2.6833405017852785\n",
      "Batch 640,  loss: 2.1315842628479005\n",
      "Batch 645,  loss: 2.072759437561035\n",
      "Batch 650,  loss: 1.8585222721099854\n",
      "Batch 655,  loss: 1.858008623123169\n",
      "Batch 660,  loss: 1.8761503934860229\n",
      "Batch 665,  loss: 1.8930870771408081\n",
      "Batch 670,  loss: 1.9573620080947876\n",
      "Batch 675,  loss: 2.1411070585250855\n",
      "Batch 680,  loss: 2.1011839151382445\n",
      "Batch 685,  loss: 2.321529722213745\n",
      "Batch 690,  loss: 2.137231731414795\n",
      "Batch 695,  loss: 2.248738431930542\n",
      "Batch 700,  loss: 2.366414284706116\n",
      "Batch 705,  loss: 2.286952495574951\n",
      "Batch 710,  loss: 2.304430389404297\n",
      "Batch 715,  loss: 2.690154767036438\n",
      "Batch 720,  loss: 1.8953145265579223\n",
      "Batch 725,  loss: 2.115860199928284\n",
      "Batch 730,  loss: 2.120249533653259\n",
      "Batch 735,  loss: 2.790091609954834\n",
      "Batch 740,  loss: 2.476682996749878\n",
      "Batch 745,  loss: 1.6286879301071167\n",
      "Batch 750,  loss: 2.4795515537261963\n",
      "Batch 755,  loss: 1.9456250190734863\n",
      "Batch 760,  loss: 2.262882423400879\n",
      "Batch 765,  loss: 2.652769088745117\n",
      "Batch 770,  loss: 2.4500975370407105\n",
      "Batch 775,  loss: 2.7889958381652833\n",
      "Batch 780,  loss: 2.693345308303833\n",
      "Batch 785,  loss: 2.5586470127105714\n",
      "Batch 790,  loss: 2.6072733402252197\n",
      "Batch 795,  loss: 1.9548564672470092\n",
      "Batch 800,  loss: 2.1488706827163697\n",
      "Batch 805,  loss: 2.4778064250946046\n",
      "Batch 810,  loss: 2.24807345867157\n",
      "Batch 815,  loss: 2.1469138860702515\n",
      "Batch 820,  loss: 2.1847825765609743\n",
      "Batch 825,  loss: 2.453796648979187\n",
      "Batch 830,  loss: 1.6059047341346742\n",
      "Batch 835,  loss: 2.203137016296387\n",
      "Batch 840,  loss: 2.5121206283569335\n",
      "Batch 845,  loss: 1.7421213626861571\n",
      "Batch 850,  loss: 3.317909860610962\n",
      "Batch 855,  loss: 2.513558101654053\n",
      "Batch 860,  loss: 2.1090604066848755\n",
      "Batch 865,  loss: 1.9159356117248536\n",
      "Batch 870,  loss: 1.9354133605957031\n",
      "Batch 875,  loss: 2.1463578224182127\n",
      "Batch 880,  loss: 2.660343885421753\n",
      "Batch 885,  loss: 2.1708303451538087\n",
      "Batch 890,  loss: 1.9934482336044312\n",
      "Batch 895,  loss: 2.1119929552078247\n",
      "Batch 900,  loss: 2.356168580055237\n",
      "Batch 905,  loss: 1.972105574607849\n",
      "Batch 910,  loss: 2.2885035276412964\n",
      "Batch 915,  loss: 2.0549323558807373\n",
      "Batch 920,  loss: 2.0669567346572877\n",
      "Batch 925,  loss: 1.7449963808059692\n",
      "Batch 930,  loss: 2.032672810554504\n",
      "Batch 935,  loss: 2.3734585285186767\n",
      "Batch 940,  loss: 2.092830204963684\n",
      "Batch 945,  loss: 2.044311022758484\n",
      "Batch 950,  loss: 1.9074074983596803\n",
      "Batch 955,  loss: 2.1723916292190553\n",
      "Batch 960,  loss: 2.521396112442017\n",
      "Batch 965,  loss: 1.7600618600845337\n",
      "Batch 970,  loss: 2.015124559402466\n",
      "Batch 975,  loss: 2.3692490577697756\n",
      "Batch 980,  loss: 2.760960006713867\n",
      "Batch 985,  loss: 1.9899701595306396\n",
      "Batch 990,  loss: 2.033973979949951\n",
      "Batch 995,  loss: 2.2490269899368287\n",
      "Batch 1000,  loss: 2.3016430139541626\n",
      "Batch 1005,  loss: 2.2158507347106933\n",
      "Batch 1010,  loss: 2.311169147491455\n",
      "Batch 1015,  loss: 2.7301669120788574\n",
      "Batch 1020,  loss: 2.400728702545166\n",
      "Batch 1025,  loss: 1.9555745601654053\n",
      "Batch 1030,  loss: 2.4218749523162844\n",
      "Batch 1035,  loss: 2.3199800729751585\n",
      "Batch 1040,  loss: 2.382096195220947\n",
      "Batch 1045,  loss: 2.0739795207977294\n",
      "Batch 1050,  loss: 2.3642986536026003\n",
      "Batch 1055,  loss: 2.1034825801849366\n",
      "Batch 1060,  loss: 1.9348843336105346\n",
      "Batch 1065,  loss: 1.853502058982849\n",
      "Batch 1070,  loss: 2.634443664550781\n",
      "Batch 1075,  loss: 1.7890541791915893\n",
      "Batch 1080,  loss: 2.1063530921936033\n",
      "Batch 1085,  loss: 2.2251155853271483\n",
      "Batch 1090,  loss: 2.5047794818878173\n",
      "Batch 1095,  loss: 2.524340009689331\n",
      "Batch 1100,  loss: 1.9568741083145142\n",
      "Batch 1105,  loss: 2.006043791770935\n",
      "Batch 1110,  loss: 2.214878702163696\n",
      "Batch 1115,  loss: 2.824692964553833\n",
      "Batch 1120,  loss: 2.471937894821167\n",
      "Batch 1125,  loss: 2.0856893062591553\n",
      "Batch 1130,  loss: 1.986213493347168\n",
      "Batch 1135,  loss: 2.551290678977966\n",
      "Batch 1140,  loss: 2.419897937774658\n",
      "Batch 1145,  loss: 2.1855148792266847\n",
      "Batch 1150,  loss: 2.4084029674530028\n",
      "Batch 1155,  loss: 2.7044845819473267\n",
      "Batch 1160,  loss: 2.425781011581421\n",
      "Batch 1165,  loss: 2.03589403629303\n",
      "Batch 1170,  loss: 2.3375600576400757\n",
      "Batch 1175,  loss: 2.1205914497375487\n",
      "Batch 1180,  loss: 2.030080270767212\n",
      "Batch 1185,  loss: 1.7672931909561158\n",
      "Batch 1190,  loss: 3.0171273708343507\n",
      "Batch 1195,  loss: 2.559696412086487\n",
      "Batch 1200,  loss: 2.2577728748321535\n",
      "Batch 1205,  loss: 1.8899974822998047\n",
      "Batch 1210,  loss: 2.3863271474838257\n",
      "Batch 1215,  loss: 2.3081660985946657\n",
      "Batch 1220,  loss: 1.7672192811965943\n",
      "Batch 1225,  loss: 1.9988535404205323\n",
      "Batch 1230,  loss: 2.0435870409011843\n",
      "Batch 1235,  loss: 1.7453694343566895\n",
      "Batch 1240,  loss: 1.9620154619216919\n",
      "Batch 1245,  loss: 2.0898935556411744\n",
      "Batch 1250,  loss: 2.4438589096069334\n",
      "Batch 1255,  loss: 1.9918814420700073\n",
      "Batch 1260,  loss: 2.0149898767471313\n",
      "Batch 1265,  loss: 2.302581763267517\n",
      "Batch 1270,  loss: 2.66755051612854\n",
      "Batch 1275,  loss: 2.0429443597793577\n",
      "Batch 1280,  loss: 2.154857873916626\n",
      "Batch 1285,  loss: 2.4648035287857057\n",
      "Batch 1290,  loss: 2.219131278991699\n",
      "Batch 1295,  loss: 2.257697582244873\n",
      "LOSS train 2.257697582244873. Validation loss: 2.5083358028144747 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 15:\n",
      "Batch 5,  loss: 2.1477420806884764\n",
      "Batch 10,  loss: 2.361765170097351\n",
      "Batch 15,  loss: 1.7710174322128296\n",
      "Batch 20,  loss: 2.432761716842651\n",
      "Batch 25,  loss: 2.6216469287872313\n",
      "Batch 30,  loss: 1.9735360860824585\n",
      "Batch 35,  loss: 2.023255801200867\n",
      "Batch 40,  loss: 1.6011026263237\n",
      "Batch 45,  loss: 2.000236439704895\n",
      "Batch 50,  loss: 2.2067317247390745\n",
      "Batch 55,  loss: 2.0149230003356933\n",
      "Batch 60,  loss: 1.9225577354431151\n",
      "Batch 65,  loss: 1.8767447471618652\n",
      "Batch 70,  loss: 1.478077507019043\n",
      "Batch 75,  loss: 2.2344868898391725\n",
      "Batch 80,  loss: 2.628826856613159\n",
      "Batch 85,  loss: 2.1525310277938843\n",
      "Batch 90,  loss: 2.1705812931060793\n",
      "Batch 95,  loss: 2.028597378730774\n",
      "Batch 100,  loss: 2.0467149019241333\n",
      "Batch 105,  loss: 1.9386449098587035\n",
      "Batch 110,  loss: 1.5079962730407714\n",
      "Batch 115,  loss: 2.7323649883270265\n",
      "Batch 120,  loss: 2.1522424697875975\n",
      "Batch 125,  loss: 1.8657272577285766\n",
      "Batch 130,  loss: 2.3002369165420533\n",
      "Batch 135,  loss: 1.6267080783843995\n",
      "Batch 140,  loss: 2.363138723373413\n",
      "Batch 145,  loss: 2.1253054618835447\n",
      "Batch 150,  loss: 2.3493775606155394\n",
      "Batch 155,  loss: 2.415031623840332\n",
      "Batch 160,  loss: 2.2003340482711793\n",
      "Batch 165,  loss: 2.3291956424713134\n",
      "Batch 170,  loss: 2.002568578720093\n",
      "Batch 175,  loss: 2.351759171485901\n",
      "Batch 180,  loss: 1.8086403369903565\n",
      "Batch 185,  loss: 1.6693817377090454\n",
      "Batch 190,  loss: 2.380048656463623\n",
      "Batch 195,  loss: 2.36672043800354\n",
      "Batch 200,  loss: 2.5005442619323732\n",
      "Batch 205,  loss: 2.0961727142333983\n",
      "Batch 210,  loss: 2.3343249559402466\n",
      "Batch 215,  loss: 2.3900506019592287\n",
      "Batch 220,  loss: 2.2222373247146607\n",
      "Batch 225,  loss: 2.4307706356048584\n",
      "Batch 230,  loss: 2.47971715927124\n",
      "Batch 235,  loss: 1.962047529220581\n",
      "Batch 240,  loss: 2.4738890171051025\n",
      "Batch 245,  loss: 2.2375319480895994\n",
      "Batch 250,  loss: 3.2518452644348144\n",
      "Batch 255,  loss: 2.004255175590515\n",
      "Batch 260,  loss: 2.0486228704452514\n",
      "Batch 265,  loss: 2.731116771697998\n",
      "Batch 270,  loss: 2.0441082239151003\n",
      "Batch 275,  loss: 1.9385226249694825\n",
      "Batch 280,  loss: 2.4788530111312865\n",
      "Batch 285,  loss: 1.85766441822052\n",
      "Batch 290,  loss: 2.367373514175415\n",
      "Batch 295,  loss: 1.791775608062744\n",
      "Batch 300,  loss: 2.345024299621582\n",
      "Batch 305,  loss: 2.0970507144927977\n",
      "Batch 310,  loss: 1.93433735370636\n",
      "Batch 315,  loss: 2.0079329013824463\n",
      "Batch 320,  loss: 2.527060627937317\n",
      "Batch 325,  loss: 2.027307558059692\n",
      "Batch 330,  loss: 1.796021819114685\n",
      "Batch 335,  loss: 2.3989165782928468\n",
      "Batch 340,  loss: 2.2943691968917848\n",
      "Batch 345,  loss: 2.088570809364319\n",
      "Batch 350,  loss: 2.313297963142395\n",
      "Batch 355,  loss: 1.7457252025604248\n",
      "Batch 360,  loss: 2.2649354457855226\n",
      "Batch 365,  loss: 1.791556751728058\n",
      "Batch 370,  loss: 2.300681233406067\n",
      "Batch 375,  loss: 1.9081373929977417\n",
      "Batch 380,  loss: 1.8735968589782714\n",
      "Batch 385,  loss: 1.7493297100067138\n",
      "Batch 390,  loss: 1.9997883319854737\n",
      "Batch 395,  loss: 2.3176653146743775\n",
      "Batch 400,  loss: 2.016064929962158\n",
      "Batch 405,  loss: 1.6853464841842651\n",
      "Batch 410,  loss: 2.2495642423629763\n",
      "Batch 415,  loss: 2.178682041168213\n",
      "Batch 420,  loss: 1.6480823516845704\n",
      "Batch 425,  loss: 2.0272186994552612\n",
      "Batch 430,  loss: 2.125685214996338\n",
      "Batch 435,  loss: 2.220467281341553\n",
      "Batch 440,  loss: 1.7328819751739502\n",
      "Batch 445,  loss: 2.149202656745911\n",
      "Batch 450,  loss: 2.5833223819732667\n",
      "Batch 455,  loss: 1.9521499395370483\n",
      "Batch 460,  loss: 2.9169151067733763\n",
      "Batch 465,  loss: 2.5532862186431884\n",
      "Batch 470,  loss: 1.865540623664856\n",
      "Batch 475,  loss: 2.4776613235473635\n",
      "Batch 480,  loss: 2.2966640472412108\n",
      "Batch 485,  loss: 2.2348015785217283\n",
      "Batch 490,  loss: 1.7528787136077881\n",
      "Batch 495,  loss: 2.271654510498047\n",
      "Batch 500,  loss: 2.098828887939453\n",
      "Batch 505,  loss: 1.9293420553207397\n",
      "Batch 510,  loss: 1.6826768159866332\n",
      "Batch 515,  loss: 2.1590553283691407\n",
      "Batch 520,  loss: 2.266889476776123\n",
      "Batch 525,  loss: 2.2044891119003296\n",
      "Batch 530,  loss: 2.1161256313323973\n",
      "Batch 535,  loss: 1.7970917701721192\n",
      "Batch 540,  loss: 2.124557280540466\n",
      "Batch 545,  loss: 2.5190905332565308\n",
      "Batch 550,  loss: 2.5819534063339233\n",
      "Batch 555,  loss: 2.302277445793152\n",
      "Batch 560,  loss: 2.2669331312179564\n",
      "Batch 565,  loss: 2.132757139205933\n",
      "Batch 570,  loss: 2.80146586894989\n",
      "Batch 575,  loss: 2.0597469091415403\n",
      "Batch 580,  loss: 2.051148533821106\n",
      "Batch 585,  loss: 1.9831389904022216\n",
      "Batch 590,  loss: 1.8117563247680664\n",
      "Batch 595,  loss: 1.7612126350402832\n",
      "Batch 600,  loss: 2.339084005355835\n",
      "Batch 605,  loss: 2.044519329071045\n",
      "Batch 610,  loss: 2.7023481845855715\n",
      "Batch 615,  loss: 2.067873501777649\n",
      "Batch 620,  loss: 2.2673213481903076\n",
      "Batch 625,  loss: 1.9974555492401123\n",
      "Batch 630,  loss: 2.5603299140930176\n",
      "Batch 635,  loss: 2.065967488288879\n",
      "Batch 640,  loss: 2.072711372375488\n",
      "Batch 645,  loss: 2.192687177658081\n",
      "Batch 650,  loss: 2.3671895980834963\n",
      "Batch 655,  loss: 1.7757413148880006\n",
      "Batch 660,  loss: 2.2070738077163696\n",
      "Batch 665,  loss: 2.1925504922866823\n",
      "Batch 670,  loss: 2.0148574829101564\n",
      "Batch 675,  loss: 2.308219480514526\n",
      "Batch 680,  loss: 2.2477186918258667\n",
      "Batch 685,  loss: 2.013919544219971\n",
      "Batch 690,  loss: 1.77071270942688\n",
      "Batch 695,  loss: 2.196548271179199\n",
      "Batch 700,  loss: 2.1117048263549805\n",
      "Batch 705,  loss: 2.6216830730438234\n",
      "Batch 710,  loss: 2.326836585998535\n",
      "Batch 715,  loss: 1.8604090213775635\n",
      "Batch 720,  loss: 2.3239377975463866\n",
      "Batch 725,  loss: 1.6968709468841552\n",
      "Batch 730,  loss: 2.788237524032593\n",
      "Batch 735,  loss: 2.4168043613433836\n",
      "Batch 740,  loss: 2.309397745132446\n",
      "Batch 745,  loss: 2.0726133823394775\n",
      "Batch 750,  loss: 1.9786581516265869\n",
      "Batch 755,  loss: 1.8604743242263795\n",
      "Batch 760,  loss: 2.2138835906982424\n",
      "Batch 765,  loss: 1.544209361076355\n",
      "Batch 770,  loss: 2.0122392058372496\n",
      "Batch 775,  loss: 2.0351892471313477\n",
      "Batch 780,  loss: 2.459811782836914\n",
      "Batch 785,  loss: 2.5052085399627684\n",
      "Batch 790,  loss: 1.883915090560913\n",
      "Batch 795,  loss: 2.0220797061920166\n",
      "Batch 800,  loss: 1.757539486885071\n",
      "Batch 805,  loss: 2.5824242353439333\n",
      "Batch 810,  loss: 2.678925037384033\n",
      "Batch 815,  loss: 2.2066280364990236\n",
      "Batch 820,  loss: 2.219079303741455\n",
      "Batch 825,  loss: 1.959346055984497\n",
      "Batch 830,  loss: 2.0025913238525392\n",
      "Batch 835,  loss: 2.0651854038238526\n",
      "Batch 840,  loss: 2.2944000244140623\n",
      "Batch 845,  loss: 1.7609909296035766\n",
      "Batch 850,  loss: 2.055752086639404\n",
      "Batch 855,  loss: 2.281950354576111\n",
      "Batch 860,  loss: 2.407197666168213\n",
      "Batch 865,  loss: 2.2515803813934325\n",
      "Batch 870,  loss: 2.587126302719116\n",
      "Batch 875,  loss: 2.5592566967010497\n",
      "Batch 880,  loss: 2.114349031448364\n",
      "Batch 885,  loss: 1.8803292036056518\n",
      "Batch 890,  loss: 2.0654587268829347\n",
      "Batch 895,  loss: 1.6405983209609984\n",
      "Batch 900,  loss: 2.0310171365737917\n",
      "Batch 905,  loss: 1.7869593381881714\n",
      "Batch 910,  loss: 2.3961095333099367\n",
      "Batch 915,  loss: 1.8797377109527589\n",
      "Batch 920,  loss: 2.1501256704330443\n",
      "Batch 925,  loss: 1.9403577089309691\n",
      "Batch 930,  loss: 2.1485147953033445\n",
      "Batch 935,  loss: 2.159307932853699\n",
      "Batch 940,  loss: 2.1741644144058228\n",
      "Batch 945,  loss: 2.240540361404419\n",
      "Batch 950,  loss: 2.782189702987671\n",
      "Batch 955,  loss: 2.0524197816848755\n",
      "Batch 960,  loss: 1.8782572984695434\n",
      "Batch 965,  loss: 2.006272554397583\n",
      "Batch 970,  loss: 2.023747706413269\n",
      "Batch 975,  loss: 1.9336797714233398\n",
      "Batch 980,  loss: 2.1336806535720827\n",
      "Batch 985,  loss: 2.3315830707550047\n",
      "Batch 990,  loss: 2.289489507675171\n",
      "Batch 995,  loss: 1.710175347328186\n",
      "Batch 1000,  loss: 2.2081008195877074\n",
      "Batch 1005,  loss: 2.045395088195801\n",
      "Batch 1010,  loss: 1.9796807765960693\n",
      "Batch 1015,  loss: 2.0301574945449827\n",
      "Batch 1020,  loss: 1.8339385986328125\n",
      "Batch 1025,  loss: 2.009102940559387\n",
      "Batch 1030,  loss: 2.153525447845459\n",
      "Batch 1035,  loss: 1.859285020828247\n",
      "Batch 1040,  loss: 2.150615191459656\n",
      "Batch 1045,  loss: 1.9886237859725953\n",
      "Batch 1050,  loss: 1.9938597917556762\n",
      "Batch 1055,  loss: 1.9474139928817749\n",
      "Batch 1060,  loss: 2.067802596092224\n",
      "Batch 1065,  loss: 2.2945058584213256\n",
      "Batch 1070,  loss: 1.8469748973846436\n",
      "Batch 1075,  loss: 2.2982813358306884\n",
      "Batch 1080,  loss: 2.0150834560394286\n",
      "Batch 1085,  loss: 2.007184076309204\n",
      "Batch 1090,  loss: 1.692503237724304\n",
      "Batch 1095,  loss: 2.2926105737686155\n",
      "Batch 1100,  loss: 2.0833173990249634\n",
      "Batch 1105,  loss: 2.0628551244735718\n",
      "Batch 1110,  loss: 1.8094460725784303\n",
      "Batch 1115,  loss: 2.3944727897644045\n",
      "Batch 1120,  loss: 2.4888210535049438\n",
      "Batch 1125,  loss: 2.2067850828170776\n",
      "Batch 1130,  loss: 1.8966521978378297\n",
      "Batch 1135,  loss: 2.542723059654236\n",
      "Batch 1140,  loss: 1.9757724761962892\n",
      "Batch 1145,  loss: 1.6742189407348633\n",
      "Batch 1150,  loss: 1.8572112560272216\n",
      "Batch 1155,  loss: 2.926016163825989\n",
      "Batch 1160,  loss: 2.1472083806991575\n",
      "Batch 1165,  loss: 2.1152745962142943\n",
      "Batch 1170,  loss: 1.881718897819519\n",
      "Batch 1175,  loss: 2.3890196800231935\n",
      "Batch 1180,  loss: 2.0165154933929443\n",
      "Batch 1185,  loss: 1.5727845668792724\n",
      "Batch 1190,  loss: 2.04107506275177\n",
      "Batch 1195,  loss: 2.3666099309921265\n",
      "Batch 1200,  loss: 1.9647889375686645\n",
      "Batch 1205,  loss: 2.530429410934448\n",
      "Batch 1210,  loss: 2.2826855182647705\n",
      "Batch 1215,  loss: 2.3257046222686766\n",
      "Batch 1220,  loss: 2.2236682891845705\n",
      "Batch 1225,  loss: 2.2345685005187987\n",
      "Batch 1230,  loss: 1.7010605573654174\n",
      "Batch 1235,  loss: 2.1443606853485107\n",
      "Batch 1240,  loss: 2.334988069534302\n",
      "Batch 1245,  loss: 1.8910666346549987\n",
      "Batch 1250,  loss: 1.756638503074646\n",
      "Batch 1255,  loss: 2.4781240463256835\n",
      "Batch 1260,  loss: 2.153229594230652\n",
      "Batch 1265,  loss: 2.255952501296997\n",
      "Batch 1270,  loss: 1.7131982564926147\n",
      "Batch 1275,  loss: 1.8583998918533324\n",
      "Batch 1280,  loss: 2.4270853757858277\n",
      "Batch 1285,  loss: 2.4940486907958985\n",
      "Batch 1290,  loss: 2.2260525226593018\n",
      "Batch 1295,  loss: 2.267014503479004\n",
      "LOSS train 2.267014503479004. Validation loss: 2.5002153758649475 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 16:\n",
      "Batch 5,  loss: 2.107017755508423\n",
      "Batch 10,  loss: 2.037081265449524\n",
      "Batch 15,  loss: 1.9225104093551635\n",
      "Batch 20,  loss: 1.9667929410934448\n",
      "Batch 25,  loss: 1.7141759157180787\n",
      "Batch 30,  loss: 2.2672053575515747\n",
      "Batch 35,  loss: 1.751425576210022\n",
      "Batch 40,  loss: 2.3622193574905395\n",
      "Batch 45,  loss: 1.7790270090103149\n",
      "Batch 50,  loss: 1.8215425968170167\n",
      "Batch 55,  loss: 2.048118996620178\n",
      "Batch 60,  loss: 2.193724179267883\n",
      "Batch 65,  loss: 2.609527587890625\n",
      "Batch 70,  loss: 2.039976716041565\n",
      "Batch 75,  loss: 1.9516754150390625\n",
      "Batch 80,  loss: 3.0257643699645995\n",
      "Batch 85,  loss: 1.763226318359375\n",
      "Batch 90,  loss: 2.121353578567505\n",
      "Batch 95,  loss: 2.226863694190979\n",
      "Batch 100,  loss: 2.195776915550232\n",
      "Batch 105,  loss: 2.2154953956604\n",
      "Batch 110,  loss: 1.9155077934265137\n",
      "Batch 115,  loss: 2.0052682161331177\n",
      "Batch 120,  loss: 2.091296148300171\n",
      "Batch 125,  loss: 2.1120375394821167\n",
      "Batch 130,  loss: 2.317906212806702\n",
      "Batch 135,  loss: 2.3882440328598022\n",
      "Batch 140,  loss: 2.004921889305115\n",
      "Batch 145,  loss: 1.6635223865509032\n",
      "Batch 150,  loss: 2.511015748977661\n",
      "Batch 155,  loss: 2.779513120651245\n",
      "Batch 160,  loss: 2.4616236686706543\n",
      "Batch 165,  loss: 2.0054647445678713\n",
      "Batch 170,  loss: 2.1754188776016234\n",
      "Batch 175,  loss: 2.0351975917816163\n",
      "Batch 180,  loss: 1.6832646608352662\n",
      "Batch 185,  loss: 2.017411231994629\n",
      "Batch 190,  loss: 2.257291841506958\n",
      "Batch 195,  loss: 1.7761773586273193\n",
      "Batch 200,  loss: 2.3125996589660645\n",
      "Batch 205,  loss: 2.209609937667847\n",
      "Batch 210,  loss: 2.210132694244385\n",
      "Batch 215,  loss: 2.2518669605255126\n",
      "Batch 220,  loss: 1.5217133283615112\n",
      "Batch 225,  loss: 2.3763757705688477\n",
      "Batch 230,  loss: 2.022080659866333\n",
      "Batch 235,  loss: 1.8550405740737914\n",
      "Batch 240,  loss: 1.789263081550598\n",
      "Batch 245,  loss: 1.918178415298462\n",
      "Batch 250,  loss: 2.3044352531433105\n",
      "Batch 255,  loss: 2.1810213327407837\n",
      "Batch 260,  loss: 2.0212443828582765\n",
      "Batch 265,  loss: 2.0492501258850098\n",
      "Batch 270,  loss: 2.0571993350982667\n",
      "Batch 275,  loss: 1.9954140424728393\n",
      "Batch 280,  loss: 2.0110238194465637\n",
      "Batch 285,  loss: 2.3029550552368163\n",
      "Batch 290,  loss: 2.586156797409058\n",
      "Batch 295,  loss: 2.5403725862503053\n",
      "Batch 300,  loss: 2.2009513854980467\n",
      "Batch 305,  loss: 1.9676022291183473\n",
      "Batch 310,  loss: 2.0478162288665773\n",
      "Batch 315,  loss: 1.8592942714691163\n",
      "Batch 320,  loss: 2.188543176651001\n",
      "Batch 325,  loss: 1.760130524635315\n",
      "Batch 330,  loss: 2.3917352676391603\n",
      "Batch 335,  loss: 1.6907285451889038\n",
      "Batch 340,  loss: 2.407219386100769\n",
      "Batch 345,  loss: 2.216638422012329\n",
      "Batch 350,  loss: 1.8708642959594726\n",
      "Batch 355,  loss: 2.041773772239685\n",
      "Batch 360,  loss: 1.8514660596847534\n",
      "Batch 365,  loss: 2.0658923387527466\n",
      "Batch 370,  loss: 2.3561538219451905\n",
      "Batch 375,  loss: 2.4841971397399902\n",
      "Batch 380,  loss: 2.3485476970672607\n",
      "Batch 385,  loss: 2.0723983526229857\n",
      "Batch 390,  loss: 1.737387228012085\n",
      "Batch 395,  loss: 2.222541856765747\n",
      "Batch 400,  loss: 1.8624769449234009\n",
      "Batch 405,  loss: 2.2097353458404543\n",
      "Batch 410,  loss: 2.037392282485962\n",
      "Batch 415,  loss: 1.760501456260681\n",
      "Batch 420,  loss: 1.4495384693145752\n",
      "Batch 425,  loss: 1.9508354663848877\n",
      "Batch 430,  loss: 2.40163106918335\n",
      "Batch 435,  loss: 2.136441159248352\n",
      "Batch 440,  loss: 2.202914071083069\n",
      "Batch 445,  loss: 2.2196044921875\n",
      "Batch 450,  loss: 1.6473416090011597\n",
      "Batch 455,  loss: 1.7933721780776977\n",
      "Batch 460,  loss: 1.6475266695022583\n",
      "Batch 465,  loss: 2.094956564903259\n",
      "Batch 470,  loss: 2.155967855453491\n",
      "Batch 475,  loss: 2.0298299551010133\n",
      "Batch 480,  loss: 2.4396232843399046\n",
      "Batch 485,  loss: 1.749349594116211\n",
      "Batch 490,  loss: 1.8616578817367553\n",
      "Batch 495,  loss: 1.7981066465377809\n",
      "Batch 500,  loss: 1.7472687005996703\n",
      "Batch 505,  loss: 1.6280500411987304\n",
      "Batch 510,  loss: 2.3958736181259157\n",
      "Batch 515,  loss: 2.628985786437988\n",
      "Batch 520,  loss: 2.6784363269805906\n",
      "Batch 525,  loss: 1.73327579498291\n",
      "Batch 530,  loss: 1.735784649848938\n",
      "Batch 535,  loss: 2.1003910303115845\n",
      "Batch 540,  loss: 1.614673113822937\n",
      "Batch 545,  loss: 2.06181857585907\n",
      "Batch 550,  loss: 2.183310079574585\n",
      "Batch 555,  loss: 1.9006949424743653\n",
      "Batch 560,  loss: 2.2408026218414308\n",
      "Batch 565,  loss: 1.8009653568267823\n",
      "Batch 570,  loss: 1.9994722843170165\n",
      "Batch 575,  loss: 1.8949267864227295\n",
      "Batch 580,  loss: 2.011742424964905\n",
      "Batch 585,  loss: 2.4296427249908445\n",
      "Batch 590,  loss: 1.835370707511902\n",
      "Batch 595,  loss: 2.348938512802124\n",
      "Batch 600,  loss: 2.1695550203323366\n",
      "Batch 605,  loss: 1.6324622273445129\n",
      "Batch 610,  loss: 2.2311089038848877\n",
      "Batch 615,  loss: 1.7994644045829773\n",
      "Batch 620,  loss: 2.2258108615875245\n",
      "Batch 625,  loss: 2.05665442943573\n",
      "Batch 630,  loss: 2.2743784904479982\n",
      "Batch 635,  loss: 1.8579828023910523\n",
      "Batch 640,  loss: 2.59104905128479\n",
      "Batch 645,  loss: 1.6849642515182495\n",
      "Batch 650,  loss: 1.953109359741211\n",
      "Batch 655,  loss: 2.4846025466918946\n",
      "Batch 660,  loss: 2.189789628982544\n",
      "Batch 665,  loss: 1.8432177305221558\n",
      "Batch 670,  loss: 2.1906567096710203\n",
      "Batch 675,  loss: 3.1440991878509523\n",
      "Batch 680,  loss: 2.6902968883514404\n",
      "Batch 685,  loss: 1.6918370246887207\n",
      "Batch 690,  loss: 1.5226390838623047\n",
      "Batch 695,  loss: 1.9417765855789184\n",
      "Batch 700,  loss: 1.9406466484069824\n",
      "Batch 705,  loss: 1.859891653060913\n",
      "Batch 710,  loss: 2.2678637742996215\n",
      "Batch 715,  loss: 2.12398841381073\n",
      "Batch 720,  loss: 1.7250827550888062\n",
      "Batch 725,  loss: 2.004807186126709\n",
      "Batch 730,  loss: 1.7140650987625121\n",
      "Batch 735,  loss: 2.3537103176116942\n",
      "Batch 740,  loss: 2.205412745475769\n",
      "Batch 745,  loss: 2.191440963745117\n",
      "Batch 750,  loss: 1.9315144538879394\n",
      "Batch 755,  loss: 1.7730195283889771\n",
      "Batch 760,  loss: 1.5817371606826782\n",
      "Batch 765,  loss: 1.827966046333313\n",
      "Batch 770,  loss: 2.0372401237487794\n",
      "Batch 775,  loss: 1.8101717948913574\n",
      "Batch 780,  loss: 1.8815844774246215\n",
      "Batch 785,  loss: 2.1716134309768678\n",
      "Batch 790,  loss: 2.043121647834778\n",
      "Batch 795,  loss: 1.803281307220459\n",
      "Batch 800,  loss: 2.301563000679016\n",
      "Batch 805,  loss: 1.6804685115814209\n",
      "Batch 810,  loss: 1.703065848350525\n",
      "Batch 815,  loss: 1.9382007122039795\n",
      "Batch 820,  loss: 1.8293861150741577\n",
      "Batch 825,  loss: 2.134006142616272\n",
      "Batch 830,  loss: 1.8538372278213502\n",
      "Batch 835,  loss: 2.167099404335022\n",
      "Batch 840,  loss: 1.9746235370635987\n",
      "Batch 845,  loss: 2.199246406555176\n",
      "Batch 850,  loss: 1.998900008201599\n",
      "Batch 855,  loss: 1.9120555162429809\n",
      "Batch 860,  loss: 2.018680238723755\n",
      "Batch 865,  loss: 2.3431859731674196\n",
      "Batch 870,  loss: 2.2311362266540526\n",
      "Batch 875,  loss: 2.2948411226272585\n",
      "Batch 880,  loss: 2.6449416637420655\n",
      "Batch 885,  loss: 1.8937714576721192\n",
      "Batch 890,  loss: 2.349263072013855\n",
      "Batch 895,  loss: 2.262871837615967\n",
      "Batch 900,  loss: 1.5606621980667115\n",
      "Batch 905,  loss: 2.066401958465576\n",
      "Batch 910,  loss: 2.0987328767776487\n",
      "Batch 915,  loss: 1.8454405307769775\n",
      "Batch 920,  loss: 1.9201128005981445\n",
      "Batch 925,  loss: 1.6380162000656129\n",
      "Batch 930,  loss: 2.1463531970977785\n",
      "Batch 935,  loss: 1.8653805255889893\n",
      "Batch 940,  loss: 1.7976197242736816\n",
      "Batch 945,  loss: 2.6069904088974\n",
      "Batch 950,  loss: 2.1171505212783814\n",
      "Batch 955,  loss: 2.070373773574829\n",
      "Batch 960,  loss: 1.8640466451644897\n",
      "Batch 965,  loss: 1.7270774364471435\n",
      "Batch 970,  loss: 2.1707420110702516\n",
      "Batch 975,  loss: 1.8138508796691895\n",
      "Batch 980,  loss: 2.3785093307495115\n",
      "Batch 985,  loss: 1.9459300756454467\n",
      "Batch 990,  loss: 1.9157326936721801\n",
      "Batch 995,  loss: 2.0111782789230346\n",
      "Batch 1000,  loss: 2.232347512245178\n",
      "Batch 1005,  loss: 1.7741416215896606\n",
      "Batch 1010,  loss: 2.1866740942001344\n",
      "Batch 1015,  loss: 2.1212982177734374\n",
      "Batch 1020,  loss: 1.723936629295349\n",
      "Batch 1025,  loss: 1.834004783630371\n",
      "Batch 1030,  loss: 2.179458737373352\n",
      "Batch 1035,  loss: 1.9498041152954102\n",
      "Batch 1040,  loss: 1.8054988861083985\n",
      "Batch 1045,  loss: 1.6648966073989868\n",
      "Batch 1050,  loss: 2.193154788017273\n",
      "Batch 1055,  loss: 2.335467886924744\n",
      "Batch 1060,  loss: 2.182687997817993\n",
      "Batch 1065,  loss: 2.047544765472412\n",
      "Batch 1070,  loss: 2.237633752822876\n",
      "Batch 1075,  loss: 2.007726812362671\n",
      "Batch 1080,  loss: 2.1914130449295044\n",
      "Batch 1085,  loss: 2.254970932006836\n",
      "Batch 1090,  loss: 2.3149405479431153\n",
      "Batch 1095,  loss: 2.7539786815643312\n",
      "Batch 1100,  loss: 2.519886875152588\n",
      "Batch 1105,  loss: 2.2426653146743774\n",
      "Batch 1110,  loss: 1.8522397994995117\n",
      "Batch 1115,  loss: 2.3388254284858703\n",
      "Batch 1120,  loss: 1.7889512300491333\n",
      "Batch 1125,  loss: 2.2762804985046388\n",
      "Batch 1130,  loss: 2.4734355926513674\n",
      "Batch 1135,  loss: 1.9696732044219971\n",
      "Batch 1140,  loss: 2.4792223453521727\n",
      "Batch 1145,  loss: 1.9512766361236573\n",
      "Batch 1150,  loss: 2.03040406703949\n",
      "Batch 1155,  loss: 1.8314626455307006\n",
      "Batch 1160,  loss: 1.7054648876190186\n",
      "Batch 1165,  loss: 2.0333385467529297\n",
      "Batch 1170,  loss: 1.8800813913345338\n",
      "Batch 1175,  loss: 1.6442123651504517\n",
      "Batch 1180,  loss: 2.5443734169006347\n",
      "Batch 1185,  loss: 2.367374134063721\n",
      "Batch 1190,  loss: 1.892226791381836\n",
      "Batch 1195,  loss: 1.583585262298584\n",
      "Batch 1200,  loss: 1.8452371120452882\n",
      "Batch 1205,  loss: 1.8449243307113647\n",
      "Batch 1210,  loss: 1.8456199884414672\n",
      "Batch 1215,  loss: 2.0007721424102782\n",
      "Batch 1220,  loss: 1.9638043642044067\n",
      "Batch 1225,  loss: 1.9460442304611205\n",
      "Batch 1230,  loss: 2.0613639831542967\n",
      "Batch 1235,  loss: 1.730982279777527\n",
      "Batch 1240,  loss: 1.7418624401092528\n",
      "Batch 1245,  loss: 2.4815273761749266\n",
      "Batch 1250,  loss: 2.1221917152404783\n",
      "Batch 1255,  loss: 2.394215798377991\n",
      "Batch 1260,  loss: 1.734880805015564\n",
      "Batch 1265,  loss: 2.094343972206116\n",
      "Batch 1270,  loss: 1.9120565414428712\n",
      "Batch 1275,  loss: 2.779337453842163\n",
      "Batch 1280,  loss: 1.491623091697693\n",
      "Batch 1285,  loss: 2.1291682958602904\n",
      "Batch 1290,  loss: 1.906701683998108\n",
      "Batch 1295,  loss: 1.4340654611587524\n",
      "LOSS train 1.4340654611587524. Validation loss: 2.2743733852498096 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 17:\n",
      "Batch 5,  loss: 2.171490287780762\n",
      "Batch 10,  loss: 1.6849421739578248\n",
      "Batch 15,  loss: 2.328253984451294\n",
      "Batch 20,  loss: 1.9916052341461181\n",
      "Batch 25,  loss: 2.452109956741333\n",
      "Batch 30,  loss: 2.1647425651550294\n",
      "Batch 35,  loss: 2.164018702507019\n",
      "Batch 40,  loss: 1.9795998334884644\n",
      "Batch 45,  loss: 2.1183621406555178\n",
      "Batch 50,  loss: 1.9680193662643433\n",
      "Batch 55,  loss: 2.1749670267105103\n",
      "Batch 60,  loss: 2.4075867414474486\n",
      "Batch 65,  loss: 2.137450408935547\n",
      "Batch 70,  loss: 1.7781164169311523\n",
      "Batch 75,  loss: 2.3218878507614136\n",
      "Batch 80,  loss: 1.8021476030349732\n",
      "Batch 85,  loss: 1.8118086099624633\n",
      "Batch 90,  loss: 1.628324556350708\n",
      "Batch 95,  loss: 2.8564006805419924\n",
      "Batch 100,  loss: 1.9506098270416259\n",
      "Batch 105,  loss: 2.0131436824798583\n",
      "Batch 110,  loss: 1.983463501930237\n",
      "Batch 115,  loss: 1.7936347246170044\n",
      "Batch 120,  loss: 2.3775314807891847\n",
      "Batch 125,  loss: 2.1520512819290163\n",
      "Batch 130,  loss: 2.085561418533325\n",
      "Batch 135,  loss: 1.7345276355743409\n",
      "Batch 140,  loss: 2.1123050689697265\n",
      "Batch 145,  loss: 1.7305639266967774\n",
      "Batch 150,  loss: 1.9480912208557128\n",
      "Batch 155,  loss: 1.8442019939422607\n",
      "Batch 160,  loss: 2.6215993642807005\n",
      "Batch 165,  loss: 2.7586410522460936\n",
      "Batch 170,  loss: 1.6363707542419434\n",
      "Batch 175,  loss: 1.8252504110336303\n",
      "Batch 180,  loss: 1.7229326248168946\n",
      "Batch 185,  loss: 1.9643933773040771\n",
      "Batch 190,  loss: 1.9007643222808839\n",
      "Batch 195,  loss: 2.193196725845337\n",
      "Batch 200,  loss: 1.750873827934265\n",
      "Batch 205,  loss: 1.8219896316528321\n",
      "Batch 210,  loss: 1.9696703195571899\n",
      "Batch 215,  loss: 1.8742638111114502\n",
      "Batch 220,  loss: 1.8564459562301636\n",
      "Batch 225,  loss: 1.9782968282699585\n",
      "Batch 230,  loss: 2.348112440109253\n",
      "Batch 235,  loss: 2.586505174636841\n",
      "Batch 240,  loss: 1.683244800567627\n",
      "Batch 245,  loss: 1.7629508018493651\n",
      "Batch 250,  loss: 1.795307493209839\n",
      "Batch 255,  loss: 1.8243256568908692\n",
      "Batch 260,  loss: 1.5702672243118285\n",
      "Batch 265,  loss: 1.836332082748413\n",
      "Batch 270,  loss: 2.0640965938568114\n",
      "Batch 275,  loss: 1.7077552795410156\n",
      "Batch 280,  loss: 2.765649175643921\n",
      "Batch 285,  loss: 2.0253785133361815\n",
      "Batch 290,  loss: 2.295922541618347\n",
      "Batch 295,  loss: 1.9100442409515381\n",
      "Batch 300,  loss: 2.240967893600464\n",
      "Batch 305,  loss: 2.210252571105957\n",
      "Batch 310,  loss: 2.3272469520568846\n",
      "Batch 315,  loss: 1.9485519886016847\n",
      "Batch 320,  loss: 1.8574665784835815\n",
      "Batch 325,  loss: 1.9294437885284423\n",
      "Batch 330,  loss: 2.27614483833313\n",
      "Batch 335,  loss: 2.630704641342163\n",
      "Batch 340,  loss: 2.067806100845337\n",
      "Batch 345,  loss: 2.0039125204086305\n",
      "Batch 350,  loss: 2.317656707763672\n",
      "Batch 355,  loss: 2.6936821937561035\n",
      "Batch 360,  loss: 2.390107583999634\n",
      "Batch 365,  loss: 1.9031324863433838\n",
      "Batch 370,  loss: 1.900380802154541\n",
      "Batch 375,  loss: 2.1207329750061037\n",
      "Batch 380,  loss: 2.4069799184799194\n",
      "Batch 385,  loss: 1.6277026176452636\n",
      "Batch 390,  loss: 1.949178147315979\n",
      "Batch 395,  loss: 1.983627438545227\n",
      "Batch 400,  loss: 1.7974002361297607\n",
      "Batch 405,  loss: 2.2040881395339964\n",
      "Batch 410,  loss: 2.0083600759506224\n",
      "Batch 415,  loss: 2.1023932218551638\n",
      "Batch 420,  loss: 1.5934073209762574\n",
      "Batch 425,  loss: 2.400818920135498\n",
      "Batch 430,  loss: 2.0841570854187013\n",
      "Batch 435,  loss: 2.209521174430847\n",
      "Batch 440,  loss: 1.8581608295440675\n",
      "Batch 445,  loss: 2.1033143758773805\n",
      "Batch 450,  loss: 1.8889421463012694\n",
      "Batch 455,  loss: 2.138410758972168\n",
      "Batch 460,  loss: 1.8134133338928222\n",
      "Batch 465,  loss: 2.4867974281311036\n",
      "Batch 470,  loss: 2.249140167236328\n",
      "Batch 475,  loss: 1.8163671493530273\n",
      "Batch 480,  loss: 2.082024359703064\n",
      "Batch 485,  loss: 2.373995065689087\n",
      "Batch 490,  loss: 1.6062330961227418\n",
      "Batch 495,  loss: 2.0750139474868776\n",
      "Batch 500,  loss: 1.5701096534729004\n",
      "Batch 505,  loss: 1.903404450416565\n",
      "Batch 510,  loss: 2.244730567932129\n",
      "Batch 515,  loss: 1.9237726926803589\n",
      "Batch 520,  loss: 2.1191239833831785\n",
      "Batch 525,  loss: 1.7407779455184937\n",
      "Batch 530,  loss: 2.0154175996780395\n",
      "Batch 535,  loss: 2.290424418449402\n",
      "Batch 540,  loss: 2.3205628395080566\n",
      "Batch 545,  loss: 1.7858875036239623\n",
      "Batch 550,  loss: 1.6867452144622803\n",
      "Batch 555,  loss: 1.8576354742050172\n",
      "Batch 560,  loss: 1.9590422868728639\n",
      "Batch 565,  loss: 1.7498058795928955\n",
      "Batch 570,  loss: 1.60468647480011\n",
      "Batch 575,  loss: 1.8044131755828858\n",
      "Batch 580,  loss: 1.9181968688964843\n",
      "Batch 585,  loss: 2.141850447654724\n",
      "Batch 590,  loss: 1.7676859855651856\n",
      "Batch 595,  loss: 1.6922374486923217\n",
      "Batch 600,  loss: 2.0305145025253295\n",
      "Batch 605,  loss: 2.074554991722107\n",
      "Batch 610,  loss: 1.821382713317871\n",
      "Batch 615,  loss: 2.135621166229248\n",
      "Batch 620,  loss: 2.240699291229248\n",
      "Batch 625,  loss: 1.9309492111206055\n",
      "Batch 630,  loss: 1.97479190826416\n",
      "Batch 635,  loss: 2.4456952571868897\n",
      "Batch 640,  loss: 2.253606939315796\n",
      "Batch 645,  loss: 2.210555601119995\n",
      "Batch 650,  loss: 2.1188180208206178\n",
      "Batch 655,  loss: 1.8254692077636718\n",
      "Batch 660,  loss: 1.9674658298492431\n",
      "Batch 665,  loss: 2.0293235540390016\n",
      "Batch 670,  loss: 1.6805830955505372\n",
      "Batch 675,  loss: 1.840234923362732\n",
      "Batch 680,  loss: 2.4739280462265016\n",
      "Batch 685,  loss: 2.2541783809661866\n",
      "Batch 690,  loss: 2.088635969161987\n",
      "Batch 695,  loss: 2.0247275114059446\n",
      "Batch 700,  loss: 1.9114438056945802\n",
      "Batch 705,  loss: 1.7361924648284912\n",
      "Batch 710,  loss: 1.9171096086502075\n",
      "Batch 715,  loss: 1.7260578155517579\n",
      "Batch 720,  loss: 2.0029460430145263\n",
      "Batch 725,  loss: 1.931063985824585\n",
      "Batch 730,  loss: 2.0359485149383545\n",
      "Batch 735,  loss: 2.143288779258728\n",
      "Batch 740,  loss: 2.2980706453323365\n",
      "Batch 745,  loss: 2.1318376064300537\n",
      "Batch 750,  loss: 1.86796875\n",
      "Batch 755,  loss: 1.8990527868270874\n",
      "Batch 760,  loss: 1.660826063156128\n",
      "Batch 765,  loss: 1.6975902557373046\n",
      "Batch 770,  loss: 1.8093724012374879\n",
      "Batch 775,  loss: 1.805657649040222\n",
      "Batch 780,  loss: 1.3411317586898803\n",
      "Batch 785,  loss: 1.9520984172821045\n",
      "Batch 790,  loss: 2.337251663208008\n",
      "Batch 795,  loss: 1.9641444444656373\n",
      "Batch 800,  loss: 1.9244932651519775\n",
      "Batch 805,  loss: 2.0551859378814696\n",
      "Batch 810,  loss: 1.837114191055298\n",
      "Batch 815,  loss: 2.162119007110596\n",
      "Batch 820,  loss: 1.787216305732727\n",
      "Batch 825,  loss: 1.8295609951019287\n",
      "Batch 830,  loss: 1.6740198135375977\n",
      "Batch 835,  loss: 2.571618127822876\n",
      "Batch 840,  loss: 1.601246166229248\n",
      "Batch 845,  loss: 2.016551160812378\n",
      "Batch 850,  loss: 2.009330940246582\n",
      "Batch 855,  loss: 2.123084831237793\n",
      "Batch 860,  loss: 1.8324713706970215\n",
      "Batch 865,  loss: 1.902905559539795\n",
      "Batch 870,  loss: 2.1252366065979005\n",
      "Batch 875,  loss: 1.802470064163208\n",
      "Batch 880,  loss: 2.0176169872283936\n",
      "Batch 885,  loss: 1.6582409143447876\n",
      "Batch 890,  loss: 1.6390984058380127\n",
      "Batch 895,  loss: 2.2126752376556396\n",
      "Batch 900,  loss: 2.154419755935669\n",
      "Batch 905,  loss: 1.6899877786636353\n",
      "Batch 910,  loss: 1.8269930362701416\n",
      "Batch 915,  loss: 2.084162950515747\n",
      "Batch 920,  loss: 1.9717003107070923\n",
      "Batch 925,  loss: 1.8845741987228393\n",
      "Batch 930,  loss: 1.8219444513320924\n",
      "Batch 935,  loss: 1.9618876457214356\n",
      "Batch 940,  loss: 1.6933177709579468\n",
      "Batch 945,  loss: 1.8221205711364745\n",
      "Batch 950,  loss: 1.6432063341140748\n",
      "Batch 955,  loss: 1.7986547470092773\n",
      "Batch 960,  loss: 2.0313119649887086\n",
      "Batch 965,  loss: 2.532541561126709\n",
      "Batch 970,  loss: 2.172216796875\n",
      "Batch 975,  loss: 1.8985291481018067\n",
      "Batch 980,  loss: 1.9751755952835084\n",
      "Batch 985,  loss: 2.1154369115829468\n",
      "Batch 990,  loss: 2.1102502822875975\n",
      "Batch 995,  loss: 1.6036685705184937\n",
      "Batch 1000,  loss: 1.986758255958557\n",
      "Batch 1005,  loss: 1.9906003713607787\n",
      "Batch 1010,  loss: 1.5936286687850951\n",
      "Batch 1015,  loss: 1.9958181858062745\n",
      "Batch 1020,  loss: 2.0566650390625\n",
      "Batch 1025,  loss: 2.5649617671966554\n",
      "Batch 1030,  loss: 2.234603524208069\n",
      "Batch 1035,  loss: 2.0681044340133665\n",
      "Batch 1040,  loss: 1.8395211458206178\n",
      "Batch 1045,  loss: 2.210753655433655\n",
      "Batch 1050,  loss: 1.826370906829834\n",
      "Batch 1055,  loss: 1.9535088539123535\n",
      "Batch 1060,  loss: 2.201740264892578\n",
      "Batch 1065,  loss: 2.1961253881454468\n",
      "Batch 1070,  loss: 2.280468559265137\n",
      "Batch 1075,  loss: 2.2734114646911623\n",
      "Batch 1080,  loss: 1.5261813402175903\n",
      "Batch 1085,  loss: 2.089747953414917\n",
      "Batch 1090,  loss: 2.455618953704834\n",
      "Batch 1095,  loss: 1.9351030349731446\n",
      "Batch 1100,  loss: 1.8989644527435303\n",
      "Batch 1105,  loss: 2.041664481163025\n",
      "Batch 1110,  loss: 1.912001943588257\n",
      "Batch 1115,  loss: 2.3270664691925047\n",
      "Batch 1120,  loss: 1.8708448171615601\n",
      "Batch 1125,  loss: 1.8472406387329101\n",
      "Batch 1130,  loss: 1.8234811305999756\n",
      "Batch 1135,  loss: 2.040811038017273\n",
      "Batch 1140,  loss: 2.5118255853652953\n",
      "Batch 1145,  loss: 1.8881733894348145\n",
      "Batch 1150,  loss: 1.7713577270507812\n",
      "Batch 1155,  loss: 1.5463798880577087\n",
      "Batch 1160,  loss: 2.0336863517761232\n",
      "Batch 1165,  loss: 1.7217914581298828\n",
      "Batch 1170,  loss: 2.267545700073242\n",
      "Batch 1175,  loss: 1.7636179327964783\n",
      "Batch 1180,  loss: 1.347632145881653\n",
      "Batch 1185,  loss: 2.257038140296936\n",
      "Batch 1190,  loss: 2.10967435836792\n",
      "Batch 1195,  loss: 1.7107778549194337\n",
      "Batch 1200,  loss: 2.071743679046631\n",
      "Batch 1205,  loss: 2.4886317014694215\n",
      "Batch 1210,  loss: 2.0465933322906493\n",
      "Batch 1215,  loss: 2.2205134868621825\n",
      "Batch 1220,  loss: 1.8741277933120728\n",
      "Batch 1225,  loss: 2.7146429061889648\n",
      "Batch 1230,  loss: 1.7686211109161376\n",
      "Batch 1235,  loss: 1.9080117702484132\n",
      "Batch 1240,  loss: 1.782951331138611\n",
      "Batch 1245,  loss: 2.255134677886963\n",
      "Batch 1250,  loss: 1.8992355585098266\n",
      "Batch 1255,  loss: 1.7997956991195678\n",
      "Batch 1260,  loss: 1.8768104791641236\n",
      "Batch 1265,  loss: 2.274519348144531\n",
      "Batch 1270,  loss: 1.684556484222412\n",
      "Batch 1275,  loss: 1.9954745292663574\n",
      "Batch 1280,  loss: 2.01727499961853\n",
      "Batch 1285,  loss: 1.8600706100463866\n",
      "Batch 1290,  loss: 1.6174166679382325\n",
      "Batch 1295,  loss: 2.28763427734375\n",
      "LOSS train 2.28763427734375. Validation loss: 2.37799371038736 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 18:\n",
      "Batch 5,  loss: 2.7805490016937258\n",
      "Batch 10,  loss: 1.8553722381591797\n",
      "Batch 15,  loss: 2.132795882225037\n",
      "Batch 20,  loss: 2.6321151733398436\n",
      "Batch 25,  loss: 2.3787222385406492\n",
      "Batch 30,  loss: 2.663179063796997\n",
      "Batch 35,  loss: 2.048421049118042\n",
      "Batch 40,  loss: 1.7212730169296264\n",
      "Batch 45,  loss: 1.912195348739624\n",
      "Batch 50,  loss: 2.153718137741089\n",
      "Batch 55,  loss: 1.4681456089019775\n",
      "Batch 60,  loss: 2.2812287330627443\n",
      "Batch 65,  loss: 2.299048972129822\n",
      "Batch 70,  loss: 2.093598747253418\n",
      "Batch 75,  loss: 2.272978162765503\n",
      "Batch 80,  loss: 2.292435073852539\n",
      "Batch 85,  loss: 2.149647760391235\n",
      "Batch 90,  loss: 2.3449307918548583\n",
      "Batch 95,  loss: 1.9856724977493285\n",
      "Batch 100,  loss: 1.7056284189224242\n",
      "Batch 105,  loss: 2.0725643396377564\n",
      "Batch 110,  loss: 2.309857940673828\n",
      "Batch 115,  loss: 1.9187768697738647\n",
      "Batch 120,  loss: 1.9848156213760375\n",
      "Batch 125,  loss: 2.042209434509277\n",
      "Batch 130,  loss: 1.6451431155204772\n",
      "Batch 135,  loss: 1.6595285892486573\n",
      "Batch 140,  loss: 1.8711508750915526\n",
      "Batch 145,  loss: 2.2246691465377806\n",
      "Batch 150,  loss: 1.980745005607605\n",
      "Batch 155,  loss: 1.5614057064056397\n",
      "Batch 160,  loss: 1.6381555795669556\n",
      "Batch 165,  loss: 1.6150679349899293\n",
      "Batch 170,  loss: 2.271736216545105\n",
      "Batch 175,  loss: 2.2029072284698485\n",
      "Batch 180,  loss: 2.114635705947876\n",
      "Batch 185,  loss: 2.037159299850464\n",
      "Batch 190,  loss: 1.9663510084152223\n",
      "Batch 195,  loss: 2.5079135417938234\n",
      "Batch 200,  loss: 1.7097926139831543\n",
      "Batch 205,  loss: 1.799083662033081\n",
      "Batch 210,  loss: 1.572753620147705\n",
      "Batch 215,  loss: 1.9243594408035278\n",
      "Batch 220,  loss: 1.9951926231384278\n",
      "Batch 225,  loss: 1.78738272190094\n",
      "Batch 230,  loss: 2.5901976108551024\n",
      "Batch 235,  loss: 1.8559182167053223\n",
      "Batch 240,  loss: 2.020843815803528\n",
      "Batch 245,  loss: 1.8921350717544556\n",
      "Batch 250,  loss: 1.9396658897399903\n",
      "Batch 255,  loss: 1.828115701675415\n",
      "Batch 260,  loss: 1.5743328094482423\n",
      "Batch 265,  loss: 1.2993047952651977\n",
      "Batch 270,  loss: 1.8242558479309081\n",
      "Batch 275,  loss: 2.0054272413253784\n",
      "Batch 280,  loss: 1.8788962602615356\n",
      "Batch 285,  loss: 1.7902007341384887\n",
      "Batch 290,  loss: 1.5523060798645019\n",
      "Batch 295,  loss: 1.9166870355606078\n",
      "Batch 300,  loss: 2.3716275215148928\n",
      "Batch 305,  loss: 1.5629699945449829\n",
      "Batch 310,  loss: 1.871897530555725\n",
      "Batch 315,  loss: 1.83615140914917\n",
      "Batch 320,  loss: 2.173742485046387\n",
      "Batch 325,  loss: 2.120800256729126\n",
      "Batch 330,  loss: 1.9518592596054076\n",
      "Batch 335,  loss: 2.233650875091553\n",
      "Batch 340,  loss: 2.033556604385376\n",
      "Batch 345,  loss: 2.0059409618377684\n",
      "Batch 350,  loss: 1.7369203329086305\n",
      "Batch 355,  loss: 2.194906163215637\n",
      "Batch 360,  loss: 1.9612881660461425\n",
      "Batch 365,  loss: 1.746446704864502\n",
      "Batch 370,  loss: 1.8171015977859497\n",
      "Batch 375,  loss: 1.7995015621185302\n",
      "Batch 380,  loss: 1.5364761829376221\n",
      "Batch 385,  loss: 1.5660504579544068\n",
      "Batch 390,  loss: 2.1490922570228577\n",
      "Batch 395,  loss: 2.4132823944091797\n",
      "Batch 400,  loss: 2.107239103317261\n",
      "Batch 405,  loss: 2.0288886547088625\n",
      "Batch 410,  loss: 2.0294575691223145\n",
      "Batch 415,  loss: 1.7414185762405396\n",
      "Batch 420,  loss: 1.7494690418243408\n",
      "Batch 425,  loss: 2.2104525804519652\n",
      "Batch 430,  loss: 1.7012223482131958\n",
      "Batch 435,  loss: 1.9203161716461181\n",
      "Batch 440,  loss: 2.197878670692444\n",
      "Batch 445,  loss: 1.579232096672058\n",
      "Batch 450,  loss: 1.843858289718628\n",
      "Batch 455,  loss: 2.3018893003463745\n",
      "Batch 460,  loss: 2.5249370336532593\n",
      "Batch 465,  loss: 1.7390895366668702\n",
      "Batch 470,  loss: 2.188664126396179\n",
      "Batch 475,  loss: 2.036525297164917\n",
      "Batch 480,  loss: 1.8991819381713868\n",
      "Batch 485,  loss: 1.5333403825759888\n",
      "Batch 490,  loss: 2.252612900733948\n",
      "Batch 495,  loss: 2.2231149911880492\n",
      "Batch 500,  loss: 2.052488374710083\n",
      "Batch 505,  loss: 1.934068751335144\n",
      "Batch 510,  loss: 2.0654337406158447\n",
      "Batch 515,  loss: 1.95168137550354\n",
      "Batch 520,  loss: 1.5966352939605712\n",
      "Batch 525,  loss: 1.6912631511688232\n",
      "Batch 530,  loss: 1.697535252571106\n",
      "Batch 535,  loss: 1.9164978981018066\n",
      "Batch 540,  loss: 1.8353339910507203\n",
      "Batch 545,  loss: 1.6743906021118165\n",
      "Batch 550,  loss: 1.6095128536224366\n",
      "Batch 555,  loss: 1.581628942489624\n",
      "Batch 560,  loss: 1.8844759941101075\n",
      "Batch 565,  loss: 1.8771376132965087\n",
      "Batch 570,  loss: 2.2153821706771852\n",
      "Batch 575,  loss: 2.0615076541900637\n",
      "Batch 580,  loss: 1.8550156116485597\n",
      "Batch 585,  loss: 1.9228357076644897\n",
      "Batch 590,  loss: 1.44878808259964\n",
      "Batch 595,  loss: 1.8073888778686524\n",
      "Batch 600,  loss: 1.9221019983291625\n",
      "Batch 605,  loss: 1.660327959060669\n",
      "Batch 610,  loss: 2.117913603782654\n",
      "Batch 615,  loss: 1.8698194980621339\n",
      "Batch 620,  loss: 1.4160062551498414\n",
      "Batch 625,  loss: 2.202431631088257\n",
      "Batch 630,  loss: 1.8965498685836792\n",
      "Batch 635,  loss: 2.0703984975814818\n",
      "Batch 640,  loss: 1.7829214096069337\n",
      "Batch 645,  loss: 2.125492739677429\n",
      "Batch 650,  loss: 1.8053054094314576\n",
      "Batch 655,  loss: 1.864058017730713\n",
      "Batch 660,  loss: 1.9285970449447631\n",
      "Batch 665,  loss: 2.3533055067062376\n",
      "Batch 670,  loss: 2.393173408508301\n",
      "Batch 675,  loss: 1.9168376922607422\n",
      "Batch 680,  loss: 1.9781147956848144\n",
      "Batch 685,  loss: 2.011967992782593\n",
      "Batch 690,  loss: 1.7828088283538819\n",
      "Batch 695,  loss: 1.8783946990966798\n",
      "Batch 700,  loss: 1.8382800102233887\n",
      "Batch 705,  loss: 1.7955303192138672\n",
      "Batch 710,  loss: 2.1440017223358154\n",
      "Batch 715,  loss: 2.2103148698806763\n",
      "Batch 720,  loss: 1.825394368171692\n",
      "Batch 725,  loss: 2.544818162918091\n",
      "Batch 730,  loss: 1.6464280843734742\n",
      "Batch 735,  loss: 2.2845495700836183\n",
      "Batch 740,  loss: 2.35372679233551\n",
      "Batch 745,  loss: 1.7417611837387086\n",
      "Batch 750,  loss: 1.4378965377807618\n",
      "Batch 755,  loss: 2.4129378318786623\n",
      "Batch 760,  loss: 1.9922841787338257\n",
      "Batch 765,  loss: 1.8579549074172974\n",
      "Batch 770,  loss: 2.1145393371582033\n",
      "Batch 775,  loss: 1.8627614498138427\n",
      "Batch 780,  loss: 1.5984524965286255\n",
      "Batch 785,  loss: 1.7420681715011597\n",
      "Batch 790,  loss: 2.040691924095154\n",
      "Batch 795,  loss: 1.8581332921981812\n",
      "Batch 800,  loss: 1.701326560974121\n",
      "Batch 805,  loss: 2.094991850852966\n",
      "Batch 810,  loss: 1.479693388938904\n",
      "Batch 815,  loss: 1.9728788375854491\n",
      "Batch 820,  loss: 2.2548537492752074\n",
      "Batch 825,  loss: 1.473806118965149\n",
      "Batch 830,  loss: 2.229303169250488\n",
      "Batch 835,  loss: 2.1589784383773805\n",
      "Batch 840,  loss: 1.5949010133743287\n",
      "Batch 845,  loss: 1.8255584239959717\n",
      "Batch 850,  loss: 2.109639024734497\n",
      "Batch 855,  loss: 1.7404395818710328\n",
      "Batch 860,  loss: 2.2186712265014648\n",
      "Batch 865,  loss: 1.8124477863311768\n",
      "Batch 870,  loss: 1.9737953901290894\n",
      "Batch 875,  loss: 1.6262053728103638\n",
      "Batch 880,  loss: 1.8953140258789063\n",
      "Batch 885,  loss: 1.510665488243103\n",
      "Batch 890,  loss: 1.4991273403167724\n",
      "Batch 895,  loss: 1.373969841003418\n",
      "Batch 900,  loss: 1.3601051688194274\n",
      "Batch 905,  loss: 2.2167961835861205\n",
      "Batch 910,  loss: 1.616838026046753\n",
      "Batch 915,  loss: 2.2422669649124147\n",
      "Batch 920,  loss: 2.2263460397720336\n",
      "Batch 925,  loss: 1.8625779151916504\n",
      "Batch 930,  loss: 1.7581360578536986\n",
      "Batch 935,  loss: 2.3556947231292726\n",
      "Batch 940,  loss: 1.9862090826034546\n",
      "Batch 945,  loss: 1.8429994344711305\n",
      "Batch 950,  loss: 2.1736976861953736\n",
      "Batch 955,  loss: 2.045229506492615\n",
      "Batch 960,  loss: 1.6215750932693482\n",
      "Batch 965,  loss: 1.7378727436065673\n",
      "Batch 970,  loss: 2.116621255874634\n",
      "Batch 975,  loss: 1.6443201065063477\n",
      "Batch 980,  loss: 1.966169571876526\n",
      "Batch 985,  loss: 2.0322576999664306\n",
      "Batch 990,  loss: 1.99984290599823\n",
      "Batch 995,  loss: 1.6505314350128173\n",
      "Batch 1000,  loss: 2.149843692779541\n",
      "Batch 1005,  loss: 2.1961649894714355\n",
      "Batch 1010,  loss: 1.7792210817337035\n",
      "Batch 1015,  loss: 1.8223403453826905\n",
      "Batch 1020,  loss: 1.855653977394104\n",
      "Batch 1025,  loss: 2.1238908052444456\n",
      "Batch 1030,  loss: 2.350017857551575\n",
      "Batch 1035,  loss: 1.7189257621765137\n",
      "Batch 1040,  loss: 2.4408614158630373\n",
      "Batch 1045,  loss: 1.7378562211990356\n",
      "Batch 1050,  loss: 1.6977992057800293\n",
      "Batch 1055,  loss: 2.1591892004013062\n",
      "Batch 1060,  loss: 2.134288120269775\n",
      "Batch 1065,  loss: 1.8334258317947387\n",
      "Batch 1070,  loss: 2.30902681350708\n",
      "Batch 1075,  loss: 1.6699302673339844\n",
      "Batch 1080,  loss: 1.8420924901962281\n",
      "Batch 1085,  loss: 2.0149590969085693\n",
      "Batch 1090,  loss: 1.8694030523300171\n",
      "Batch 1095,  loss: 1.8900879144668579\n",
      "Batch 1100,  loss: 2.4209622144699097\n",
      "Batch 1105,  loss: 1.684090805053711\n",
      "Batch 1110,  loss: 2.116193723678589\n",
      "Batch 1115,  loss: 1.7046848297119142\n",
      "Batch 1120,  loss: 1.7569730043411256\n",
      "Batch 1125,  loss: 2.0392175912857056\n",
      "Batch 1130,  loss: 2.1109264850616456\n",
      "Batch 1135,  loss: 2.1588778495788574\n",
      "Batch 1140,  loss: 1.7067737102508544\n",
      "Batch 1145,  loss: 1.9839783191680909\n",
      "Batch 1150,  loss: 1.522569227218628\n",
      "Batch 1155,  loss: 2.0415090322494507\n",
      "Batch 1160,  loss: 1.859085774421692\n",
      "Batch 1165,  loss: 1.8209028124809266\n",
      "Batch 1170,  loss: 2.031006956100464\n",
      "Batch 1175,  loss: 1.5474257111549377\n",
      "Batch 1180,  loss: 2.1441595554351807\n",
      "Batch 1185,  loss: 1.8850479125976562\n",
      "Batch 1190,  loss: 1.7453160762786866\n",
      "Batch 1195,  loss: 2.537758803367615\n",
      "Batch 1200,  loss: 1.5643327951431274\n",
      "Batch 1205,  loss: 1.8487542152404786\n",
      "Batch 1210,  loss: 1.9481698036193849\n",
      "Batch 1215,  loss: 2.0263211011886595\n",
      "Batch 1220,  loss: 1.9417377710342407\n",
      "Batch 1225,  loss: 2.0847140073776247\n",
      "Batch 1230,  loss: 2.3010021924972532\n",
      "Batch 1235,  loss: 1.6994898080825807\n",
      "Batch 1240,  loss: 2.3401848316192626\n",
      "Batch 1245,  loss: 1.6695243120193481\n",
      "Batch 1250,  loss: 2.08855402469635\n",
      "Batch 1255,  loss: 1.5128262281417846\n",
      "Batch 1260,  loss: 1.9661518335342407\n",
      "Batch 1265,  loss: 1.5144798517227174\n",
      "Batch 1270,  loss: 1.544052791595459\n",
      "Batch 1275,  loss: 1.8868925333023072\n",
      "Batch 1280,  loss: 1.7347913265228272\n",
      "Batch 1285,  loss: 2.0651559829711914\n",
      "Batch 1290,  loss: 1.7018125772476196\n",
      "Batch 1295,  loss: 1.3772223234176635\n",
      "LOSS train 1.3772223234176635. Validation loss: 2.267643762062545 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 19:\n",
      "Batch 5,  loss: 1.785808801651001\n",
      "Batch 10,  loss: 2.311439299583435\n",
      "Batch 15,  loss: 1.9257335662841797\n",
      "Batch 20,  loss: 2.172592043876648\n",
      "Batch 25,  loss: 1.7895689249038695\n",
      "Batch 30,  loss: 1.8281636714935303\n",
      "Batch 35,  loss: 1.987087345123291\n",
      "Batch 40,  loss: 2.2465411186218263\n",
      "Batch 45,  loss: 1.8270817041397094\n",
      "Batch 50,  loss: 1.291221594810486\n",
      "Batch 55,  loss: 2.3566490650177\n",
      "Batch 60,  loss: 1.696981930732727\n",
      "Batch 65,  loss: 1.4542839765548705\n",
      "Batch 70,  loss: 1.8681482076644897\n",
      "Batch 75,  loss: 1.7581939220428466\n",
      "Batch 80,  loss: 2.0086701631546022\n",
      "Batch 85,  loss: 1.8768970727920533\n",
      "Batch 90,  loss: 2.326168346405029\n",
      "Batch 95,  loss: 2.385871434211731\n",
      "Batch 100,  loss: 1.7372463941574097\n",
      "Batch 105,  loss: 1.7980976104736328\n",
      "Batch 110,  loss: 1.785223937034607\n",
      "Batch 115,  loss: 2.168353796005249\n",
      "Batch 120,  loss: 2.1844093799591064\n",
      "Batch 125,  loss: 1.7962807655334472\n",
      "Batch 130,  loss: 1.460989236831665\n",
      "Batch 135,  loss: 1.5626917123794555\n",
      "Batch 140,  loss: 1.5977559089660645\n",
      "Batch 145,  loss: 2.13068573474884\n",
      "Batch 150,  loss: 1.7422792434692382\n",
      "Batch 155,  loss: 2.2030988693237306\n",
      "Batch 160,  loss: 1.7933598756790161\n",
      "Batch 165,  loss: 1.9741446733474732\n",
      "Batch 170,  loss: 2.1284783363342283\n",
      "Batch 175,  loss: 1.8296644687652588\n",
      "Batch 180,  loss: 1.7685985803604125\n",
      "Batch 185,  loss: 1.7393654227256774\n",
      "Batch 190,  loss: 1.6873533010482789\n",
      "Batch 195,  loss: 2.044814872741699\n",
      "Batch 200,  loss: 2.323592281341553\n",
      "Batch 205,  loss: 2.1211416006088255\n",
      "Batch 210,  loss: 1.5808362007141112\n",
      "Batch 215,  loss: 1.8400492191314697\n",
      "Batch 220,  loss: 1.3551462173461915\n",
      "Batch 225,  loss: 1.6306239366531372\n",
      "Batch 230,  loss: 1.8862231254577637\n",
      "Batch 235,  loss: 1.9744393825531006\n",
      "Batch 240,  loss: 2.4281340837478638\n",
      "Batch 245,  loss: 2.0857766389846804\n",
      "Batch 250,  loss: 1.7490901470184326\n",
      "Batch 255,  loss: 1.6833143353462219\n",
      "Batch 260,  loss: 1.6372923374176025\n",
      "Batch 265,  loss: 2.2070170879364013\n",
      "Batch 270,  loss: 1.8882058143615723\n",
      "Batch 275,  loss: 2.138899803161621\n",
      "Batch 280,  loss: 1.7779005289077758\n",
      "Batch 285,  loss: 1.959237265586853\n",
      "Batch 290,  loss: 2.1743442535400392\n",
      "Batch 295,  loss: 1.738950514793396\n",
      "Batch 300,  loss: 2.0401522159576415\n",
      "Batch 305,  loss: 2.088211250305176\n",
      "Batch 310,  loss: 1.8133087396621703\n",
      "Batch 315,  loss: 2.239099454879761\n",
      "Batch 320,  loss: 2.328788685798645\n",
      "Batch 325,  loss: 1.7064791917800903\n",
      "Batch 330,  loss: 1.89312686920166\n",
      "Batch 335,  loss: 1.865458369255066\n",
      "Batch 340,  loss: 1.9279560089111327\n",
      "Batch 345,  loss: 1.8909902811050414\n",
      "Batch 350,  loss: 1.7903810262680053\n",
      "Batch 355,  loss: 1.69840829372406\n",
      "Batch 360,  loss: 2.179824709892273\n",
      "Batch 365,  loss: 1.638158345222473\n",
      "Batch 370,  loss: 2.263878679275513\n",
      "Batch 375,  loss: 1.7181168794631958\n",
      "Batch 380,  loss: 1.6647901773452758\n",
      "Batch 385,  loss: 1.9630184888839721\n",
      "Batch 390,  loss: 1.8506705284118652\n",
      "Batch 395,  loss: 1.8448285341262818\n",
      "Batch 400,  loss: 1.9103036403656006\n",
      "Batch 405,  loss: 1.5513283967971803\n",
      "Batch 410,  loss: 1.8116684913635255\n",
      "Batch 415,  loss: 1.8186498403549194\n",
      "Batch 420,  loss: 1.698617124557495\n",
      "Batch 425,  loss: 1.5889948606491089\n",
      "Batch 430,  loss: 1.449672245979309\n",
      "Batch 435,  loss: 2.1039254903793334\n",
      "Batch 440,  loss: 2.654593992233276\n",
      "Batch 445,  loss: 1.737260341644287\n",
      "Batch 450,  loss: 1.7951512098312379\n",
      "Batch 455,  loss: 1.717527151107788\n",
      "Batch 460,  loss: 2.091187763214111\n",
      "Batch 465,  loss: 1.7400314092636109\n",
      "Batch 470,  loss: 2.1833377838134767\n",
      "Batch 475,  loss: 2.1396600723266603\n",
      "Batch 480,  loss: 1.8292865037918091\n",
      "Batch 485,  loss: 1.6189269542694091\n",
      "Batch 490,  loss: 2.1909065723419188\n",
      "Batch 495,  loss: 2.26255350112915\n",
      "Batch 500,  loss: 1.84244863986969\n",
      "Batch 505,  loss: 2.0065447092056274\n",
      "Batch 510,  loss: 1.508633828163147\n",
      "Batch 515,  loss: 2.0591068983078005\n",
      "Batch 520,  loss: 1.5490064382553101\n",
      "Batch 525,  loss: 1.9991426944732666\n",
      "Batch 530,  loss: 1.931771445274353\n",
      "Batch 535,  loss: 1.4878421545028686\n",
      "Batch 540,  loss: 1.8004029273986817\n",
      "Batch 545,  loss: 1.7536534547805787\n",
      "Batch 550,  loss: 2.4335071802139283\n",
      "Batch 555,  loss: 1.8592146158218383\n",
      "Batch 560,  loss: 2.1075772762298586\n",
      "Batch 565,  loss: 2.1299516201019286\n",
      "Batch 570,  loss: 2.237921142578125\n",
      "Batch 575,  loss: 2.016233134269714\n",
      "Batch 580,  loss: 2.103772735595703\n",
      "Batch 585,  loss: 1.5809805870056153\n",
      "Batch 590,  loss: 1.7962621927261353\n",
      "Batch 595,  loss: 2.1255430221557616\n",
      "Batch 600,  loss: 1.6872766256332397\n",
      "Batch 605,  loss: 2.177282381057739\n",
      "Batch 610,  loss: 2.080800914764404\n",
      "Batch 615,  loss: 1.9883878350257873\n",
      "Batch 620,  loss: 2.2272693634033205\n",
      "Batch 625,  loss: 2.3094696283340452\n",
      "Batch 630,  loss: 2.2613244533538817\n",
      "Batch 635,  loss: 1.8155977487564088\n",
      "Batch 640,  loss: 1.6975080013275146\n",
      "Batch 645,  loss: 1.7481213808059692\n",
      "Batch 650,  loss: 1.909298801422119\n",
      "Batch 655,  loss: 1.661369514465332\n",
      "Batch 660,  loss: 1.8777915239334106\n",
      "Batch 665,  loss: 2.0110400438308718\n",
      "Batch 670,  loss: 1.693310761451721\n",
      "Batch 675,  loss: 1.997211790084839\n",
      "Batch 680,  loss: 1.8756524324417114\n",
      "Batch 685,  loss: 1.9535435914993287\n",
      "Batch 690,  loss: 1.6071999073028564\n",
      "Batch 695,  loss: 1.8809837579727173\n",
      "Batch 700,  loss: 1.6387848377227783\n",
      "Batch 705,  loss: 1.7901548624038697\n",
      "Batch 710,  loss: 1.9184796571731568\n",
      "Batch 715,  loss: 2.1788206815719606\n",
      "Batch 720,  loss: 1.6781880378723144\n",
      "Batch 725,  loss: 1.525721836090088\n",
      "Batch 730,  loss: 2.0505702972412108\n",
      "Batch 735,  loss: 1.7049617290496826\n",
      "Batch 740,  loss: 1.828861927986145\n",
      "Batch 745,  loss: 1.8106911182403564\n",
      "Batch 750,  loss: 1.4767711400985717\n",
      "Batch 755,  loss: 1.8611979007720947\n",
      "Batch 760,  loss: 2.152198553085327\n",
      "Batch 765,  loss: 1.8429454803466796\n",
      "Batch 770,  loss: 1.821932053565979\n",
      "Batch 775,  loss: 1.9362657070159912\n",
      "Batch 780,  loss: 1.7385056018829346\n",
      "Batch 785,  loss: 2.3357784748077393\n",
      "Batch 790,  loss: 1.8825860977172852\n",
      "Batch 795,  loss: 2.279672455787659\n",
      "Batch 800,  loss: 1.558902096748352\n",
      "Batch 805,  loss: 1.4825561761856079\n",
      "Batch 810,  loss: 2.1827465295791626\n",
      "Batch 815,  loss: 1.8152278423309327\n",
      "Batch 820,  loss: 1.9665756225585938\n",
      "Batch 825,  loss: 1.8150378704071044\n",
      "Batch 830,  loss: 2.0808843612670898\n",
      "Batch 835,  loss: 1.8273134231567383\n",
      "Batch 840,  loss: 1.6044893980026245\n",
      "Batch 845,  loss: 1.991547703742981\n",
      "Batch 850,  loss: 2.244010257720947\n",
      "Batch 855,  loss: 1.6324252128601073\n",
      "Batch 860,  loss: 2.1750075817108154\n",
      "Batch 865,  loss: 1.7629196643829346\n",
      "Batch 870,  loss: 1.9887809038162232\n",
      "Batch 875,  loss: 1.6941184282302857\n",
      "Batch 880,  loss: 2.032578134536743\n",
      "Batch 885,  loss: 1.6275165319442748\n",
      "Batch 890,  loss: 1.9503905057907105\n",
      "Batch 895,  loss: 1.61345374584198\n",
      "Batch 900,  loss: 1.7765042781829834\n",
      "Batch 905,  loss: 1.4742274284362793\n",
      "Batch 910,  loss: 2.091492033004761\n",
      "Batch 915,  loss: 2.090602731704712\n",
      "Batch 920,  loss: 1.8553924083709716\n",
      "Batch 925,  loss: 1.431595754623413\n",
      "Batch 930,  loss: 1.4317898631095887\n",
      "Batch 935,  loss: 1.6602033138275147\n",
      "Batch 940,  loss: 2.418428587913513\n",
      "Batch 945,  loss: 1.8127529859542846\n",
      "Batch 950,  loss: 1.5265372037887572\n",
      "Batch 955,  loss: 2.2095444202423096\n",
      "Batch 960,  loss: 2.0371011018753054\n",
      "Batch 965,  loss: 1.9710325241088866\n",
      "Batch 970,  loss: 2.3320793867111207\n",
      "Batch 975,  loss: 1.7353172540664672\n",
      "Batch 980,  loss: 1.872937512397766\n",
      "Batch 985,  loss: 1.8357083320617675\n",
      "Batch 990,  loss: 2.2157579898834228\n",
      "Batch 995,  loss: 1.4982372522354126\n",
      "Batch 1000,  loss: 1.8529303550720215\n",
      "Batch 1005,  loss: 1.7266713857650757\n",
      "Batch 1010,  loss: 1.794252300262451\n",
      "Batch 1015,  loss: 2.074313759803772\n",
      "Batch 1020,  loss: 1.689096164703369\n",
      "Batch 1025,  loss: 1.9526589870452882\n",
      "Batch 1030,  loss: 1.711724615097046\n",
      "Batch 1035,  loss: 1.8418089866638183\n",
      "Batch 1040,  loss: 1.410732364654541\n",
      "Batch 1045,  loss: 1.942705750465393\n",
      "Batch 1050,  loss: 1.6043544292449952\n",
      "Batch 1055,  loss: 2.057162117958069\n",
      "Batch 1060,  loss: 1.4857298612594605\n",
      "Batch 1065,  loss: 2.077712798118591\n",
      "Batch 1070,  loss: 1.474936294555664\n",
      "Batch 1075,  loss: 1.624136757850647\n",
      "Batch 1080,  loss: 2.0233412742614747\n",
      "Batch 1085,  loss: 1.974589204788208\n",
      "Batch 1090,  loss: 1.5912091970443725\n",
      "Batch 1095,  loss: 1.393632173538208\n",
      "Batch 1100,  loss: 2.0183287382125856\n",
      "Batch 1105,  loss: 1.9322124242782592\n",
      "Batch 1110,  loss: 1.7288816332817079\n",
      "Batch 1115,  loss: 2.271029496192932\n",
      "Batch 1120,  loss: 1.8108897924423217\n",
      "Batch 1125,  loss: 1.8732183933258058\n",
      "Batch 1130,  loss: 2.0692357301712034\n",
      "Batch 1135,  loss: 1.6626432180404662\n",
      "Batch 1140,  loss: 1.9358925104141236\n",
      "Batch 1145,  loss: 2.0445846557617187\n",
      "Batch 1150,  loss: 1.6600630283355713\n",
      "Batch 1155,  loss: 1.9224013090133667\n",
      "Batch 1160,  loss: 1.7614229440689086\n",
      "Batch 1165,  loss: 1.499897313117981\n",
      "Batch 1170,  loss: 1.6415470361709594\n",
      "Batch 1175,  loss: 2.308454751968384\n",
      "Batch 1180,  loss: 2.1262974619865416\n",
      "Batch 1185,  loss: 1.6959853887557983\n",
      "Batch 1190,  loss: 1.672174072265625\n",
      "Batch 1195,  loss: 1.5394320487976074\n",
      "Batch 1200,  loss: 1.9315993785858154\n",
      "Batch 1205,  loss: 2.0725931406021116\n",
      "Batch 1210,  loss: 1.951919937133789\n",
      "Batch 1215,  loss: 1.7723559856414794\n",
      "Batch 1220,  loss: 1.9306299448013307\n",
      "Batch 1225,  loss: 1.879054856300354\n",
      "Batch 1230,  loss: 1.497999334335327\n",
      "Batch 1235,  loss: 1.8562156677246093\n",
      "Batch 1240,  loss: 1.778164792060852\n",
      "Batch 1245,  loss: 1.8631309509277343\n",
      "Batch 1250,  loss: 1.5738665342330933\n",
      "Batch 1255,  loss: 1.847508192062378\n",
      "Batch 1260,  loss: 1.9312536001205445\n",
      "Batch 1265,  loss: 2.082651209831238\n",
      "Batch 1270,  loss: 1.5523329019546508\n",
      "Batch 1275,  loss: 2.275402784347534\n",
      "Batch 1280,  loss: 2.280835509300232\n",
      "Batch 1285,  loss: 1.8796823501586915\n",
      "Batch 1290,  loss: 2.010444235801697\n",
      "Batch 1295,  loss: 1.9577810049057007\n",
      "LOSS train 1.9577810049057007. Validation loss: 2.2925380739624854 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 20:\n",
      "Batch 5,  loss: 1.7013936281204223\n",
      "Batch 10,  loss: 1.8668304920196532\n",
      "Batch 15,  loss: 1.543863296508789\n",
      "Batch 20,  loss: 1.8505660533905028\n",
      "Batch 25,  loss: 1.6000673294067382\n",
      "Batch 30,  loss: 2.1006660223007203\n",
      "Batch 35,  loss: 1.9133297443389892\n",
      "Batch 40,  loss: 1.8824155569076537\n",
      "Batch 45,  loss: 1.8047590255737305\n",
      "Batch 50,  loss: 1.809058141708374\n",
      "Batch 55,  loss: 2.008998394012451\n",
      "Batch 60,  loss: 1.5789583921432495\n",
      "Batch 65,  loss: 1.9308120012283325\n",
      "Batch 70,  loss: 1.8105355978012085\n",
      "Batch 75,  loss: 1.721586489677429\n",
      "Batch 80,  loss: 1.7496665716171265\n",
      "Batch 85,  loss: 1.874027943611145\n",
      "Batch 90,  loss: 1.8558953762054444\n",
      "Batch 95,  loss: 1.6050570726394653\n",
      "Batch 100,  loss: 1.8633317470550537\n",
      "Batch 105,  loss: 2.1898650407791136\n",
      "Batch 110,  loss: 1.9013449907302857\n",
      "Batch 115,  loss: 1.5900772333145141\n",
      "Batch 120,  loss: 2.172473835945129\n",
      "Batch 125,  loss: 1.8831138372421266\n",
      "Batch 130,  loss: 1.690075731277466\n",
      "Batch 135,  loss: 2.0933669090270994\n",
      "Batch 140,  loss: 1.5325992107391357\n",
      "Batch 145,  loss: 1.6900927066802978\n",
      "Batch 150,  loss: 1.5649116396903993\n",
      "Batch 155,  loss: 2.278911566734314\n",
      "Batch 160,  loss: 1.7975361824035645\n",
      "Batch 165,  loss: 1.5939057111740111\n",
      "Batch 170,  loss: 1.8488166570663451\n",
      "Batch 175,  loss: 2.044296455383301\n",
      "Batch 180,  loss: 1.7356007814407348\n",
      "Batch 185,  loss: 1.9313796520233155\n",
      "Batch 190,  loss: 1.5924313068389893\n",
      "Batch 195,  loss: 1.6335545301437377\n",
      "Batch 200,  loss: 1.8929539561271667\n",
      "Batch 205,  loss: 1.6945710897445678\n",
      "Batch 210,  loss: 1.6034439086914063\n",
      "Batch 215,  loss: 2.007994270324707\n",
      "Batch 220,  loss: 2.0899864435195923\n",
      "Batch 225,  loss: 1.506984567642212\n",
      "Batch 230,  loss: 2.1018560886383058\n",
      "Batch 235,  loss: 2.2907081842422485\n",
      "Batch 240,  loss: 1.8094260692596436\n",
      "Batch 245,  loss: 1.4370389223098754\n",
      "Batch 250,  loss: 1.646336269378662\n",
      "Batch 255,  loss: 2.4454219341278076\n",
      "Batch 260,  loss: 2.0994549036026\n",
      "Batch 265,  loss: 1.6604779481887817\n",
      "Batch 270,  loss: 1.9838788032531738\n",
      "Batch 275,  loss: 1.9369360685348511\n",
      "Batch 280,  loss: 1.908812975883484\n",
      "Batch 285,  loss: 2.023538064956665\n",
      "Batch 290,  loss: 2.0375156879425047\n",
      "Batch 295,  loss: 1.7192831993103028\n",
      "Batch 300,  loss: 2.1196155548095703\n",
      "Batch 305,  loss: 1.3434237241744995\n",
      "Batch 310,  loss: 2.0573676109313963\n",
      "Batch 315,  loss: 2.120781898498535\n",
      "Batch 320,  loss: 1.627242136001587\n",
      "Batch 325,  loss: 2.095624232292175\n",
      "Batch 330,  loss: 2.0983347654342652\n",
      "Batch 335,  loss: 1.6360935688018798\n",
      "Batch 340,  loss: 1.88397114276886\n",
      "Batch 345,  loss: 1.6053515434265138\n",
      "Batch 350,  loss: 1.694006896018982\n",
      "Batch 355,  loss: 1.65964195728302\n",
      "Batch 360,  loss: 1.6673559665679931\n",
      "Batch 365,  loss: 2.3953531265258787\n",
      "Batch 370,  loss: 1.5820045948028565\n",
      "Batch 375,  loss: 1.9272980690002441\n",
      "Batch 380,  loss: 1.5643603324890136\n",
      "Batch 385,  loss: 1.6056005001068114\n",
      "Batch 390,  loss: 1.9054028987884521\n",
      "Batch 395,  loss: 2.0575381994247435\n",
      "Batch 400,  loss: 1.5631661295890809\n",
      "Batch 405,  loss: 2.0987860202789306\n",
      "Batch 410,  loss: 2.1336188554763793\n",
      "Batch 415,  loss: 1.8353451013565063\n",
      "Batch 420,  loss: 2.189923310279846\n",
      "Batch 425,  loss: 1.9073817729949951\n",
      "Batch 430,  loss: 1.8291507720947267\n",
      "Batch 435,  loss: 1.5596959471702576\n",
      "Batch 440,  loss: 2.1809664726257325\n",
      "Batch 445,  loss: 1.5615882873535156\n",
      "Batch 450,  loss: 1.8007311820983887\n",
      "Batch 455,  loss: 1.9151087999343872\n",
      "Batch 460,  loss: 1.811936902999878\n",
      "Batch 465,  loss: 2.0219005346298218\n",
      "Batch 470,  loss: 1.5940674066543579\n",
      "Batch 475,  loss: 2.3005160331726073\n",
      "Batch 480,  loss: 1.7149744987487794\n",
      "Batch 485,  loss: 2.1132106304168703\n",
      "Batch 490,  loss: 1.7549754619598388\n",
      "Batch 495,  loss: 1.620459246635437\n",
      "Batch 500,  loss: 1.2715359210968018\n",
      "Batch 505,  loss: 2.380485415458679\n",
      "Batch 510,  loss: 2.1493322610855103\n",
      "Batch 515,  loss: 1.8518916606903075\n",
      "Batch 520,  loss: 1.8740187168121338\n",
      "Batch 525,  loss: 1.8458983421325683\n",
      "Batch 530,  loss: 2.0072884798049926\n",
      "Batch 535,  loss: 1.718899703025818\n",
      "Batch 540,  loss: 1.654061985015869\n",
      "Batch 545,  loss: 1.9497084617614746\n",
      "Batch 550,  loss: 2.092315983772278\n",
      "Batch 555,  loss: 1.9758150100708007\n",
      "Batch 560,  loss: 1.746557879447937\n",
      "Batch 565,  loss: 1.5916635274887085\n",
      "Batch 570,  loss: 1.8855394840240478\n",
      "Batch 575,  loss: 1.6159255981445313\n",
      "Batch 580,  loss: 1.5442408800125123\n",
      "Batch 585,  loss: 1.908677101135254\n",
      "Batch 590,  loss: 1.801936149597168\n",
      "Batch 595,  loss: 2.269000220298767\n",
      "Batch 600,  loss: 2.0101587772369385\n",
      "Batch 605,  loss: 1.7765436887741088\n",
      "Batch 610,  loss: 1.8256606340408326\n",
      "Batch 615,  loss: 2.4109745025634766\n",
      "Batch 620,  loss: 1.5241599559783936\n",
      "Batch 625,  loss: 2.2056353807449343\n",
      "Batch 630,  loss: 2.1958019733428955\n",
      "Batch 635,  loss: 1.9104716539382935\n",
      "Batch 640,  loss: 2.789550852775574\n",
      "Batch 645,  loss: 1.5613913774490356\n",
      "Batch 650,  loss: 2.0809796333312987\n",
      "Batch 655,  loss: 1.7552054166793822\n",
      "Batch 660,  loss: 1.9325273513793946\n",
      "Batch 665,  loss: 1.5131746530532837\n",
      "Batch 670,  loss: 2.0197428941726683\n",
      "Batch 675,  loss: 2.0103925704956054\n",
      "Batch 680,  loss: 1.5254866123199462\n",
      "Batch 685,  loss: 1.8934189319610595\n",
      "Batch 690,  loss: 1.7494285583496094\n",
      "Batch 695,  loss: 1.96873779296875\n",
      "Batch 700,  loss: 1.8027417421340943\n",
      "Batch 705,  loss: 1.8921085357666017\n",
      "Batch 710,  loss: 1.951988124847412\n",
      "Batch 715,  loss: 1.783791184425354\n",
      "Batch 720,  loss: 1.690359902381897\n",
      "Batch 725,  loss: 1.37491135597229\n",
      "Batch 730,  loss: 1.9441104650497436\n",
      "Batch 735,  loss: 2.0895249128341673\n",
      "Batch 740,  loss: 2.0551700592041016\n",
      "Batch 745,  loss: 1.768295407295227\n",
      "Batch 750,  loss: 2.2202515602111816\n",
      "Batch 755,  loss: 1.733484673500061\n",
      "Batch 760,  loss: 1.830654263496399\n",
      "Batch 765,  loss: 1.5546664237976073\n",
      "Batch 770,  loss: 1.695554256439209\n",
      "Batch 775,  loss: 1.8150569200515747\n",
      "Batch 780,  loss: 1.8184476613998413\n",
      "Batch 785,  loss: 2.1356136560440064\n",
      "Batch 790,  loss: 1.7901491284370423\n",
      "Batch 795,  loss: 1.8179156303405761\n",
      "Batch 800,  loss: 1.5340728759765625\n",
      "Batch 805,  loss: 2.0290113210678102\n",
      "Batch 810,  loss: 1.7664114713668824\n",
      "Batch 815,  loss: 1.7242176055908203\n",
      "Batch 820,  loss: 1.6590126514434815\n",
      "Batch 825,  loss: 1.8881627559661864\n",
      "Batch 830,  loss: 1.6737213850021362\n",
      "Batch 835,  loss: 1.8844711065292359\n",
      "Batch 840,  loss: 1.5729562520980835\n",
      "Batch 845,  loss: 1.334315586090088\n",
      "Batch 850,  loss: 1.7015700101852418\n",
      "Batch 855,  loss: 1.8370465278625487\n",
      "Batch 860,  loss: 2.319598150253296\n",
      "Batch 865,  loss: 1.611050271987915\n",
      "Batch 870,  loss: 1.3493489265441894\n",
      "Batch 875,  loss: 1.6152982473373414\n",
      "Batch 880,  loss: 1.9101399421691894\n",
      "Batch 885,  loss: 1.6188523769378662\n",
      "Batch 890,  loss: 1.8621126651763915\n",
      "Batch 895,  loss: 1.9144435167312621\n",
      "Batch 900,  loss: 2.157209539413452\n",
      "Batch 905,  loss: 1.56435284614563\n",
      "Batch 910,  loss: 1.890716314315796\n",
      "Batch 915,  loss: 1.6187034130096436\n",
      "Batch 920,  loss: 1.7035606384277344\n",
      "Batch 925,  loss: 1.7972424030303955\n",
      "Batch 930,  loss: 1.8325586795806885\n",
      "Batch 935,  loss: 1.4811737060546875\n",
      "Batch 940,  loss: 1.8393339395523072\n",
      "Batch 945,  loss: 1.8173418045043945\n",
      "Batch 950,  loss: 1.2187900304794312\n",
      "Batch 955,  loss: 2.1407507419586183\n",
      "Batch 960,  loss: 1.8658573150634765\n",
      "Batch 965,  loss: 1.5200156211853026\n",
      "Batch 970,  loss: 1.8983706712722779\n",
      "Batch 975,  loss: 1.7505069732666017\n",
      "Batch 980,  loss: 1.7041148900985719\n",
      "Batch 985,  loss: 2.0527703046798704\n",
      "Batch 990,  loss: 2.2927016973495484\n",
      "Batch 995,  loss: 1.8087904930114747\n",
      "Batch 1000,  loss: 1.5610589981079102\n",
      "Batch 1005,  loss: 2.144625926017761\n",
      "Batch 1010,  loss: 2.058882713317871\n",
      "Batch 1015,  loss: 1.800294542312622\n",
      "Batch 1020,  loss: 1.766938328742981\n",
      "Batch 1025,  loss: 1.9739529132843017\n",
      "Batch 1030,  loss: 1.9261892795562745\n",
      "Batch 1035,  loss: 1.541822624206543\n",
      "Batch 1040,  loss: 1.572365665435791\n",
      "Batch 1045,  loss: 1.9006466627120973\n",
      "Batch 1050,  loss: 1.6515870571136475\n",
      "Batch 1055,  loss: 1.3496148586273193\n",
      "Batch 1060,  loss: 1.4106337189674378\n",
      "Batch 1065,  loss: 2.082172417640686\n",
      "Batch 1070,  loss: 2.0438637256622316\n",
      "Batch 1075,  loss: 1.762847399711609\n",
      "Batch 1080,  loss: 1.4702147960662841\n",
      "Batch 1085,  loss: 1.9411522626876831\n",
      "Batch 1090,  loss: 2.144816017150879\n",
      "Batch 1095,  loss: 2.299069547653198\n",
      "Batch 1100,  loss: 1.983826208114624\n",
      "Batch 1105,  loss: 1.8429177522659301\n",
      "Batch 1110,  loss: 1.724176001548767\n",
      "Batch 1115,  loss: 1.5103126287460327\n",
      "Batch 1120,  loss: 1.8850587844848632\n",
      "Batch 1125,  loss: 1.9530888557434083\n",
      "Batch 1130,  loss: 1.955647921562195\n",
      "Batch 1135,  loss: 2.255771112442017\n",
      "Batch 1140,  loss: 2.1411983013153075\n",
      "Batch 1145,  loss: 1.9506179809570312\n",
      "Batch 1150,  loss: 1.4799342155456543\n",
      "Batch 1155,  loss: 1.5588299751281738\n",
      "Batch 1160,  loss: 1.7102508783340453\n",
      "Batch 1165,  loss: 2.149513339996338\n",
      "Batch 1170,  loss: 1.6309157609939575\n",
      "Batch 1175,  loss: 1.794460618495941\n",
      "Batch 1180,  loss: 1.717464303970337\n",
      "Batch 1185,  loss: 1.5677248239517212\n",
      "Batch 1190,  loss: 2.04594247341156\n",
      "Batch 1195,  loss: 2.2895790338516235\n",
      "Batch 1200,  loss: 2.1092763423919676\n",
      "Batch 1205,  loss: 1.893081831932068\n",
      "Batch 1210,  loss: 1.778947353363037\n",
      "Batch 1215,  loss: 1.6019644260406494\n",
      "Batch 1220,  loss: 2.1014336585998534\n",
      "Batch 1225,  loss: 1.8944541215896606\n",
      "Batch 1230,  loss: 1.829172396659851\n",
      "Batch 1235,  loss: 1.8755754947662353\n",
      "Batch 1240,  loss: 2.4859081983566282\n",
      "Batch 1245,  loss: 1.9008168220520019\n",
      "Batch 1250,  loss: 1.6813688039779664\n",
      "Batch 1255,  loss: 1.7842720746994019\n",
      "Batch 1260,  loss: 1.6535900831222534\n",
      "Batch 1265,  loss: 1.8894909381866456\n",
      "Batch 1270,  loss: 1.7168812513351441\n",
      "Batch 1275,  loss: 2.4410935401916505\n",
      "Batch 1280,  loss: 1.9389420986175536\n",
      "Batch 1285,  loss: 2.207109522819519\n",
      "Batch 1290,  loss: 2.0880579233169554\n",
      "Batch 1295,  loss: 1.9115480184555054\n",
      "LOSS train 1.9115480184555054. Validation loss: 2.2046971565319433 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 21:\n",
      "Batch 5,  loss: 1.5409985542297364\n",
      "Batch 10,  loss: 1.81283700466156\n",
      "Batch 15,  loss: 2.0686848163604736\n",
      "Batch 20,  loss: 1.7192598581314087\n",
      "Batch 25,  loss: 1.97818865776062\n",
      "Batch 30,  loss: 1.8379525184631347\n",
      "Batch 35,  loss: 1.6524622201919557\n",
      "Batch 40,  loss: 1.7804488658905029\n",
      "Batch 45,  loss: 1.421105408668518\n",
      "Batch 50,  loss: 1.448628568649292\n",
      "Batch 55,  loss: 1.7367169618606568\n",
      "Batch 60,  loss: 2.0519718647003176\n",
      "Batch 65,  loss: 1.649213480949402\n",
      "Batch 70,  loss: 1.9171379804611206\n",
      "Batch 75,  loss: 1.8160135746002197\n",
      "Batch 80,  loss: 1.8213256359100343\n",
      "Batch 85,  loss: 1.7968359470367432\n",
      "Batch 90,  loss: 1.9768890142440796\n",
      "Batch 95,  loss: 2.4615980625152587\n",
      "Batch 100,  loss: 2.03886559009552\n",
      "Batch 105,  loss: 2.4396571636199953\n",
      "Batch 110,  loss: 1.405811643600464\n",
      "Batch 115,  loss: 1.7289848804473877\n",
      "Batch 120,  loss: 1.8795427322387694\n",
      "Batch 125,  loss: 1.5359561443328857\n",
      "Batch 130,  loss: 1.683972430229187\n",
      "Batch 135,  loss: 1.6312368392944336\n",
      "Batch 140,  loss: 1.5689103841781615\n",
      "Batch 145,  loss: 1.647069549560547\n",
      "Batch 150,  loss: 1.7605822801589965\n",
      "Batch 155,  loss: 1.6546310901641845\n",
      "Batch 160,  loss: 1.7854378938674926\n",
      "Batch 165,  loss: 1.9501432657241822\n",
      "Batch 170,  loss: 1.6041343212127686\n",
      "Batch 175,  loss: 1.6602192640304565\n",
      "Batch 180,  loss: 1.7903684616088866\n",
      "Batch 185,  loss: 1.8450120687484741\n",
      "Batch 190,  loss: 1.7900829315185547\n",
      "Batch 195,  loss: 1.7952741622924804\n",
      "Batch 200,  loss: 1.6204222917556763\n",
      "Batch 205,  loss: 1.5178224325180054\n",
      "Batch 210,  loss: 1.7306117773056031\n",
      "Batch 215,  loss: 1.4542988777160644\n",
      "Batch 220,  loss: 1.609381103515625\n",
      "Batch 225,  loss: 1.6052330017089844\n",
      "Batch 230,  loss: 1.4727267265319823\n",
      "Batch 235,  loss: 1.7063136100769043\n",
      "Batch 240,  loss: 1.7347318172454833\n",
      "Batch 245,  loss: 1.4307264328002929\n",
      "Batch 250,  loss: 2.3954359531402587\n",
      "Batch 255,  loss: 1.9054489612579346\n",
      "Batch 260,  loss: 1.6617053747177124\n",
      "Batch 265,  loss: 1.6051724672317504\n",
      "Batch 270,  loss: 1.914242720603943\n",
      "Batch 275,  loss: 1.840966534614563\n",
      "Batch 280,  loss: 1.6013023614883424\n",
      "Batch 285,  loss: 2.198318028450012\n",
      "Batch 290,  loss: 1.8460328102111816\n",
      "Batch 295,  loss: 1.9542423009872436\n",
      "Batch 300,  loss: 2.3854389905929567\n",
      "Batch 305,  loss: 1.7085737705230712\n",
      "Batch 310,  loss: 1.9950742959976195\n",
      "Batch 315,  loss: 1.6283655643463135\n",
      "Batch 320,  loss: 1.7764854669570922\n",
      "Batch 325,  loss: 1.9051294088363648\n",
      "Batch 330,  loss: 1.6865124464035035\n",
      "Batch 335,  loss: 1.895522689819336\n",
      "Batch 340,  loss: 1.8490192413330078\n",
      "Batch 345,  loss: 1.435032320022583\n",
      "Batch 350,  loss: 1.9117945671081542\n",
      "Batch 355,  loss: 1.9875356435775757\n",
      "Batch 360,  loss: 2.028857135772705\n",
      "Batch 365,  loss: 1.838208270072937\n",
      "Batch 370,  loss: 1.7935062408447267\n",
      "Batch 375,  loss: 1.3751275300979615\n",
      "Batch 380,  loss: 2.0173132181167603\n",
      "Batch 385,  loss: 2.118742299079895\n",
      "Batch 390,  loss: 1.7899331092834472\n",
      "Batch 395,  loss: 1.9342992305755615\n",
      "Batch 400,  loss: 1.671259832382202\n",
      "Batch 405,  loss: 2.0386242151260374\n",
      "Batch 410,  loss: 1.6352602958679199\n",
      "Batch 415,  loss: 1.4954224109649659\n",
      "Batch 420,  loss: 1.857262945175171\n",
      "Batch 425,  loss: 2.1310497522354126\n",
      "Batch 430,  loss: 1.9089448690414428\n",
      "Batch 435,  loss: 1.7684566259384156\n",
      "Batch 440,  loss: 1.6656171321868896\n",
      "Batch 445,  loss: 1.970018696784973\n",
      "Batch 450,  loss: 1.7414738893508912\n",
      "Batch 455,  loss: 1.97948215007782\n",
      "Batch 460,  loss: 1.9572153329849242\n",
      "Batch 465,  loss: 1.9698368549346923\n",
      "Batch 470,  loss: 1.5885385751724244\n",
      "Batch 475,  loss: 1.6124953746795654\n",
      "Batch 480,  loss: 1.667530393600464\n",
      "Batch 485,  loss: 1.5519569158554076\n",
      "Batch 490,  loss: 1.6140061140060424\n",
      "Batch 495,  loss: 1.6848500728607179\n",
      "Batch 500,  loss: 1.5312931299209596\n",
      "Batch 505,  loss: 1.9466700315475465\n",
      "Batch 510,  loss: 1.8769736766815186\n",
      "Batch 515,  loss: 1.970424199104309\n",
      "Batch 520,  loss: 2.00127637386322\n",
      "Batch 525,  loss: 2.105989098548889\n",
      "Batch 530,  loss: 2.047015738487244\n",
      "Batch 535,  loss: 1.8257213830947876\n",
      "Batch 540,  loss: 1.96386821269989\n",
      "Batch 545,  loss: 1.9717676877975463\n",
      "Batch 550,  loss: 1.9164559841156006\n",
      "Batch 555,  loss: 1.9191005229949951\n",
      "Batch 560,  loss: 1.7305022716522216\n",
      "Batch 565,  loss: 2.400240731239319\n",
      "Batch 570,  loss: 1.787246870994568\n",
      "Batch 575,  loss: 1.321436309814453\n",
      "Batch 580,  loss: 1.67993061542511\n",
      "Batch 585,  loss: 1.8797366619110107\n",
      "Batch 590,  loss: 1.6946625471115113\n",
      "Batch 595,  loss: 2.0832189083099366\n",
      "Batch 600,  loss: 1.414179229736328\n",
      "Batch 605,  loss: 1.9201564073562623\n",
      "Batch 610,  loss: 2.3108712673187255\n",
      "Batch 615,  loss: 2.026430296897888\n",
      "Batch 620,  loss: 1.8568512439727782\n",
      "Batch 625,  loss: 1.7573415517807007\n",
      "Batch 630,  loss: 1.3390492439270019\n",
      "Batch 635,  loss: 1.885509157180786\n",
      "Batch 640,  loss: 2.095567774772644\n",
      "Batch 645,  loss: 1.4700177192687989\n",
      "Batch 650,  loss: 1.9568101644515992\n",
      "Batch 655,  loss: 1.5997063636779785\n",
      "Batch 660,  loss: 1.6981038808822633\n",
      "Batch 665,  loss: 2.557709789276123\n",
      "Batch 670,  loss: 2.1448952913284303\n",
      "Batch 675,  loss: 1.3674909114837646\n",
      "Batch 680,  loss: 1.9314535975456237\n",
      "Batch 685,  loss: 1.8041219234466552\n",
      "Batch 690,  loss: 1.7093663930892944\n",
      "Batch 695,  loss: 2.016630268096924\n",
      "Batch 700,  loss: 2.021038770675659\n",
      "Batch 705,  loss: 1.6346814393997193\n",
      "Batch 710,  loss: 2.0503602743148805\n",
      "Batch 715,  loss: 1.7864315986633301\n",
      "Batch 720,  loss: 1.6637866973876954\n",
      "Batch 725,  loss: 1.5757852792739868\n",
      "Batch 730,  loss: 1.6779545783996581\n",
      "Batch 735,  loss: 2.0201963424682616\n",
      "Batch 740,  loss: 1.7925278663635253\n",
      "Batch 745,  loss: 2.1857996463775633\n",
      "Batch 750,  loss: 1.4755994081497192\n",
      "Batch 755,  loss: 1.5526822090148926\n",
      "Batch 760,  loss: 1.9295360088348388\n",
      "Batch 765,  loss: 1.4543383836746215\n",
      "Batch 770,  loss: 1.7059248685836792\n",
      "Batch 775,  loss: 1.7142789125442506\n",
      "Batch 780,  loss: 1.3846584796905517\n",
      "Batch 785,  loss: 1.6392178297042848\n",
      "Batch 790,  loss: 1.9290579557418823\n",
      "Batch 795,  loss: 1.8457550525665283\n",
      "Batch 800,  loss: 1.9510886669158936\n",
      "Batch 805,  loss: 2.000971221923828\n",
      "Batch 810,  loss: 1.7412079572677612\n",
      "Batch 815,  loss: 1.8209127902984619\n",
      "Batch 820,  loss: 1.8586510181427003\n",
      "Batch 825,  loss: 1.3577742338180543\n",
      "Batch 830,  loss: 1.4785972356796264\n",
      "Batch 835,  loss: 1.7198811531066895\n",
      "Batch 840,  loss: 1.6460429668426513\n",
      "Batch 845,  loss: 2.144426107406616\n",
      "Batch 850,  loss: 1.853766131401062\n",
      "Batch 855,  loss: 1.8452780723571778\n",
      "Batch 860,  loss: 1.6412096738815307\n",
      "Batch 865,  loss: 1.952687406539917\n",
      "Batch 870,  loss: 2.2857392549514772\n",
      "Batch 875,  loss: 1.677882480621338\n",
      "Batch 880,  loss: 1.8375660300254821\n",
      "Batch 885,  loss: 1.713297176361084\n",
      "Batch 890,  loss: 1.6777098417282104\n",
      "Batch 895,  loss: 1.9684812068939208\n",
      "Batch 900,  loss: 1.4582577466964721\n",
      "Batch 905,  loss: 1.6946994543075562\n",
      "Batch 910,  loss: 2.2195994377136232\n",
      "Batch 915,  loss: 2.4699491024017335\n",
      "Batch 920,  loss: 2.0641100645065307\n",
      "Batch 925,  loss: 2.121008610725403\n",
      "Batch 930,  loss: 2.1609461069107057\n",
      "Batch 935,  loss: 1.5268176555633546\n",
      "Batch 940,  loss: 1.6290392875671387\n",
      "Batch 945,  loss: 1.8508586883544922\n",
      "Batch 950,  loss: 1.734492588043213\n",
      "Batch 955,  loss: 1.5714687347412108\n",
      "Batch 960,  loss: 1.3809147357940674\n",
      "Batch 965,  loss: 1.920929765701294\n",
      "Batch 970,  loss: 1.7514729738235473\n",
      "Batch 975,  loss: 1.7668396472930907\n",
      "Batch 980,  loss: 1.5717262744903564\n",
      "Batch 985,  loss: 1.8405298471450806\n",
      "Batch 990,  loss: 1.4491087913513183\n",
      "Batch 995,  loss: 1.999120020866394\n",
      "Batch 1000,  loss: 1.8648607969284057\n",
      "Batch 1005,  loss: 1.7494465351104735\n",
      "Batch 1010,  loss: 1.649493145942688\n",
      "Batch 1015,  loss: 1.6917160034179688\n",
      "Batch 1020,  loss: 1.6129467248916627\n",
      "Batch 1025,  loss: 2.1140026569366457\n",
      "Batch 1030,  loss: 1.5979696035385131\n",
      "Batch 1035,  loss: 1.5910395860671998\n",
      "Batch 1040,  loss: 2.032645058631897\n",
      "Batch 1045,  loss: 1.7453014135360718\n",
      "Batch 1050,  loss: 1.6648476839065551\n",
      "Batch 1055,  loss: 2.089946913719177\n",
      "Batch 1060,  loss: 1.5770831823348999\n",
      "Batch 1065,  loss: 1.6979193210601806\n",
      "Batch 1070,  loss: 1.7185893774032592\n",
      "Batch 1075,  loss: 1.623847770690918\n",
      "Batch 1080,  loss: 1.7729511618614198\n",
      "Batch 1085,  loss: 1.9123569488525392\n",
      "Batch 1090,  loss: 2.019898843765259\n",
      "Batch 1095,  loss: 1.864148497581482\n",
      "Batch 1100,  loss: 1.7306580543518066\n",
      "Batch 1105,  loss: 1.6156660079956056\n",
      "Batch 1110,  loss: 1.871835970878601\n",
      "Batch 1115,  loss: 1.4930739879608155\n",
      "Batch 1120,  loss: 2.097623085975647\n",
      "Batch 1125,  loss: 1.813122296333313\n",
      "Batch 1130,  loss: 2.3917047500610353\n",
      "Batch 1135,  loss: 1.7264810085296631\n",
      "Batch 1140,  loss: 2.1749165534973143\n",
      "Batch 1145,  loss: 1.9872805595397949\n",
      "Batch 1150,  loss: 1.6646935105323792\n",
      "Batch 1155,  loss: 1.857858657836914\n",
      "Batch 1160,  loss: 1.5804795503616333\n",
      "Batch 1165,  loss: 1.8549329042434692\n",
      "Batch 1170,  loss: 1.5371363401412963\n",
      "Batch 1175,  loss: 1.596010112762451\n",
      "Batch 1180,  loss: 1.6120959997177124\n",
      "Batch 1185,  loss: 1.8940221548080445\n",
      "Batch 1190,  loss: 1.8085634469985963\n",
      "Batch 1195,  loss: 1.7761263847351074\n",
      "Batch 1200,  loss: 1.8743919849395752\n",
      "Batch 1205,  loss: 1.713535690307617\n",
      "Batch 1210,  loss: 2.0961010456085205\n",
      "Batch 1215,  loss: 1.9843634843826294\n",
      "Batch 1220,  loss: 1.9132125854492188\n",
      "Batch 1225,  loss: 1.5833576679229737\n",
      "Batch 1230,  loss: 1.6068790435791016\n",
      "Batch 1235,  loss: 1.5460169076919557\n",
      "Batch 1240,  loss: 1.5328171730041504\n",
      "Batch 1245,  loss: 1.848738670349121\n",
      "Batch 1250,  loss: 1.4645103216171265\n",
      "Batch 1255,  loss: 1.6790716648101807\n",
      "Batch 1260,  loss: 1.781347131729126\n",
      "Batch 1265,  loss: 1.9713471174240111\n",
      "Batch 1270,  loss: 1.6965283036231995\n",
      "Batch 1275,  loss: 1.9599306106567382\n",
      "Batch 1280,  loss: 2.2732083082199095\n",
      "Batch 1285,  loss: 2.0197683334350587\n",
      "Batch 1290,  loss: 2.0678539752960203\n",
      "Batch 1295,  loss: 2.105341410636902\n",
      "LOSS train 2.105341410636902. Validation loss: 2.1594420844240596 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 22:\n",
      "Batch 5,  loss: 1.7651829719543457\n",
      "Batch 10,  loss: 1.5013875722885133\n",
      "Batch 15,  loss: 1.8691270351409912\n",
      "Batch 20,  loss: 1.8644598484039308\n",
      "Batch 25,  loss: 1.4462149620056153\n",
      "Batch 30,  loss: 1.6434202432632445\n",
      "Batch 35,  loss: 1.9475091218948364\n",
      "Batch 40,  loss: 1.5358437299728394\n",
      "Batch 45,  loss: 1.8758142948150636\n",
      "Batch 50,  loss: 1.9585909843444824\n",
      "Batch 55,  loss: 1.9423685550689698\n",
      "Batch 60,  loss: 1.8325525045394897\n",
      "Batch 65,  loss: 1.3828958749771119\n",
      "Batch 70,  loss: 1.9855027675628663\n",
      "Batch 75,  loss: 1.8414433956146241\n",
      "Batch 80,  loss: 1.8010136604309082\n",
      "Batch 85,  loss: 1.24725421667099\n",
      "Batch 90,  loss: 1.6835940599441528\n",
      "Batch 95,  loss: 1.4629982709884644\n",
      "Batch 100,  loss: 1.8651655435562133\n",
      "Batch 105,  loss: 2.1947914123535157\n",
      "Batch 110,  loss: 1.7433030605316162\n",
      "Batch 115,  loss: 1.7555939197540282\n",
      "Batch 120,  loss: 1.4062923192977905\n",
      "Batch 125,  loss: 2.0176020860671997\n",
      "Batch 130,  loss: 2.3608866930007935\n",
      "Batch 135,  loss: 1.8886788606643676\n",
      "Batch 140,  loss: 2.055606174468994\n",
      "Batch 145,  loss: 1.4408264994621276\n",
      "Batch 150,  loss: 2.0247541427612306\n",
      "Batch 155,  loss: 1.7162542343139648\n",
      "Batch 160,  loss: 2.472618007659912\n",
      "Batch 165,  loss: 1.8848048210144044\n",
      "Batch 170,  loss: 1.9669034481048584\n",
      "Batch 175,  loss: 1.6584726572036743\n",
      "Batch 180,  loss: 2.0997824907302856\n",
      "Batch 185,  loss: 1.80899920463562\n",
      "Batch 190,  loss: 1.777990961074829\n",
      "Batch 195,  loss: 1.7956052780151368\n",
      "Batch 200,  loss: 1.7010507822036742\n",
      "Batch 205,  loss: 1.7797489404678344\n",
      "Batch 210,  loss: 2.1010277032852174\n",
      "Batch 215,  loss: 1.996363139152527\n",
      "Batch 220,  loss: 2.0309375047683718\n",
      "Batch 225,  loss: 2.1283957958221436\n",
      "Batch 230,  loss: 1.7499791145324708\n",
      "Batch 235,  loss: 1.8300472497940063\n",
      "Batch 240,  loss: 1.9371583938598633\n",
      "Batch 245,  loss: 1.6698290586471558\n",
      "Batch 250,  loss: 2.007262134552002\n",
      "Batch 255,  loss: 2.024160552024841\n",
      "Batch 260,  loss: 1.5101706266403199\n",
      "Batch 265,  loss: 1.7215947389602662\n",
      "Batch 270,  loss: 2.0805173397064207\n",
      "Batch 275,  loss: 1.7061455011367799\n",
      "Batch 280,  loss: 1.6432560443878175\n",
      "Batch 285,  loss: 1.9111288070678711\n",
      "Batch 290,  loss: 1.5795876741409303\n",
      "Batch 295,  loss: 1.6900145769119264\n",
      "Batch 300,  loss: 2.1486748695373534\n",
      "Batch 305,  loss: 1.4652190923690795\n",
      "Batch 310,  loss: 2.166109013557434\n",
      "Batch 315,  loss: 1.6775208711624146\n",
      "Batch 320,  loss: 1.6990162134170532\n",
      "Batch 325,  loss: 1.7255517482757567\n",
      "Batch 330,  loss: 1.7705955743789672\n",
      "Batch 335,  loss: 1.8774143934249878\n",
      "Batch 340,  loss: 1.3676997661590575\n",
      "Batch 345,  loss: 1.8407960891723634\n",
      "Batch 350,  loss: 2.143090343475342\n",
      "Batch 355,  loss: 1.6595038890838623\n",
      "Batch 360,  loss: 1.8244317770004272\n",
      "Batch 365,  loss: 1.7187148094177247\n",
      "Batch 370,  loss: 1.8952576875686646\n",
      "Batch 375,  loss: 1.6559613466262817\n",
      "Batch 380,  loss: 1.6186466217041016\n",
      "Batch 385,  loss: 1.9684320449829102\n",
      "Batch 390,  loss: 1.6524349927902222\n",
      "Batch 395,  loss: 1.825943684577942\n",
      "Batch 400,  loss: 1.9964631080627442\n",
      "Batch 405,  loss: 1.8811692953109742\n",
      "Batch 410,  loss: 1.594194197654724\n",
      "Batch 415,  loss: 1.9311705589294434\n",
      "Batch 420,  loss: 1.6833065509796143\n",
      "Batch 425,  loss: 1.7055803298950196\n",
      "Batch 430,  loss: 1.6209065675735475\n",
      "Batch 435,  loss: 1.765815830230713\n",
      "Batch 440,  loss: 1.5888562679290772\n",
      "Batch 445,  loss: 1.9802266836166382\n",
      "Batch 450,  loss: 2.1108176708221436\n",
      "Batch 455,  loss: 1.9722207546234132\n",
      "Batch 460,  loss: 1.3104621291160583\n",
      "Batch 465,  loss: 1.6681159734725952\n",
      "Batch 470,  loss: 1.753965139389038\n",
      "Batch 475,  loss: 1.942288875579834\n",
      "Batch 480,  loss: 2.0716777563095095\n",
      "Batch 485,  loss: 1.9539453506469726\n",
      "Batch 490,  loss: 1.4672724246978759\n",
      "Batch 495,  loss: 2.072051692008972\n",
      "Batch 500,  loss: 1.5738681554794312\n",
      "Batch 505,  loss: 1.6221436738967896\n",
      "Batch 510,  loss: 1.9224349021911622\n",
      "Batch 515,  loss: 2.108986759185791\n",
      "Batch 520,  loss: 1.5530728101730347\n",
      "Batch 525,  loss: 1.6237741947174071\n",
      "Batch 530,  loss: 1.6436071753501893\n",
      "Batch 535,  loss: 1.5382579565048218\n",
      "Batch 540,  loss: 2.5287149429321287\n",
      "Batch 545,  loss: 1.5944048404693603\n",
      "Batch 550,  loss: 1.5481216669082642\n",
      "Batch 555,  loss: 1.8402735710144043\n",
      "Batch 560,  loss: 1.7434640288352967\n",
      "Batch 565,  loss: 2.214331316947937\n",
      "Batch 570,  loss: 2.028342866897583\n",
      "Batch 575,  loss: 1.937442684173584\n",
      "Batch 580,  loss: 1.808832550048828\n",
      "Batch 585,  loss: 2.0876052141189576\n",
      "Batch 590,  loss: 1.6511716365814209\n",
      "Batch 595,  loss: 1.783414113521576\n",
      "Batch 600,  loss: 1.8438542366027832\n",
      "Batch 605,  loss: 2.1417281150817873\n",
      "Batch 610,  loss: 2.0704232692718505\n",
      "Batch 615,  loss: 1.6567023515701294\n",
      "Batch 620,  loss: 1.5122711658477783\n",
      "Batch 625,  loss: 1.7229289531707763\n",
      "Batch 630,  loss: 2.0635916948318482\n",
      "Batch 635,  loss: 1.6026407480239868\n",
      "Batch 640,  loss: 1.5709658145904541\n",
      "Batch 645,  loss: 1.983626699447632\n",
      "Batch 650,  loss: 1.6997129440307617\n",
      "Batch 655,  loss: 2.0357771635055544\n",
      "Batch 660,  loss: 2.5403505325317384\n",
      "Batch 665,  loss: 1.8307698249816895\n",
      "Batch 670,  loss: 1.872065019607544\n",
      "Batch 675,  loss: 1.8973225116729737\n",
      "Batch 680,  loss: 1.7082803130149842\n",
      "Batch 685,  loss: 1.4511353731155396\n",
      "Batch 690,  loss: 1.8068050146102905\n",
      "Batch 695,  loss: 1.5458406448364257\n",
      "Batch 700,  loss: 1.7140445232391357\n",
      "Batch 705,  loss: 1.4192746162414551\n",
      "Batch 710,  loss: 1.577007234096527\n",
      "Batch 715,  loss: 1.5730886220932008\n",
      "Batch 720,  loss: 1.6230949401855468\n",
      "Batch 725,  loss: 1.734420895576477\n",
      "Batch 730,  loss: 1.662965679168701\n",
      "Batch 735,  loss: 1.607930302619934\n",
      "Batch 740,  loss: 1.7913649797439575\n",
      "Batch 745,  loss: 1.595626950263977\n",
      "Batch 750,  loss: 1.8457472562789916\n",
      "Batch 755,  loss: 1.8276413440704347\n",
      "Batch 760,  loss: 1.569922184944153\n",
      "Batch 765,  loss: 1.8890295267105102\n",
      "Batch 770,  loss: 1.743048334121704\n",
      "Batch 775,  loss: 1.4741019010543823\n",
      "Batch 780,  loss: 1.2889764070510865\n",
      "Batch 785,  loss: 1.794128966331482\n",
      "Batch 790,  loss: 1.7178166389465332\n",
      "Batch 795,  loss: 1.4493466138839721\n",
      "Batch 800,  loss: 1.8613710880279541\n",
      "Batch 805,  loss: 1.4296391248703002\n",
      "Batch 810,  loss: 1.340708816051483\n",
      "Batch 815,  loss: 1.924041748046875\n",
      "Batch 820,  loss: 1.3287360668182373\n",
      "Batch 825,  loss: 1.5753702640533447\n",
      "Batch 830,  loss: 1.7355700254440307\n",
      "Batch 835,  loss: 2.130341148376465\n",
      "Batch 840,  loss: 1.841012954711914\n",
      "Batch 845,  loss: 1.6673036575317384\n",
      "Batch 850,  loss: 1.9216158151626588\n",
      "Batch 855,  loss: 2.0557965517044066\n",
      "Batch 860,  loss: 1.6536601781845093\n",
      "Batch 865,  loss: 1.8059670448303222\n",
      "Batch 870,  loss: 2.241343307495117\n",
      "Batch 875,  loss: 1.9774836540222167\n",
      "Batch 880,  loss: 2.12372362613678\n",
      "Batch 885,  loss: 1.7549609422683716\n",
      "Batch 890,  loss: 1.668044090270996\n",
      "Batch 895,  loss: 2.108348822593689\n",
      "Batch 900,  loss: 1.7699815273284911\n",
      "Batch 905,  loss: 1.9329922199249268\n",
      "Batch 910,  loss: 1.4408865213394164\n",
      "Batch 915,  loss: 2.147550892829895\n",
      "Batch 920,  loss: 1.548458456993103\n",
      "Batch 925,  loss: 1.8872532367706298\n",
      "Batch 930,  loss: 2.066773200035095\n",
      "Batch 935,  loss: 1.438303279876709\n",
      "Batch 940,  loss: 1.679060935974121\n",
      "Batch 945,  loss: 1.9732059478759765\n",
      "Batch 950,  loss: 1.9004914522171021\n",
      "Batch 955,  loss: 1.5162969827651978\n",
      "Batch 960,  loss: 1.8546890020370483\n",
      "Batch 965,  loss: 1.105596113204956\n",
      "Batch 970,  loss: 1.6106449842453003\n",
      "Batch 975,  loss: 1.5280774116516114\n",
      "Batch 980,  loss: 1.868167757987976\n",
      "Batch 985,  loss: 1.8200911045074464\n",
      "Batch 990,  loss: 1.4577544689178468\n",
      "Batch 995,  loss: 1.7533291101455688\n",
      "Batch 1000,  loss: 1.8967007398605347\n",
      "Batch 1005,  loss: 1.72713143825531\n",
      "Batch 1010,  loss: 1.4984316110610962\n",
      "Batch 1015,  loss: 1.5603085041046143\n",
      "Batch 1020,  loss: 1.8115821599960327\n",
      "Batch 1025,  loss: 1.6421967267990112\n",
      "Batch 1030,  loss: 2.098495841026306\n",
      "Batch 1035,  loss: 1.757959771156311\n",
      "Batch 1040,  loss: 1.633927345275879\n",
      "Batch 1045,  loss: 1.4899763822555543\n",
      "Batch 1050,  loss: 1.5679659605026246\n",
      "Batch 1055,  loss: 1.9020952701568603\n",
      "Batch 1060,  loss: 1.4576278448104858\n",
      "Batch 1065,  loss: 1.541937494277954\n",
      "Batch 1070,  loss: 1.6609060764312744\n",
      "Batch 1075,  loss: 1.3605792760848998\n",
      "Batch 1080,  loss: 2.1862793922424317\n",
      "Batch 1085,  loss: 1.7615661382675172\n",
      "Batch 1090,  loss: 1.5715526342391968\n",
      "Batch 1095,  loss: 1.82288818359375\n",
      "Batch 1100,  loss: 1.7866716146469117\n",
      "Batch 1105,  loss: 2.529755163192749\n",
      "Batch 1110,  loss: 1.7915310859680176\n",
      "Batch 1115,  loss: 2.2510152578353884\n",
      "Batch 1120,  loss: 1.750520896911621\n",
      "Batch 1125,  loss: 1.6703106880187988\n",
      "Batch 1130,  loss: 1.3702383041381836\n",
      "Batch 1135,  loss: 1.9336852550506591\n",
      "Batch 1140,  loss: 1.6518506288528443\n",
      "Batch 1145,  loss: 1.8971230506896972\n",
      "Batch 1150,  loss: 1.6308842897415161\n",
      "Batch 1155,  loss: 1.5931767463684081\n",
      "Batch 1160,  loss: 1.3375594854354858\n",
      "Batch 1165,  loss: 1.8654619216918946\n",
      "Batch 1170,  loss: 1.6936742305755614\n",
      "Batch 1175,  loss: 1.6516085624694825\n",
      "Batch 1180,  loss: 1.3180297017097473\n",
      "Batch 1185,  loss: 1.7340828418731689\n",
      "Batch 1190,  loss: 1.7822757482528686\n",
      "Batch 1195,  loss: 2.0195043087005615\n",
      "Batch 1200,  loss: 2.147513198852539\n",
      "Batch 1205,  loss: 1.550430417060852\n",
      "Batch 1210,  loss: 2.055861067771912\n",
      "Batch 1215,  loss: 1.3438388109207153\n",
      "Batch 1220,  loss: 2.0839077949523928\n",
      "Batch 1225,  loss: 2.0250094652175905\n",
      "Batch 1230,  loss: 1.7668173551559447\n",
      "Batch 1235,  loss: 1.6820122480392456\n",
      "Batch 1240,  loss: 1.5543309926986695\n",
      "Batch 1245,  loss: 1.7263161897659303\n",
      "Batch 1250,  loss: 1.7587299823760987\n",
      "Batch 1255,  loss: 2.485119581222534\n",
      "Batch 1260,  loss: 1.6060173511505127\n",
      "Batch 1265,  loss: 1.984161877632141\n",
      "Batch 1270,  loss: 1.6547261714935302\n",
      "Batch 1275,  loss: 1.417864990234375\n",
      "Batch 1280,  loss: 2.2658339023590086\n",
      "Batch 1285,  loss: 1.512174129486084\n",
      "Batch 1290,  loss: 1.7765376567840576\n",
      "Batch 1295,  loss: 2.121026873588562\n",
      "LOSS train 2.121026873588562. Validation loss: 2.2972482573240995 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 23:\n",
      "Batch 5,  loss: 1.7799851417541503\n",
      "Batch 10,  loss: 2.62795033454895\n",
      "Batch 15,  loss: 2.0618205785751345\n",
      "Batch 20,  loss: 1.9838064193725586\n",
      "Batch 25,  loss: 1.941944169998169\n",
      "Batch 30,  loss: 1.9219811677932739\n",
      "Batch 35,  loss: 2.079175281524658\n",
      "Batch 40,  loss: 1.66660817861557\n",
      "Batch 45,  loss: 1.4634509563446045\n",
      "Batch 50,  loss: 2.0571751832962035\n",
      "Batch 55,  loss: 1.379362666606903\n",
      "Batch 60,  loss: 1.7458133220672607\n",
      "Batch 65,  loss: 1.7077485084533692\n",
      "Batch 70,  loss: 1.6123704671859742\n",
      "Batch 75,  loss: 1.872418475151062\n",
      "Batch 80,  loss: 1.4182757616043091\n",
      "Batch 85,  loss: 2.007221555709839\n",
      "Batch 90,  loss: 1.9328399896621704\n",
      "Batch 95,  loss: 2.1722028493881225\n",
      "Batch 100,  loss: 1.6830888271331788\n",
      "Batch 105,  loss: 1.4407278537750243\n",
      "Batch 110,  loss: 1.8364311933517456\n",
      "Batch 115,  loss: 1.7978678941726685\n",
      "Batch 120,  loss: 1.2807669639587402\n",
      "Batch 125,  loss: 2.0294607639312745\n",
      "Batch 130,  loss: 1.7877007722854614\n",
      "Batch 135,  loss: 1.7863171815872192\n",
      "Batch 140,  loss: 1.8729593992233275\n",
      "Batch 145,  loss: 1.4206727743148804\n",
      "Batch 150,  loss: 2.1035526514053347\n",
      "Batch 155,  loss: 1.6398352146148683\n",
      "Batch 160,  loss: 1.968069887161255\n",
      "Batch 165,  loss: 1.3431792259216309\n",
      "Batch 170,  loss: 1.5296501398086548\n",
      "Batch 175,  loss: 1.9424622297286986\n",
      "Batch 180,  loss: 1.7249458074569701\n",
      "Batch 185,  loss: 1.5473808765411377\n",
      "Batch 190,  loss: 1.5987557888031005\n",
      "Batch 195,  loss: 1.7280760049819945\n",
      "Batch 200,  loss: 1.4765464544296265\n",
      "Batch 205,  loss: 1.7139611005783082\n",
      "Batch 210,  loss: 2.415528893470764\n",
      "Batch 215,  loss: 1.539115881919861\n",
      "Batch 220,  loss: 1.832745575904846\n",
      "Batch 225,  loss: 1.6198021173477173\n",
      "Batch 230,  loss: 1.428402829170227\n",
      "Batch 235,  loss: 1.8965627431869507\n",
      "Batch 240,  loss: 1.8665059089660645\n",
      "Batch 245,  loss: 1.7033263444900513\n",
      "Batch 250,  loss: 1.7610709190368652\n",
      "Batch 255,  loss: 1.6855660438537599\n",
      "Batch 260,  loss: 1.724363422393799\n",
      "Batch 265,  loss: 2.0228387832641603\n",
      "Batch 270,  loss: 1.688767910003662\n",
      "Batch 275,  loss: 1.645164966583252\n",
      "Batch 280,  loss: 1.5621870756149292\n",
      "Batch 285,  loss: 1.9162251949310303\n",
      "Batch 290,  loss: 1.549186372756958\n",
      "Batch 295,  loss: 1.803239607810974\n",
      "Batch 300,  loss: 1.4006338119506836\n",
      "Batch 305,  loss: 1.7418498992919922\n",
      "Batch 310,  loss: 1.45069363117218\n",
      "Batch 315,  loss: 1.7035380125045776\n",
      "Batch 320,  loss: 1.743494439125061\n",
      "Batch 325,  loss: 1.7441709756851196\n",
      "Batch 330,  loss: 1.5663305282592774\n",
      "Batch 335,  loss: 1.7800156354904175\n",
      "Batch 340,  loss: 2.22950074672699\n",
      "Batch 345,  loss: 1.6713609933853149\n",
      "Batch 350,  loss: 1.8824368953704833\n",
      "Batch 355,  loss: 1.3736950874328613\n",
      "Batch 360,  loss: 1.657705807685852\n",
      "Batch 365,  loss: 1.3624013185501098\n",
      "Batch 370,  loss: 1.6343583345413208\n",
      "Batch 375,  loss: 1.8846230030059814\n",
      "Batch 380,  loss: 1.9041232347488404\n",
      "Batch 385,  loss: 1.6154132604598999\n",
      "Batch 390,  loss: 1.5617753267288208\n",
      "Batch 395,  loss: 1.609835147857666\n",
      "Batch 400,  loss: 2.1539217591285706\n",
      "Batch 405,  loss: 1.5661221742630005\n",
      "Batch 410,  loss: 1.9712229132652284\n",
      "Batch 415,  loss: 1.9007513999938965\n",
      "Batch 420,  loss: 1.6446882963180542\n",
      "Batch 425,  loss: 1.634801161289215\n",
      "Batch 430,  loss: 1.6989339113235473\n",
      "Batch 435,  loss: 2.0456894397735597\n",
      "Batch 440,  loss: 1.8951339960098266\n",
      "Batch 445,  loss: 1.4971609115600586\n",
      "Batch 450,  loss: 1.9361630439758302\n",
      "Batch 455,  loss: 1.9075670480728149\n",
      "Batch 460,  loss: 1.8474907875061035\n",
      "Batch 465,  loss: 1.3334036111831664\n",
      "Batch 470,  loss: 1.5219547986984252\n",
      "Batch 475,  loss: 1.503313386440277\n",
      "Batch 480,  loss: 1.5900782823562623\n",
      "Batch 485,  loss: 1.6676079988479615\n",
      "Batch 490,  loss: 2.1248670339584352\n",
      "Batch 495,  loss: 1.5825947999954224\n",
      "Batch 500,  loss: 1.5856271505355835\n",
      "Batch 505,  loss: 1.599918532371521\n",
      "Batch 510,  loss: 1.3855569124221803\n",
      "Batch 515,  loss: 2.008961725234985\n",
      "Batch 520,  loss: 1.8036605358123778\n",
      "Batch 525,  loss: 2.0393739223480223\n",
      "Batch 530,  loss: 2.1448923349380493\n",
      "Batch 535,  loss: 1.9064794540405274\n",
      "Batch 540,  loss: 1.6354164361953736\n",
      "Batch 545,  loss: 2.108459305763245\n",
      "Batch 550,  loss: 2.051760745048523\n",
      "Batch 555,  loss: 1.6000028371810913\n",
      "Batch 560,  loss: 1.6498101234436036\n",
      "Batch 565,  loss: 2.2976218700408935\n",
      "Batch 570,  loss: 1.3542026042938233\n",
      "Batch 575,  loss: 1.6673470735549927\n",
      "Batch 580,  loss: 1.9538890838623046\n",
      "Batch 585,  loss: 1.3968770503997803\n",
      "Batch 590,  loss: 2.0923972368240356\n",
      "Batch 595,  loss: 1.5423582077026368\n",
      "Batch 600,  loss: 1.8137415647506714\n",
      "Batch 605,  loss: 1.6592082500457763\n",
      "Batch 610,  loss: 1.807069230079651\n",
      "Batch 615,  loss: 1.9774413585662842\n",
      "Batch 620,  loss: 1.6310750722885132\n",
      "Batch 625,  loss: 2.26536545753479\n",
      "Batch 630,  loss: 1.797246241569519\n",
      "Batch 635,  loss: 1.4339746475219726\n",
      "Batch 640,  loss: 1.7587541818618775\n",
      "Batch 645,  loss: 1.2630818605422973\n",
      "Batch 650,  loss: 1.7849635839462281\n",
      "Batch 655,  loss: 1.6580201148986817\n",
      "Batch 660,  loss: 1.5262123823165894\n",
      "Batch 665,  loss: 2.006408166885376\n",
      "Batch 670,  loss: 1.6026666641235352\n",
      "Batch 675,  loss: 1.7604274988174438\n",
      "Batch 680,  loss: 1.9996283769607544\n",
      "Batch 685,  loss: 1.8703984260559081\n",
      "Batch 690,  loss: 1.7073681354522705\n",
      "Batch 695,  loss: 1.7099677324295044\n",
      "Batch 700,  loss: 1.7367249608039856\n",
      "Batch 705,  loss: 2.516433095932007\n",
      "Batch 710,  loss: 1.8729466915130615\n",
      "Batch 715,  loss: 1.7500485897064209\n",
      "Batch 720,  loss: 1.5217609405517578\n",
      "Batch 725,  loss: 2.0781976222991942\n",
      "Batch 730,  loss: 1.5240263938903809\n",
      "Batch 735,  loss: 1.6006341934204102\n",
      "Batch 740,  loss: 1.5200788497924804\n",
      "Batch 745,  loss: 1.7767302751541139\n",
      "Batch 750,  loss: 1.5759453058242798\n",
      "Batch 755,  loss: 1.919358992576599\n",
      "Batch 760,  loss: 2.3265687465667724\n",
      "Batch 765,  loss: 1.6647375345230102\n",
      "Batch 770,  loss: 1.8683879375457764\n",
      "Batch 775,  loss: 1.5662133693695068\n",
      "Batch 780,  loss: 1.850255799293518\n",
      "Batch 785,  loss: 1.7170605659484863\n",
      "Batch 790,  loss: 1.7917168140411377\n",
      "Batch 795,  loss: 1.7562394857406616\n",
      "Batch 800,  loss: 1.448807668685913\n",
      "Batch 805,  loss: 1.595985221862793\n",
      "Batch 810,  loss: 1.6089966773986817\n",
      "Batch 815,  loss: 1.9677526473999023\n",
      "Batch 820,  loss: 1.5916732788085937\n",
      "Batch 825,  loss: 1.6511082887649535\n",
      "Batch 830,  loss: 1.5255093574523926\n",
      "Batch 835,  loss: 1.8180130958557128\n",
      "Batch 840,  loss: 1.6018677711486817\n",
      "Batch 845,  loss: 1.5070653438568116\n",
      "Batch 850,  loss: 1.7925030708312988\n",
      "Batch 855,  loss: 1.5187673568725586\n",
      "Batch 860,  loss: 1.429999589920044\n",
      "Batch 865,  loss: 1.789154314994812\n",
      "Batch 870,  loss: 1.5641448736190795\n",
      "Batch 875,  loss: 1.665444803237915\n",
      "Batch 880,  loss: 1.1245360612869262\n",
      "Batch 885,  loss: 1.5357255697250367\n",
      "Batch 890,  loss: 1.2202608585357666\n",
      "Batch 895,  loss: 1.4245843648910523\n",
      "Batch 900,  loss: 1.7533344984054566\n",
      "Batch 905,  loss: 1.6326848030090333\n",
      "Batch 910,  loss: 2.025728678703308\n",
      "Batch 915,  loss: 1.59747713804245\n",
      "Batch 920,  loss: 2.0117126226425173\n",
      "Batch 925,  loss: 2.107400345802307\n",
      "Batch 930,  loss: 1.7080947637557984\n",
      "Batch 935,  loss: 1.6060296297073364\n",
      "Batch 940,  loss: 1.7444793462753296\n",
      "Batch 945,  loss: 1.7008279085159301\n",
      "Batch 950,  loss: 1.569616174697876\n",
      "Batch 955,  loss: 1.9256734609603883\n",
      "Batch 960,  loss: 1.6345427989959718\n",
      "Batch 965,  loss: 2.127806305885315\n",
      "Batch 970,  loss: 1.5117450952529907\n",
      "Batch 975,  loss: 2.0108206987380983\n",
      "Batch 980,  loss: 1.8957920789718627\n",
      "Batch 985,  loss: 1.4306031227111817\n",
      "Batch 990,  loss: 1.946719717979431\n",
      "Batch 995,  loss: 1.682833981513977\n",
      "Batch 1000,  loss: 1.4324942111968995\n",
      "Batch 1005,  loss: 1.6585566043853759\n",
      "Batch 1010,  loss: 1.5197856187820435\n",
      "Batch 1015,  loss: 1.3609681963920592\n",
      "Batch 1020,  loss: 1.5535633206367492\n",
      "Batch 1025,  loss: 1.9976125597953795\n",
      "Batch 1030,  loss: 1.7130623817443849\n",
      "Batch 1035,  loss: 2.0234399795532227\n",
      "Batch 1040,  loss: 1.9490355253219604\n",
      "Batch 1045,  loss: 1.811051368713379\n",
      "Batch 1050,  loss: 1.534447979927063\n",
      "Batch 1055,  loss: 1.9492228507995606\n",
      "Batch 1060,  loss: 1.929416799545288\n",
      "Batch 1065,  loss: 1.7974151372909546\n",
      "Batch 1070,  loss: 2.1612712383270263\n",
      "Batch 1075,  loss: 2.025146245956421\n",
      "Batch 1080,  loss: 1.5311666488647462\n",
      "Batch 1085,  loss: 1.764493751525879\n",
      "Batch 1090,  loss: 2.069726896286011\n",
      "Batch 1095,  loss: 1.4502179145812988\n",
      "Batch 1100,  loss: 1.591057014465332\n",
      "Batch 1105,  loss: 1.7641130447387696\n",
      "Batch 1110,  loss: 1.3996681690216064\n",
      "Batch 1115,  loss: 1.7281654834747315\n",
      "Batch 1120,  loss: 1.467661714553833\n",
      "Batch 1125,  loss: 1.5181323766708374\n",
      "Batch 1130,  loss: 1.5844353914260865\n",
      "Batch 1135,  loss: 1.7335199117660522\n",
      "Batch 1140,  loss: 1.8102418422698974\n",
      "Batch 1145,  loss: 1.2950216293334962\n",
      "Batch 1150,  loss: 1.57549946308136\n",
      "Batch 1155,  loss: 1.5749799251556396\n",
      "Batch 1160,  loss: 1.702957320213318\n",
      "Batch 1165,  loss: 2.1827247858047487\n",
      "Batch 1170,  loss: 2.113741135597229\n",
      "Batch 1175,  loss: 1.7715381145477296\n",
      "Batch 1180,  loss: 1.4646028995513916\n",
      "Batch 1185,  loss: 1.4380779266357422\n",
      "Batch 1190,  loss: 1.4678454399108887\n",
      "Batch 1195,  loss: 1.5767313957214355\n",
      "Batch 1200,  loss: 1.9907311916351318\n",
      "Batch 1205,  loss: 2.0990418910980226\n",
      "Batch 1210,  loss: 1.8421885013580321\n",
      "Batch 1215,  loss: 2.0783045053482057\n",
      "Batch 1220,  loss: 2.0669506072998045\n",
      "Batch 1225,  loss: 2.1656065702438356\n",
      "Batch 1230,  loss: 1.4500908136367798\n",
      "Batch 1235,  loss: 1.5702208280563354\n",
      "Batch 1240,  loss: 1.8462170720100404\n",
      "Batch 1245,  loss: 1.8147760391235352\n",
      "Batch 1250,  loss: 1.8382112979888916\n",
      "Batch 1255,  loss: 1.7234216690063477\n",
      "Batch 1260,  loss: 1.447493028640747\n",
      "Batch 1265,  loss: 1.9703576564788818\n",
      "Batch 1270,  loss: 2.2672091007232664\n",
      "Batch 1275,  loss: 1.4958725690841674\n",
      "Batch 1280,  loss: 2.2053412437438964\n",
      "Batch 1285,  loss: 1.5554210901260377\n",
      "Batch 1290,  loss: 1.910841202735901\n",
      "Batch 1295,  loss: 1.9793184518814086\n",
      "LOSS train 1.9793184518814086. Validation loss: 2.2138360135992383 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 24:\n",
      "Batch 5,  loss: 1.9736845374107361\n",
      "Batch 10,  loss: 1.8995182514190674\n",
      "Batch 15,  loss: 2.2019123315811155\n",
      "Batch 20,  loss: 2.1498176574707033\n",
      "Batch 25,  loss: 1.827184796333313\n",
      "Batch 30,  loss: 1.5868533611297608\n",
      "Batch 35,  loss: 2.0713747262954714\n",
      "Batch 40,  loss: 2.068761372566223\n",
      "Batch 45,  loss: 1.5307448148727416\n",
      "Batch 50,  loss: 2.113927435874939\n",
      "Batch 55,  loss: 2.006186318397522\n",
      "Batch 60,  loss: 1.838927721977234\n",
      "Batch 65,  loss: 1.4272391557693482\n",
      "Batch 70,  loss: 1.6228124141693114\n",
      "Batch 75,  loss: 1.5921289443969726\n",
      "Batch 80,  loss: 1.9831573724746705\n",
      "Batch 85,  loss: 1.7546519994735719\n",
      "Batch 90,  loss: 2.161017823219299\n",
      "Batch 95,  loss: 1.5991596460342408\n",
      "Batch 100,  loss: 1.623507595062256\n",
      "Batch 105,  loss: 1.3153571605682373\n",
      "Batch 110,  loss: 1.5780073404312134\n",
      "Batch 115,  loss: 1.8569111108779908\n",
      "Batch 120,  loss: 1.331384038925171\n",
      "Batch 125,  loss: 1.6724270820617675\n",
      "Batch 130,  loss: 1.6628145217895507\n",
      "Batch 135,  loss: 1.5688319206237793\n",
      "Batch 140,  loss: 1.9301468849182128\n",
      "Batch 145,  loss: 1.6309364080429076\n",
      "Batch 150,  loss: 1.7181870937347412\n",
      "Batch 155,  loss: 2.081388258934021\n",
      "Batch 160,  loss: 1.775933074951172\n",
      "Batch 165,  loss: 1.8995044231414795\n",
      "Batch 170,  loss: 2.0257100105285644\n",
      "Batch 175,  loss: 1.6034419775009154\n",
      "Batch 180,  loss: 2.2106207847595214\n",
      "Batch 185,  loss: 1.9599050283432007\n",
      "Batch 190,  loss: 1.6968505382537842\n",
      "Batch 195,  loss: 1.8044060230255128\n",
      "Batch 200,  loss: 2.349989414215088\n",
      "Batch 205,  loss: 1.8732942342758179\n",
      "Batch 210,  loss: 1.7244313716888429\n",
      "Batch 215,  loss: 1.6201907634735107\n",
      "Batch 220,  loss: 1.7261883974075318\n",
      "Batch 225,  loss: 1.5875108242034912\n",
      "Batch 230,  loss: 1.699292778968811\n",
      "Batch 235,  loss: 1.8867703914642333\n",
      "Batch 240,  loss: 1.5217303514480591\n",
      "Batch 245,  loss: 2.200652599334717\n",
      "Batch 250,  loss: 2.076405668258667\n",
      "Batch 255,  loss: 1.5519121646881104\n",
      "Batch 260,  loss: 1.7318553447723388\n",
      "Batch 265,  loss: 1.5830074787139892\n",
      "Batch 270,  loss: 1.8329402446746825\n",
      "Batch 275,  loss: 1.6621598243713378\n",
      "Batch 280,  loss: 2.0035285711288453\n",
      "Batch 285,  loss: 1.8635632157325746\n",
      "Batch 290,  loss: 1.529562032222748\n",
      "Batch 295,  loss: 1.8300714015960693\n",
      "Batch 300,  loss: 1.8845601081848145\n",
      "Batch 305,  loss: 1.346913504600525\n",
      "Batch 310,  loss: 1.7809681177139283\n",
      "Batch 315,  loss: 1.6244247674942016\n",
      "Batch 320,  loss: 1.7065407991409303\n",
      "Batch 325,  loss: 1.592481756210327\n",
      "Batch 330,  loss: 1.6199520587921143\n",
      "Batch 335,  loss: 1.5952791929244996\n",
      "Batch 340,  loss: 1.4956822156906129\n",
      "Batch 345,  loss: 1.4226702451705933\n",
      "Batch 350,  loss: 1.4606922388076782\n",
      "Batch 355,  loss: 1.7145915269851684\n",
      "Batch 360,  loss: 1.2778931856155396\n",
      "Batch 365,  loss: 1.5036911249160767\n",
      "Batch 370,  loss: 1.8446077585220337\n",
      "Batch 375,  loss: 1.3068277359008789\n",
      "Batch 380,  loss: 1.2830913066864014\n",
      "Batch 385,  loss: 1.6544938564300538\n",
      "Batch 390,  loss: 1.7108379364013673\n",
      "Batch 395,  loss: 2.0087714910507204\n",
      "Batch 400,  loss: 1.7403845548629762\n",
      "Batch 405,  loss: 1.9684142112731933\n",
      "Batch 410,  loss: 1.9571996927261353\n",
      "Batch 415,  loss: 1.6167211055755615\n",
      "Batch 420,  loss: 1.7013678550720215\n",
      "Batch 425,  loss: 1.8170501232147216\n",
      "Batch 430,  loss: 1.4622804164886474\n",
      "Batch 435,  loss: 1.5682944178581237\n",
      "Batch 440,  loss: 2.0770942687988283\n",
      "Batch 445,  loss: 1.7187087059020996\n",
      "Batch 450,  loss: 2.381085824966431\n",
      "Batch 455,  loss: 1.6202659130096435\n",
      "Batch 460,  loss: 1.4310369729995727\n",
      "Batch 465,  loss: 1.817493772506714\n",
      "Batch 470,  loss: 1.6446566820144652\n",
      "Batch 475,  loss: 1.6406791448593139\n",
      "Batch 480,  loss: 1.811443853378296\n",
      "Batch 485,  loss: 1.9705626964569092\n",
      "Batch 490,  loss: 1.4258402347564698\n",
      "Batch 495,  loss: 1.567508053779602\n",
      "Batch 500,  loss: 1.7030168294906616\n",
      "Batch 505,  loss: 1.674943733215332\n",
      "Batch 510,  loss: 1.3955738186836242\n",
      "Batch 515,  loss: 1.55396249294281\n",
      "Batch 520,  loss: 1.242884349822998\n",
      "Batch 525,  loss: 1.8348666667938232\n",
      "Batch 530,  loss: 1.5186272621154786\n",
      "Batch 535,  loss: 1.6014166355133057\n",
      "Batch 540,  loss: 1.684792447090149\n",
      "Batch 545,  loss: 1.6324660539627076\n",
      "Batch 550,  loss: 1.473378074169159\n",
      "Batch 555,  loss: 2.089857745170593\n",
      "Batch 560,  loss: 1.551656222343445\n",
      "Batch 565,  loss: 1.6407157182693481\n",
      "Batch 570,  loss: 1.5426423311233521\n",
      "Batch 575,  loss: 1.7418300747871398\n",
      "Batch 580,  loss: 1.7488810062408446\n",
      "Batch 585,  loss: 1.6288586139678956\n",
      "Batch 590,  loss: 1.8244417905807495\n",
      "Batch 595,  loss: 1.9973154067993164\n",
      "Batch 600,  loss: 2.3305012464523314\n",
      "Batch 605,  loss: 2.0206183791160583\n",
      "Batch 610,  loss: 1.615130615234375\n",
      "Batch 615,  loss: 1.3569120645523072\n",
      "Batch 620,  loss: 2.043133592605591\n",
      "Batch 625,  loss: 2.042629933357239\n",
      "Batch 630,  loss: 1.8238044738769532\n",
      "Batch 635,  loss: 1.9912923336029054\n",
      "Batch 640,  loss: 1.773420810699463\n",
      "Batch 645,  loss: 1.9595101118087768\n",
      "Batch 650,  loss: 1.3388872861862182\n",
      "Batch 655,  loss: 1.8330322742462157\n",
      "Batch 660,  loss: 1.9785571813583374\n",
      "Batch 665,  loss: 1.7737711668014526\n",
      "Batch 670,  loss: 1.6866093635559083\n",
      "Batch 675,  loss: 1.8562475919723511\n",
      "Batch 680,  loss: 1.616648054122925\n",
      "Batch 685,  loss: 1.8774268865585326\n",
      "Batch 690,  loss: 1.4589736700057983\n",
      "Batch 695,  loss: 1.5945812106132506\n",
      "Batch 700,  loss: 1.7323420286178588\n",
      "Batch 705,  loss: 1.6316869974136352\n",
      "Batch 710,  loss: 2.0537609577178957\n",
      "Batch 715,  loss: 1.4636088132858276\n",
      "Batch 720,  loss: 1.6815331697463989\n",
      "Batch 725,  loss: 2.0132949113845826\n",
      "Batch 730,  loss: 1.573176670074463\n",
      "Batch 735,  loss: 1.9917244911193848\n",
      "Batch 740,  loss: 1.6979746103286744\n",
      "Batch 745,  loss: 1.7907013416290283\n",
      "Batch 750,  loss: 1.5269556760787963\n",
      "Batch 755,  loss: 1.7035908460617066\n",
      "Batch 760,  loss: 1.6401932001113892\n",
      "Batch 765,  loss: 1.910288143157959\n",
      "Batch 770,  loss: 2.0754273891448975\n",
      "Batch 775,  loss: 1.889906406402588\n",
      "Batch 780,  loss: 1.9378336191177368\n",
      "Batch 785,  loss: 2.2423797845840454\n",
      "Batch 790,  loss: 1.7193681716918945\n",
      "Batch 795,  loss: 1.5515359401702882\n",
      "Batch 800,  loss: 1.32256520986557\n",
      "Batch 805,  loss: 1.642340898513794\n",
      "Batch 810,  loss: 1.8927730798721314\n",
      "Batch 815,  loss: 1.6014549493789674\n",
      "Batch 820,  loss: 1.689931035041809\n",
      "Batch 825,  loss: 1.9704633712768556\n",
      "Batch 830,  loss: 1.8585741996765137\n",
      "Batch 835,  loss: 2.0099007844924928\n",
      "Batch 840,  loss: 2.2765687704086304\n",
      "Batch 845,  loss: 1.6250546455383301\n",
      "Batch 850,  loss: 1.7250884771347046\n",
      "Batch 855,  loss: 2.2113420963287354\n",
      "Batch 860,  loss: 1.7372763872146606\n",
      "Batch 865,  loss: 1.7954582929611207\n",
      "Batch 870,  loss: 1.8347620487213134\n",
      "Batch 875,  loss: 1.8252126812934875\n",
      "Batch 880,  loss: 1.7259452581405639\n",
      "Batch 885,  loss: 1.6118050813674927\n",
      "Batch 890,  loss: 1.5706145763397217\n",
      "Batch 895,  loss: 2.1733218669891357\n",
      "Batch 900,  loss: 1.4854407787322998\n",
      "Batch 905,  loss: 1.6161680698394776\n",
      "Batch 910,  loss: 1.3926331758499146\n",
      "Batch 915,  loss: 1.6186699628829957\n",
      "Batch 920,  loss: 2.0235228300094605\n",
      "Batch 925,  loss: 1.728114366531372\n",
      "Batch 930,  loss: 1.7280840396881103\n",
      "Batch 935,  loss: 1.7081727743148805\n",
      "Batch 940,  loss: 1.8237590312957763\n",
      "Batch 945,  loss: 1.5765546679496765\n",
      "Batch 950,  loss: 1.4482852458953857\n",
      "Batch 955,  loss: 1.7449005365371704\n",
      "Batch 960,  loss: 1.5017319202423096\n",
      "Batch 965,  loss: 1.5914008140563964\n",
      "Batch 970,  loss: 1.8859005451202393\n",
      "Batch 975,  loss: 1.5198076009750365\n",
      "Batch 980,  loss: 1.757843589782715\n",
      "Batch 985,  loss: 1.66331148147583\n",
      "Batch 990,  loss: 1.8283840894699097\n",
      "Batch 995,  loss: 1.3607784509658813\n",
      "Batch 1000,  loss: 1.6691716194152832\n",
      "Batch 1005,  loss: 1.7212443113327027\n",
      "Batch 1010,  loss: 1.893327283859253\n",
      "Batch 1015,  loss: 1.9838606595993042\n",
      "Batch 1020,  loss: 1.485980010032654\n",
      "Batch 1025,  loss: 1.678354024887085\n",
      "Batch 1030,  loss: 1.735379695892334\n",
      "Batch 1035,  loss: 1.7772421836853027\n",
      "Batch 1040,  loss: 1.466497826576233\n",
      "Batch 1045,  loss: 2.2361765623092653\n",
      "Batch 1050,  loss: 1.6639634609222411\n",
      "Batch 1055,  loss: 1.423032546043396\n",
      "Batch 1060,  loss: 2.1043996810913086\n",
      "Batch 1065,  loss: 1.7838807821273803\n",
      "Batch 1070,  loss: 1.643744945526123\n",
      "Batch 1075,  loss: 1.3896896839141846\n",
      "Batch 1080,  loss: 1.1160125851631164\n",
      "Batch 1085,  loss: 1.9100783824920655\n",
      "Batch 1090,  loss: 1.5960304737091064\n",
      "Batch 1095,  loss: 1.7507260322570801\n",
      "Batch 1100,  loss: 1.5555386781692504\n",
      "Batch 1105,  loss: 1.9687935829162597\n",
      "Batch 1110,  loss: 1.41022732257843\n",
      "Batch 1115,  loss: 1.6855979919433595\n",
      "Batch 1120,  loss: 1.619196105003357\n",
      "Batch 1125,  loss: 2.09594886302948\n",
      "Batch 1130,  loss: 1.6488914728164672\n",
      "Batch 1135,  loss: 1.5781165599822997\n",
      "Batch 1140,  loss: 1.5959572315216064\n",
      "Batch 1145,  loss: 1.6681866645812988\n",
      "Batch 1150,  loss: 1.5669354915618896\n",
      "Batch 1155,  loss: 1.7287706136703491\n",
      "Batch 1160,  loss: 1.6142932176589966\n",
      "Batch 1165,  loss: 1.9991727828979493\n",
      "Batch 1170,  loss: 1.7111458778381348\n",
      "Batch 1175,  loss: 1.8960432291030884\n",
      "Batch 1180,  loss: 1.2661264896392823\n",
      "Batch 1185,  loss: 1.7327154636383058\n",
      "Batch 1190,  loss: 1.5987336874008178\n",
      "Batch 1195,  loss: 1.6094536781311035\n",
      "Batch 1200,  loss: 1.619183611869812\n",
      "Batch 1205,  loss: 1.3879808187484741\n",
      "Batch 1210,  loss: 1.8394435167312622\n",
      "Batch 1215,  loss: 1.3471804976463317\n",
      "Batch 1220,  loss: 1.6181164979934692\n",
      "Batch 1225,  loss: 1.368604350090027\n",
      "Batch 1230,  loss: 1.6898790359497071\n",
      "Batch 1235,  loss: 1.7767433643341064\n",
      "Batch 1240,  loss: 2.055914211273193\n",
      "Batch 1245,  loss: 1.9601123571395873\n",
      "Batch 1250,  loss: 2.02936327457428\n",
      "Batch 1255,  loss: 1.7136264324188233\n",
      "Batch 1260,  loss: 1.4074570417404175\n",
      "Batch 1265,  loss: 1.9280555248260498\n",
      "Batch 1270,  loss: 1.7076866626739502\n",
      "Batch 1275,  loss: 1.5517353296279908\n",
      "Batch 1280,  loss: 1.807681679725647\n",
      "Batch 1285,  loss: 1.6683919906616211\n",
      "Batch 1290,  loss: 1.4062589168548585\n",
      "Batch 1295,  loss: 1.3616137742996215\n",
      "LOSS train 1.3616137742996215. Validation loss: 1.9953702776968756 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 25:\n",
      "Batch 5,  loss: 1.614457106590271\n",
      "Batch 10,  loss: 1.837838363647461\n",
      "Batch 15,  loss: 1.9203858852386475\n",
      "Batch 20,  loss: 1.6053268432617187\n",
      "Batch 25,  loss: 1.8565540552139281\n",
      "Batch 30,  loss: 1.9280366897583008\n",
      "Batch 35,  loss: 1.8379406213760376\n",
      "Batch 40,  loss: 1.7642508506774903\n",
      "Batch 45,  loss: 1.9663718461990356\n",
      "Batch 50,  loss: 1.833083152770996\n",
      "Batch 55,  loss: 1.6413419246673584\n",
      "Batch 60,  loss: 1.7674866199493409\n",
      "Batch 65,  loss: 1.4154018878936767\n",
      "Batch 70,  loss: 1.4877022743225097\n",
      "Batch 75,  loss: 1.7113815307617188\n",
      "Batch 80,  loss: 1.8008544921875\n",
      "Batch 85,  loss: 1.9814886569976806\n",
      "Batch 90,  loss: 1.598961853981018\n",
      "Batch 95,  loss: 1.9551941871643066\n",
      "Batch 100,  loss: 1.899380087852478\n",
      "Batch 105,  loss: 1.6342782258987427\n",
      "Batch 110,  loss: 1.6895928263664246\n",
      "Batch 115,  loss: 2.2203081130981444\n",
      "Batch 120,  loss: 1.9480486869812013\n",
      "Batch 125,  loss: 2.1105894804000855\n",
      "Batch 130,  loss: 1.6176506519317626\n",
      "Batch 135,  loss: 1.389159631729126\n",
      "Batch 140,  loss: 1.8742394208908082\n",
      "Batch 145,  loss: 1.6753293514251708\n",
      "Batch 150,  loss: 1.9582859516143798\n",
      "Batch 155,  loss: 1.655968427658081\n",
      "Batch 160,  loss: 1.503774881362915\n",
      "Batch 165,  loss: 1.75606427192688\n",
      "Batch 170,  loss: 2.180785083770752\n",
      "Batch 175,  loss: 1.8173532724380492\n",
      "Batch 180,  loss: 1.5688041210174561\n",
      "Batch 185,  loss: 1.8076654434204102\n",
      "Batch 190,  loss: 1.3177761077880858\n",
      "Batch 195,  loss: 1.2641900539398194\n",
      "Batch 200,  loss: 1.7055150508880614\n",
      "Batch 205,  loss: 1.460209274291992\n",
      "Batch 210,  loss: 1.5349123001098632\n",
      "Batch 215,  loss: 1.9212318181991577\n",
      "Batch 220,  loss: 1.7367879152297974\n",
      "Batch 225,  loss: 1.37108473777771\n",
      "Batch 230,  loss: 1.3341081380844115\n",
      "Batch 235,  loss: 1.6738128185272216\n",
      "Batch 240,  loss: 1.7219798803329467\n",
      "Batch 245,  loss: 1.87776780128479\n",
      "Batch 250,  loss: 1.5858330488204957\n",
      "Batch 255,  loss: 1.6791649103164672\n",
      "Batch 260,  loss: 1.9716336011886597\n",
      "Batch 265,  loss: 1.7625373601913452\n",
      "Batch 270,  loss: 1.6509714126586914\n",
      "Batch 275,  loss: 1.6403696298599244\n",
      "Batch 280,  loss: 1.5086241722106934\n",
      "Batch 285,  loss: 1.5783694744110108\n",
      "Batch 290,  loss: 1.622812032699585\n",
      "Batch 295,  loss: 1.8065680265426636\n",
      "Batch 300,  loss: 1.752166748046875\n",
      "Batch 305,  loss: 1.493521237373352\n",
      "Batch 310,  loss: 1.686312198638916\n",
      "Batch 315,  loss: 1.3286131381988526\n",
      "Batch 320,  loss: 1.7946365833282472\n",
      "Batch 325,  loss: 1.7347342729568482\n",
      "Batch 330,  loss: 1.7248433113098145\n",
      "Batch 335,  loss: 1.599588966369629\n",
      "Batch 340,  loss: 1.5458238124847412\n",
      "Batch 345,  loss: 1.459803032875061\n",
      "Batch 350,  loss: 1.563585090637207\n",
      "Batch 355,  loss: 1.454956066608429\n",
      "Batch 360,  loss: 1.6107552289962768\n",
      "Batch 365,  loss: 1.9586282968521118\n",
      "Batch 370,  loss: 1.1408353090286254\n",
      "Batch 375,  loss: 1.7334155321121216\n",
      "Batch 380,  loss: 1.7814873933792115\n",
      "Batch 385,  loss: 1.7122454643249512\n",
      "Batch 390,  loss: 1.8683005332946778\n",
      "Batch 395,  loss: 1.6060129404067993\n",
      "Batch 400,  loss: 1.5004526615142821\n",
      "Batch 405,  loss: 1.8217913627624511\n",
      "Batch 410,  loss: 1.6971945285797119\n",
      "Batch 415,  loss: 1.5706670761108399\n",
      "Batch 420,  loss: 1.5147040605545044\n",
      "Batch 425,  loss: 1.6356862545013429\n",
      "Batch 430,  loss: 1.483676028251648\n",
      "Batch 435,  loss: 1.815157127380371\n",
      "Batch 440,  loss: 1.9422443389892579\n",
      "Batch 445,  loss: 1.6046458721160888\n",
      "Batch 450,  loss: 1.8303444623947143\n",
      "Batch 455,  loss: 1.8011232614517212\n",
      "Batch 460,  loss: 1.3375551462173463\n",
      "Batch 465,  loss: 1.7614880084991456\n",
      "Batch 470,  loss: 1.6646917819976808\n",
      "Batch 475,  loss: 1.6164713859558106\n",
      "Batch 480,  loss: 1.8853883266448974\n",
      "Batch 485,  loss: 1.6191940546035766\n",
      "Batch 490,  loss: 1.4263786554336548\n",
      "Batch 495,  loss: 1.5614489793777466\n",
      "Batch 500,  loss: 2.1267592668533326\n",
      "Batch 505,  loss: 1.8511629581451416\n",
      "Batch 510,  loss: 1.5686219453811645\n",
      "Batch 515,  loss: 1.7686192274093628\n",
      "Batch 520,  loss: 1.9516728878021241\n",
      "Batch 525,  loss: 2.0113430976867677\n",
      "Batch 530,  loss: 1.7672976970672607\n",
      "Batch 535,  loss: 1.8316952228546142\n",
      "Batch 540,  loss: 1.7703999996185302\n",
      "Batch 545,  loss: 1.5596550345420837\n",
      "Batch 550,  loss: 1.6339772939682007\n",
      "Batch 555,  loss: 1.6309743165969848\n",
      "Batch 560,  loss: 1.3745011806488037\n",
      "Batch 565,  loss: 1.6560696840286255\n",
      "Batch 570,  loss: 1.6841861963272096\n",
      "Batch 575,  loss: 1.7231073141098023\n",
      "Batch 580,  loss: 1.863256287574768\n",
      "Batch 585,  loss: 1.8477893829345704\n",
      "Batch 590,  loss: 1.8006959438323975\n",
      "Batch 595,  loss: 1.372217345237732\n",
      "Batch 600,  loss: 1.4869372606277467\n",
      "Batch 605,  loss: 1.3855449914932252\n",
      "Batch 610,  loss: 1.5401948928833007\n",
      "Batch 615,  loss: 1.948812770843506\n",
      "Batch 620,  loss: 1.8774511575698853\n",
      "Batch 625,  loss: 1.9602410078048706\n",
      "Batch 630,  loss: 2.1804492235183717\n",
      "Batch 635,  loss: 1.5188655614852906\n",
      "Batch 640,  loss: 1.2306525945663451\n",
      "Batch 645,  loss: 1.5499066591262818\n",
      "Batch 650,  loss: 2.024290084838867\n",
      "Batch 655,  loss: 1.544202971458435\n",
      "Batch 660,  loss: 1.4511236667633056\n",
      "Batch 665,  loss: 1.331326162815094\n",
      "Batch 670,  loss: 1.7959742307662965\n",
      "Batch 675,  loss: 1.855208659172058\n",
      "Batch 680,  loss: 1.4674397706985474\n",
      "Batch 685,  loss: 1.9468924760818482\n",
      "Batch 690,  loss: 1.6475888967514039\n",
      "Batch 695,  loss: 1.584929347038269\n",
      "Batch 700,  loss: 1.8337918043136596\n",
      "Batch 705,  loss: 1.8599841356277467\n",
      "Batch 710,  loss: 1.4306260704994203\n",
      "Batch 715,  loss: 1.8793662071228028\n",
      "Batch 720,  loss: 1.6569059848785401\n",
      "Batch 725,  loss: 1.3535897135734558\n",
      "Batch 730,  loss: 1.3793849229812623\n",
      "Batch 735,  loss: 2.0815087795257567\n",
      "Batch 740,  loss: 1.6081551790237427\n",
      "Batch 745,  loss: 1.5032244205474854\n",
      "Batch 750,  loss: 1.5905632972717285\n",
      "Batch 755,  loss: 1.955531644821167\n",
      "Batch 760,  loss: 1.867000985145569\n",
      "Batch 765,  loss: 1.4667583227157592\n",
      "Batch 770,  loss: 1.7749791622161866\n",
      "Batch 775,  loss: 1.7811397314071655\n",
      "Batch 780,  loss: 1.7667169570922852\n",
      "Batch 785,  loss: 1.5148672819137574\n",
      "Batch 790,  loss: 2.048352026939392\n",
      "Batch 795,  loss: 1.3396456241607666\n",
      "Batch 800,  loss: 1.752256941795349\n",
      "Batch 805,  loss: 1.8082088470458983\n",
      "Batch 810,  loss: 1.9133269786834717\n",
      "Batch 815,  loss: 1.901368832588196\n",
      "Batch 820,  loss: 1.731804323196411\n",
      "Batch 825,  loss: 1.5164270401000977\n",
      "Batch 830,  loss: 2.0367477893829347\n",
      "Batch 835,  loss: 1.7432393789291383\n",
      "Batch 840,  loss: 1.48864004611969\n",
      "Batch 845,  loss: 1.5214409470558166\n",
      "Batch 850,  loss: 1.709357738494873\n",
      "Batch 855,  loss: 1.3681379556655884\n",
      "Batch 860,  loss: 1.6365119218826294\n",
      "Batch 865,  loss: 1.5420633554458618\n",
      "Batch 870,  loss: 1.767911434173584\n",
      "Batch 875,  loss: 1.7870465755462646\n",
      "Batch 880,  loss: 1.2883538246154784\n",
      "Batch 885,  loss: 1.59691002368927\n",
      "Batch 890,  loss: 1.4617301225662231\n",
      "Batch 895,  loss: 1.7387036323547362\n",
      "Batch 900,  loss: 1.6628203868865967\n",
      "Batch 905,  loss: 1.796087658405304\n",
      "Batch 910,  loss: 1.1852698802947998\n",
      "Batch 915,  loss: 1.800665593147278\n",
      "Batch 920,  loss: 2.0665087699890137\n",
      "Batch 925,  loss: 1.9318622589111327\n",
      "Batch 930,  loss: 1.7037179470062256\n",
      "Batch 935,  loss: 1.9180171489715576\n",
      "Batch 940,  loss: 1.7692007303237915\n",
      "Batch 945,  loss: 1.946777057647705\n",
      "Batch 950,  loss: 1.520833134651184\n",
      "Batch 955,  loss: 1.497510600090027\n",
      "Batch 960,  loss: 1.7757021188735962\n",
      "Batch 965,  loss: 1.4593655347824097\n",
      "Batch 970,  loss: 1.7530493974685668\n",
      "Batch 975,  loss: 1.7130778074264525\n",
      "Batch 980,  loss: 1.950180673599243\n",
      "Batch 985,  loss: 1.6360419988632202\n",
      "Batch 990,  loss: 1.6492083311080932\n",
      "Batch 995,  loss: 1.4737449169158936\n",
      "Batch 1000,  loss: 1.7542422533035278\n",
      "Batch 1005,  loss: 1.9142250299453736\n",
      "Batch 1010,  loss: 1.6772205591201783\n",
      "Batch 1015,  loss: 1.8309895038604735\n",
      "Batch 1020,  loss: 1.708772850036621\n",
      "Batch 1025,  loss: 1.5365117549896241\n",
      "Batch 1030,  loss: 1.4966490507125854\n",
      "Batch 1035,  loss: 1.6013190507888795\n",
      "Batch 1040,  loss: 1.6866292715072633\n",
      "Batch 1045,  loss: 1.78704674243927\n",
      "Batch 1050,  loss: 1.5155398845672607\n",
      "Batch 1055,  loss: 1.6116609454154969\n",
      "Batch 1060,  loss: 1.599356460571289\n",
      "Batch 1065,  loss: 1.872111439704895\n",
      "Batch 1070,  loss: 1.7113783597946166\n",
      "Batch 1075,  loss: 1.6601627349853516\n",
      "Batch 1080,  loss: 1.5771957397460938\n",
      "Batch 1085,  loss: 1.5540536880493163\n",
      "Batch 1090,  loss: 1.2240130186080933\n",
      "Batch 1095,  loss: 2.140952134132385\n",
      "Batch 1100,  loss: 1.5857839345932008\n",
      "Batch 1105,  loss: 2.1213605403900146\n",
      "Batch 1110,  loss: 1.5351246833801269\n",
      "Batch 1115,  loss: 1.4697945833206176\n",
      "Batch 1120,  loss: 1.7949503421783448\n",
      "Batch 1125,  loss: 1.7683848857879638\n",
      "Batch 1130,  loss: 1.4576093196868896\n",
      "Batch 1135,  loss: 1.566804838180542\n",
      "Batch 1140,  loss: 1.4560255527496337\n",
      "Batch 1145,  loss: 1.2583584547042848\n",
      "Batch 1150,  loss: 1.4018452763557434\n",
      "Batch 1155,  loss: 1.791377568244934\n",
      "Batch 1160,  loss: 1.5895264148712158\n",
      "Batch 1165,  loss: 1.7050812244415283\n",
      "Batch 1170,  loss: 1.854332399368286\n",
      "Batch 1175,  loss: 1.4198649644851684\n",
      "Batch 1180,  loss: 1.5702373027801513\n",
      "Batch 1185,  loss: 1.7895855903625488\n",
      "Batch 1190,  loss: 1.9887283325195313\n",
      "Batch 1195,  loss: 1.540725541114807\n",
      "Batch 1200,  loss: 1.7835200786590577\n",
      "Batch 1205,  loss: 1.533612847328186\n",
      "Batch 1210,  loss: 1.6278779983520508\n",
      "Batch 1215,  loss: 2.006706249713898\n",
      "Batch 1220,  loss: 1.6696603059768678\n",
      "Batch 1225,  loss: 1.7739671468734741\n",
      "Batch 1230,  loss: 1.792267656326294\n",
      "Batch 1235,  loss: 1.868779683113098\n",
      "Batch 1240,  loss: 1.2652340412139893\n",
      "Batch 1245,  loss: 1.6479432106018066\n",
      "Batch 1250,  loss: 1.2820198774337768\n",
      "Batch 1255,  loss: 1.8307079792022705\n",
      "Batch 1260,  loss: 1.812550461292267\n",
      "Batch 1265,  loss: 1.5543658018112183\n",
      "Batch 1270,  loss: 1.5537245512008666\n",
      "Batch 1275,  loss: 1.6290496468544007\n",
      "Batch 1280,  loss: 1.4763257026672363\n",
      "Batch 1285,  loss: 1.8062843322753905\n",
      "Batch 1290,  loss: 1.6627243041992188\n",
      "Batch 1295,  loss: 1.5177939891815186\n",
      "LOSS train 1.5177939891815186. Validation loss: 2.0371984280645847 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 26:\n",
      "Batch 5,  loss: 1.9748081684112548\n",
      "Batch 10,  loss: 1.5565761566162108\n",
      "Batch 15,  loss: 1.511880135536194\n",
      "Batch 20,  loss: 1.6243226051330566\n",
      "Batch 25,  loss: 1.4248133182525635\n",
      "Batch 30,  loss: 1.3763532161712646\n",
      "Batch 35,  loss: 1.5665300607681274\n",
      "Batch 40,  loss: 1.8560306787490846\n",
      "Batch 45,  loss: 1.816916584968567\n",
      "Batch 50,  loss: 1.8129109621047974\n",
      "Batch 55,  loss: 1.5313453912734984\n",
      "Batch 60,  loss: 1.5961838960647583\n",
      "Batch 65,  loss: 1.7214030981063844\n",
      "Batch 70,  loss: 2.0160001516342163\n",
      "Batch 75,  loss: 1.4452929973602295\n",
      "Batch 80,  loss: 1.7670188188552856\n",
      "Batch 85,  loss: 2.040671730041504\n",
      "Batch 90,  loss: 1.440988826751709\n",
      "Batch 95,  loss: 1.4548294067382812\n",
      "Batch 100,  loss: 1.7755120635032653\n",
      "Batch 105,  loss: 1.4763411045074464\n",
      "Batch 110,  loss: 1.4377310514450072\n",
      "Batch 115,  loss: 1.320285439491272\n",
      "Batch 120,  loss: 1.5996993064880372\n",
      "Batch 125,  loss: 1.5127671957015991\n",
      "Batch 130,  loss: 1.8007684707641602\n",
      "Batch 135,  loss: 1.6613812685012816\n",
      "Batch 140,  loss: 1.8644567728042603\n",
      "Batch 145,  loss: 1.6125563621520995\n",
      "Batch 150,  loss: 1.4229931831359863\n",
      "Batch 155,  loss: 1.7984380722045898\n",
      "Batch 160,  loss: 1.820527768135071\n",
      "Batch 165,  loss: 1.7310693264007568\n",
      "Batch 170,  loss: 1.6574061155319213\n",
      "Batch 175,  loss: 1.3433271646499634\n",
      "Batch 180,  loss: 1.8189931392669678\n",
      "Batch 185,  loss: 1.503983247280121\n",
      "Batch 190,  loss: 1.682361650466919\n",
      "Batch 195,  loss: 1.9213684797286987\n",
      "Batch 200,  loss: 1.9237414598464966\n",
      "Batch 205,  loss: 2.2384692668914794\n",
      "Batch 210,  loss: 1.6299275159835815\n",
      "Batch 215,  loss: 1.6095913410186768\n",
      "Batch 220,  loss: 1.956283736228943\n",
      "Batch 225,  loss: 1.5716956615448\n",
      "Batch 230,  loss: 1.7210856914520263\n",
      "Batch 235,  loss: 1.5920881509780884\n",
      "Batch 240,  loss: 1.4828089714050292\n",
      "Batch 245,  loss: 1.4669397830963136\n",
      "Batch 250,  loss: 1.2472529888153077\n",
      "Batch 255,  loss: 1.4734918594360351\n",
      "Batch 260,  loss: 1.4760855913162232\n",
      "Batch 265,  loss: 1.7780088424682616\n",
      "Batch 270,  loss: 1.4891068935394287\n",
      "Batch 275,  loss: 1.5831881999969482\n",
      "Batch 280,  loss: 1.647021770477295\n",
      "Batch 285,  loss: 1.5561585307121277\n",
      "Batch 290,  loss: 1.676252007484436\n",
      "Batch 295,  loss: 2.122292733192444\n",
      "Batch 300,  loss: 1.678148078918457\n",
      "Batch 305,  loss: 1.6071215867996216\n",
      "Batch 310,  loss: 1.3498649597167969\n",
      "Batch 315,  loss: 2.2892882347106935\n",
      "Batch 320,  loss: 1.5003207921981812\n",
      "Batch 325,  loss: 1.9777852058410645\n",
      "Batch 330,  loss: 1.8391834259033204\n",
      "Batch 335,  loss: 1.5150780916213988\n",
      "Batch 340,  loss: 1.4095842361450195\n",
      "Batch 345,  loss: 1.6282686948776246\n",
      "Batch 350,  loss: 1.7043991804122924\n",
      "Batch 355,  loss: 1.8158471822738647\n",
      "Batch 360,  loss: 1.7840623617172242\n",
      "Batch 365,  loss: 2.0660154581069947\n",
      "Batch 370,  loss: 1.776198673248291\n",
      "Batch 375,  loss: 1.6487937688827514\n",
      "Batch 380,  loss: 1.5024833917617797\n",
      "Batch 385,  loss: 1.3026419878005981\n",
      "Batch 390,  loss: 1.8094792127609254\n",
      "Batch 395,  loss: 1.2731297254562377\n",
      "Batch 400,  loss: 1.311071801185608\n",
      "Batch 405,  loss: 1.3282120943069458\n",
      "Batch 410,  loss: 1.525334906578064\n",
      "Batch 415,  loss: 1.7262549638748168\n",
      "Batch 420,  loss: 1.746479368209839\n",
      "Batch 425,  loss: 1.4016491055488587\n",
      "Batch 430,  loss: 1.3663354158401488\n",
      "Batch 435,  loss: 1.1813800573348998\n",
      "Batch 440,  loss: 1.4337090253829956\n",
      "Batch 445,  loss: 1.415712261199951\n",
      "Batch 450,  loss: 1.5894549489021301\n",
      "Batch 455,  loss: 1.3769319176673889\n",
      "Batch 460,  loss: 1.577593445777893\n",
      "Batch 465,  loss: 1.5394128799438476\n",
      "Batch 470,  loss: 1.6012723684310912\n",
      "Batch 475,  loss: 1.7834933280944825\n",
      "Batch 480,  loss: 1.7068488359451295\n",
      "Batch 485,  loss: 1.8121040105819701\n",
      "Batch 490,  loss: 1.437153959274292\n",
      "Batch 495,  loss: 1.6840589761734008\n",
      "Batch 500,  loss: 1.820164108276367\n",
      "Batch 505,  loss: 1.7690348386764527\n",
      "Batch 510,  loss: 1.778360629081726\n",
      "Batch 515,  loss: 1.7026495456695556\n",
      "Batch 520,  loss: 2.1219953060150147\n",
      "Batch 525,  loss: 1.7720269680023193\n",
      "Batch 530,  loss: 1.7187908411026\n",
      "Batch 535,  loss: 2.176129412651062\n",
      "Batch 540,  loss: 1.6732414484024047\n",
      "Batch 545,  loss: 1.6891814947128296\n",
      "Batch 550,  loss: 1.5277619123458863\n",
      "Batch 555,  loss: 2.0492118120193483\n",
      "Batch 560,  loss: 1.4232263565063477\n",
      "Batch 565,  loss: 1.6203272819519043\n",
      "Batch 570,  loss: 1.7308382749557496\n",
      "Batch 575,  loss: 2.4065715312957763\n",
      "Batch 580,  loss: 1.5811964988708496\n",
      "Batch 585,  loss: 1.5443274259567261\n",
      "Batch 590,  loss: 1.4321550607681275\n",
      "Batch 595,  loss: 1.5273683786392211\n",
      "Batch 600,  loss: 1.4954275608062744\n",
      "Batch 605,  loss: 1.4853111743927\n",
      "Batch 610,  loss: 1.8333493709564208\n",
      "Batch 615,  loss: 1.906203293800354\n",
      "Batch 620,  loss: 1.267389214038849\n",
      "Batch 625,  loss: 1.6708526849746703\n",
      "Batch 630,  loss: 1.4787023067474365\n",
      "Batch 635,  loss: 2.028555178642273\n",
      "Batch 640,  loss: 1.8532753467559815\n",
      "Batch 645,  loss: 1.8612354516983032\n",
      "Batch 650,  loss: 1.8450397729873658\n",
      "Batch 655,  loss: 1.6468276500701904\n",
      "Batch 660,  loss: 1.3220307230949402\n",
      "Batch 665,  loss: 1.8106605768203736\n",
      "Batch 670,  loss: 2.2669039011001586\n",
      "Batch 675,  loss: 1.9771307229995727\n",
      "Batch 680,  loss: 1.6177931785583497\n",
      "Batch 685,  loss: 2.199830484390259\n",
      "Batch 690,  loss: 1.6029060363769532\n",
      "Batch 695,  loss: 1.6418872833251954\n",
      "Batch 700,  loss: 1.6115982294082642\n",
      "Batch 705,  loss: 2.076797890663147\n",
      "Batch 710,  loss: 1.690758264064789\n",
      "Batch 715,  loss: 1.8244346380233765\n",
      "Batch 720,  loss: 1.7060606479644775\n",
      "Batch 725,  loss: 1.6322403430938721\n",
      "Batch 730,  loss: 1.5058361530303954\n",
      "Batch 735,  loss: 1.463406777381897\n",
      "Batch 740,  loss: 1.665428066253662\n",
      "Batch 745,  loss: 1.5430473566055298\n",
      "Batch 750,  loss: 1.8247658252716064\n",
      "Batch 755,  loss: 1.82870032787323\n",
      "Batch 760,  loss: 1.7018087387084961\n",
      "Batch 765,  loss: 1.4276075601577758\n",
      "Batch 770,  loss: 1.8530544281005858\n",
      "Batch 775,  loss: 1.8641670227050782\n",
      "Batch 780,  loss: 1.6458832502365113\n",
      "Batch 785,  loss: 1.6221960306167602\n",
      "Batch 790,  loss: 1.6470139741897583\n",
      "Batch 795,  loss: 1.887383794784546\n",
      "Batch 800,  loss: 1.6239994525909425\n",
      "Batch 805,  loss: 1.677364420890808\n",
      "Batch 810,  loss: 1.7124045372009278\n",
      "Batch 815,  loss: 1.4981281518936158\n",
      "Batch 820,  loss: 1.6966189861297607\n",
      "Batch 825,  loss: 1.6632220268249511\n",
      "Batch 830,  loss: 1.8136383771896363\n",
      "Batch 835,  loss: 1.75197274684906\n",
      "Batch 840,  loss: 1.8178893327713013\n",
      "Batch 845,  loss: 1.7825899124145508\n",
      "Batch 850,  loss: 1.4184325456619262\n",
      "Batch 855,  loss: 1.465023159980774\n",
      "Batch 860,  loss: 1.6418752670288086\n",
      "Batch 865,  loss: 1.417853045463562\n",
      "Batch 870,  loss: 1.873599886894226\n",
      "Batch 875,  loss: 1.801454758644104\n",
      "Batch 880,  loss: 2.0278399705886843\n",
      "Batch 885,  loss: 2.0795101642608644\n",
      "Batch 890,  loss: 1.8856566190719604\n",
      "Batch 895,  loss: 1.5154122352600097\n",
      "Batch 900,  loss: 2.1361415863037108\n",
      "Batch 905,  loss: 1.7175201177597046\n",
      "Batch 910,  loss: 1.7966329336166382\n",
      "Batch 915,  loss: 2.176776313781738\n",
      "Batch 920,  loss: 1.602737832069397\n",
      "Batch 925,  loss: 1.4739176511764527\n",
      "Batch 930,  loss: 1.7029775381088257\n",
      "Batch 935,  loss: 1.5845303773880004\n",
      "Batch 940,  loss: 1.5233651161193849\n",
      "Batch 945,  loss: 1.7735855340957642\n",
      "Batch 950,  loss: 1.6977799654006958\n",
      "Batch 955,  loss: 1.4354390144348144\n",
      "Batch 960,  loss: 1.3902536869049071\n",
      "Batch 965,  loss: 1.6187658309936523\n",
      "Batch 970,  loss: 1.673302412033081\n",
      "Batch 975,  loss: 1.4572434663772582\n",
      "Batch 980,  loss: 1.8979464292526245\n",
      "Batch 985,  loss: 1.7235520601272583\n",
      "Batch 990,  loss: 1.7175356864929199\n",
      "Batch 995,  loss: 1.4995617866516113\n",
      "Batch 1000,  loss: 2.131065106391907\n",
      "Batch 1005,  loss: 1.4157995462417603\n",
      "Batch 1010,  loss: 1.5258255481719971\n",
      "Batch 1015,  loss: 2.120958995819092\n",
      "Batch 1020,  loss: 1.7449529647827149\n",
      "Batch 1025,  loss: 1.3887871980667115\n",
      "Batch 1030,  loss: 1.6773700714111328\n",
      "Batch 1035,  loss: 1.3834527015686036\n",
      "Batch 1040,  loss: 1.8051272630691528\n",
      "Batch 1045,  loss: 1.5142880439758302\n",
      "Batch 1050,  loss: 1.5949517011642456\n",
      "Batch 1055,  loss: 1.465793514251709\n",
      "Batch 1060,  loss: 1.4701890707015992\n",
      "Batch 1065,  loss: 1.5444294214248657\n",
      "Batch 1070,  loss: 1.3777025938034058\n",
      "Batch 1075,  loss: 1.5640978813171387\n",
      "Batch 1080,  loss: 2.0083813428878785\n",
      "Batch 1085,  loss: 1.1761300683021545\n",
      "Batch 1090,  loss: 1.8955849409103394\n",
      "Batch 1095,  loss: 1.6499587297439575\n",
      "Batch 1100,  loss: 1.7769286155700683\n",
      "Batch 1105,  loss: 1.701193356513977\n",
      "Batch 1110,  loss: 1.76070556640625\n",
      "Batch 1115,  loss: 1.718931531906128\n",
      "Batch 1120,  loss: 1.5283787488937377\n",
      "Batch 1125,  loss: 1.824473261833191\n",
      "Batch 1130,  loss: 1.7246346473693848\n",
      "Batch 1135,  loss: 1.2760424971580506\n",
      "Batch 1140,  loss: 1.837876319885254\n",
      "Batch 1145,  loss: 1.525226640701294\n",
      "Batch 1150,  loss: 1.5689064741134644\n",
      "Batch 1155,  loss: 1.62865207195282\n",
      "Batch 1160,  loss: 1.4473290205001832\n",
      "Batch 1165,  loss: 1.8220008850097655\n",
      "Batch 1170,  loss: 1.9556022882461548\n",
      "Batch 1175,  loss: 1.5052690029144287\n",
      "Batch 1180,  loss: 1.7760130405426025\n",
      "Batch 1185,  loss: 1.4363324642181396\n",
      "Batch 1190,  loss: 1.9530221700668335\n",
      "Batch 1195,  loss: 1.872258734703064\n",
      "Batch 1200,  loss: 2.0577616930007934\n",
      "Batch 1205,  loss: 2.1177610874176027\n",
      "Batch 1210,  loss: 1.4885002851486206\n",
      "Batch 1215,  loss: 1.4928266525268554\n",
      "Batch 1220,  loss: 1.6373812437057496\n",
      "Batch 1225,  loss: 1.7046742677688598\n",
      "Batch 1230,  loss: 1.7740217685699462\n",
      "Batch 1235,  loss: 1.891549324989319\n",
      "Batch 1240,  loss: 1.51796395778656\n",
      "Batch 1245,  loss: 1.822459888458252\n",
      "Batch 1250,  loss: 1.6973515748977661\n",
      "Batch 1255,  loss: 1.849966835975647\n",
      "Batch 1260,  loss: 1.7354219198226928\n",
      "Batch 1265,  loss: 1.3473315238952637\n",
      "Batch 1270,  loss: 1.7597156763076782\n",
      "Batch 1275,  loss: 1.7010440587997437\n",
      "Batch 1280,  loss: 1.4974791526794433\n",
      "Batch 1285,  loss: 1.3565287470817566\n",
      "Batch 1290,  loss: 1.7263524293899537\n",
      "Batch 1295,  loss: 2.1704375028610228\n",
      "LOSS train 2.1704375028610228. Validation loss: 2.119599663183369 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 27:\n",
      "Batch 5,  loss: 1.4953563451766967\n",
      "Batch 10,  loss: 1.5190828084945678\n",
      "Batch 15,  loss: 1.465447759628296\n",
      "Batch 20,  loss: 1.3514106035232545\n",
      "Batch 25,  loss: 1.9224321365356445\n",
      "Batch 30,  loss: 1.3088201522827148\n",
      "Batch 35,  loss: 1.3811888217926025\n",
      "Batch 40,  loss: 1.9437819719314575\n",
      "Batch 45,  loss: 1.4479714393615724\n",
      "Batch 50,  loss: 1.5687695741653442\n",
      "Batch 55,  loss: 1.539094877243042\n",
      "Batch 60,  loss: 1.6829254388809205\n",
      "Batch 65,  loss: 1.7266699075698853\n",
      "Batch 70,  loss: 1.5540133476257325\n",
      "Batch 75,  loss: 1.5281049728393554\n",
      "Batch 80,  loss: 2.0697696924209597\n",
      "Batch 85,  loss: 2.05926673412323\n",
      "Batch 90,  loss: 1.9055813789367675\n",
      "Batch 95,  loss: 1.213865613937378\n",
      "Batch 100,  loss: 1.5689141750335693\n",
      "Batch 105,  loss: 1.704649019241333\n",
      "Batch 110,  loss: 1.7687397718429565\n",
      "Batch 115,  loss: 1.71412250995636\n",
      "Batch 120,  loss: 1.6521925926208496\n",
      "Batch 125,  loss: 1.439738917350769\n",
      "Batch 130,  loss: 1.6483148097991944\n",
      "Batch 135,  loss: 1.5601905107498169\n",
      "Batch 140,  loss: 1.4407389879226684\n",
      "Batch 145,  loss: 1.5802586793899536\n",
      "Batch 150,  loss: 1.8006126642227174\n",
      "Batch 155,  loss: 1.9502676486968995\n",
      "Batch 160,  loss: 1.838121247291565\n",
      "Batch 165,  loss: 2.260764217376709\n",
      "Batch 170,  loss: 1.4125832915306091\n",
      "Batch 175,  loss: 1.421968412399292\n",
      "Batch 180,  loss: 1.579269027709961\n",
      "Batch 185,  loss: 1.7038152694702149\n",
      "Batch 190,  loss: 1.6389688968658447\n",
      "Batch 195,  loss: 1.439237856864929\n",
      "Batch 200,  loss: 1.5046244859695435\n",
      "Batch 205,  loss: 1.4654091596603394\n",
      "Batch 210,  loss: 1.5682267665863037\n",
      "Batch 215,  loss: 1.6098148345947265\n",
      "Batch 220,  loss: 1.5648528695106507\n",
      "Batch 225,  loss: 1.6199728012084962\n",
      "Batch 230,  loss: 1.9828795909881591\n",
      "Batch 235,  loss: 1.8397071361541748\n",
      "Batch 240,  loss: 1.6981164455413817\n",
      "Batch 245,  loss: 1.6750001192092896\n",
      "Batch 250,  loss: 1.5186141967773437\n",
      "Batch 255,  loss: 1.6652631998062133\n",
      "Batch 260,  loss: 1.903110718727112\n",
      "Batch 265,  loss: 1.7282252311706543\n",
      "Batch 270,  loss: 1.6643177270889282\n",
      "Batch 275,  loss: 1.530258011817932\n",
      "Batch 280,  loss: 1.4116236925125123\n",
      "Batch 285,  loss: 1.5229894876480103\n",
      "Batch 290,  loss: 1.4308475255966187\n",
      "Batch 295,  loss: 1.3689565658569336\n",
      "Batch 300,  loss: 2.003265357017517\n",
      "Batch 305,  loss: 1.7521989822387696\n",
      "Batch 310,  loss: 1.3376693964004516\n",
      "Batch 315,  loss: 1.3505139589309691\n",
      "Batch 320,  loss: 2.0332414150238036\n",
      "Batch 325,  loss: 1.6296965599060058\n",
      "Batch 330,  loss: 1.5534121513366699\n",
      "Batch 335,  loss: 1.535337209701538\n",
      "Batch 340,  loss: 1.4255209565162659\n",
      "Batch 345,  loss: 1.2512755393981934\n",
      "Batch 350,  loss: 1.8694712162017821\n",
      "Batch 355,  loss: 1.7641766786575317\n",
      "Batch 360,  loss: 2.0408161878585815\n",
      "Batch 365,  loss: 1.5952076673507691\n",
      "Batch 370,  loss: 1.7046053647994994\n",
      "Batch 375,  loss: 1.461739444732666\n",
      "Batch 380,  loss: 1.6470278263092042\n",
      "Batch 385,  loss: 1.888951563835144\n",
      "Batch 390,  loss: 1.5587019681930543\n",
      "Batch 395,  loss: 1.8763861179351806\n",
      "Batch 400,  loss: 1.8463130950927735\n",
      "Batch 405,  loss: 1.5583500146865845\n",
      "Batch 410,  loss: 1.958374547958374\n",
      "Batch 415,  loss: 1.8207189083099364\n",
      "Batch 420,  loss: 1.6413869857788086\n",
      "Batch 425,  loss: 1.6140046954154967\n",
      "Batch 430,  loss: 2.159899950027466\n",
      "Batch 435,  loss: 1.6868269443511963\n",
      "Batch 440,  loss: 1.5133107662200929\n",
      "Batch 445,  loss: 1.0979537487030029\n",
      "Batch 450,  loss: 2.1518837213516235\n",
      "Batch 455,  loss: 1.6666972160339355\n",
      "Batch 460,  loss: 1.5141200065612792\n",
      "Batch 465,  loss: 1.316150188446045\n",
      "Batch 470,  loss: 1.5776406049728393\n",
      "Batch 475,  loss: 1.4166431784629823\n",
      "Batch 480,  loss: 1.4745876789093018\n",
      "Batch 485,  loss: 1.4766297578811645\n",
      "Batch 490,  loss: 1.4314773559570313\n",
      "Batch 495,  loss: 1.7674052953720092\n",
      "Batch 500,  loss: 2.1806342601776123\n",
      "Batch 505,  loss: 1.7190762639045716\n",
      "Batch 510,  loss: 1.767357087135315\n",
      "Batch 515,  loss: 1.4092301845550537\n",
      "Batch 520,  loss: 1.702808403968811\n",
      "Batch 525,  loss: 2.112087678909302\n",
      "Batch 530,  loss: 1.7632245779037476\n",
      "Batch 535,  loss: 1.6931781768798828\n",
      "Batch 540,  loss: 1.6280879974365234\n",
      "Batch 545,  loss: 1.6184763193130494\n",
      "Batch 550,  loss: 1.7698280811309814\n",
      "Batch 555,  loss: 1.7216677904129027\n",
      "Batch 560,  loss: 1.6812019348144531\n",
      "Batch 565,  loss: 1.4721665382385254\n",
      "Batch 570,  loss: 1.8464939594268799\n",
      "Batch 575,  loss: 1.9752678871154785\n",
      "Batch 580,  loss: 1.6337593078613282\n",
      "Batch 585,  loss: 1.890306329727173\n",
      "Batch 590,  loss: 1.4297892570495605\n",
      "Batch 595,  loss: 1.3213749647140502\n",
      "Batch 600,  loss: 1.4683711290359498\n",
      "Batch 605,  loss: 1.773217248916626\n",
      "Batch 610,  loss: 1.5399099826812743\n",
      "Batch 615,  loss: 1.9039933919906615\n",
      "Batch 620,  loss: 1.336552667617798\n",
      "Batch 625,  loss: 1.8852362394332887\n",
      "Batch 630,  loss: 1.306037425994873\n",
      "Batch 635,  loss: 1.8559000492095947\n",
      "Batch 640,  loss: 1.529317843914032\n",
      "Batch 645,  loss: 2.059227466583252\n",
      "Batch 650,  loss: 1.9997201919555665\n",
      "Batch 655,  loss: 1.734279251098633\n",
      "Batch 660,  loss: 1.3752260684967041\n",
      "Batch 665,  loss: 1.4514994621276855\n",
      "Batch 670,  loss: 1.612015700340271\n",
      "Batch 675,  loss: 1.8306572914123536\n",
      "Batch 680,  loss: 1.5176826000213623\n",
      "Batch 685,  loss: 1.6264572381973266\n",
      "Batch 690,  loss: 1.5512652397155762\n",
      "Batch 695,  loss: 1.5332477807998657\n",
      "Batch 700,  loss: 1.357541275024414\n",
      "Batch 705,  loss: 1.7086620092391969\n",
      "Batch 710,  loss: 1.8399821758270263\n",
      "Batch 715,  loss: 1.5293917179107666\n",
      "Batch 720,  loss: 1.8000969171524048\n",
      "Batch 725,  loss: 1.5514012575149536\n",
      "Batch 730,  loss: 1.841878843307495\n",
      "Batch 735,  loss: 1.765684223175049\n",
      "Batch 740,  loss: 1.5726816415786744\n",
      "Batch 745,  loss: 1.3476531744003295\n",
      "Batch 750,  loss: 1.6105778694152832\n",
      "Batch 755,  loss: 1.9183855056762695\n",
      "Batch 760,  loss: 1.363986039161682\n",
      "Batch 765,  loss: 1.6100648283958434\n",
      "Batch 770,  loss: 2.0547011137008666\n",
      "Batch 775,  loss: 1.6054452180862426\n",
      "Batch 780,  loss: 1.233188021183014\n",
      "Batch 785,  loss: 1.9181007862091064\n",
      "Batch 790,  loss: 1.718026041984558\n",
      "Batch 795,  loss: 1.6258126258850099\n",
      "Batch 800,  loss: 1.7024449825286865\n",
      "Batch 805,  loss: 1.7713947057724\n",
      "Batch 810,  loss: 1.7000544786453247\n",
      "Batch 815,  loss: 1.4380521774291992\n",
      "Batch 820,  loss: 1.4359845876693726\n",
      "Batch 825,  loss: 1.4984199047088622\n",
      "Batch 830,  loss: 1.4738479614257813\n",
      "Batch 835,  loss: 1.4686691641807557\n",
      "Batch 840,  loss: 1.3459903717041015\n",
      "Batch 845,  loss: 1.5601115226745605\n",
      "Batch 850,  loss: 1.8233725309371949\n",
      "Batch 855,  loss: 1.8954559326171876\n",
      "Batch 860,  loss: 2.0773468017578125\n",
      "Batch 865,  loss: 1.9096638679504394\n",
      "Batch 870,  loss: 1.8166468381881713\n",
      "Batch 875,  loss: 1.88991436958313\n",
      "Batch 880,  loss: 1.6358947992324828\n",
      "Batch 885,  loss: 1.5652085781097411\n",
      "Batch 890,  loss: 1.7635146379470825\n",
      "Batch 895,  loss: 1.2332912921905517\n",
      "Batch 900,  loss: 1.645685362815857\n",
      "Batch 905,  loss: 1.6276464939117432\n",
      "Batch 910,  loss: 1.860127830505371\n",
      "Batch 915,  loss: 1.4539820194244384\n",
      "Batch 920,  loss: 1.4551745176315307\n",
      "Batch 925,  loss: 1.443688416481018\n",
      "Batch 930,  loss: 2.037091851234436\n",
      "Batch 935,  loss: 1.607792329788208\n",
      "Batch 940,  loss: 1.8856947422027588\n",
      "Batch 945,  loss: 1.4964041471481324\n",
      "Batch 950,  loss: 1.8260807156562806\n",
      "Batch 955,  loss: 1.5640765190124513\n",
      "Batch 960,  loss: 1.3452674388885497\n",
      "Batch 965,  loss: 1.8674505710601808\n",
      "Batch 970,  loss: 1.9510510921478272\n",
      "Batch 975,  loss: 1.390565848350525\n",
      "Batch 980,  loss: 1.351639747619629\n",
      "Batch 985,  loss: 1.601387882232666\n",
      "Batch 990,  loss: 1.5229884386062622\n",
      "Batch 995,  loss: 1.6497358083724976\n",
      "Batch 1000,  loss: 1.615851926803589\n",
      "Batch 1005,  loss: 1.44245183467865\n",
      "Batch 1010,  loss: 1.5363057851791382\n",
      "Batch 1015,  loss: 1.4403454065322876\n",
      "Batch 1020,  loss: 1.545792245864868\n",
      "Batch 1025,  loss: 1.6266265869140626\n",
      "Batch 1030,  loss: 1.4338283777236938\n",
      "Batch 1035,  loss: 1.731468176841736\n",
      "Batch 1040,  loss: 1.856569528579712\n",
      "Batch 1045,  loss: 1.735024333000183\n",
      "Batch 1050,  loss: 1.8074912309646607\n",
      "Batch 1055,  loss: 1.7978448867797852\n",
      "Batch 1060,  loss: 1.238083267211914\n",
      "Batch 1065,  loss: 1.7324886560440063\n",
      "Batch 1070,  loss: 1.6556855201721192\n",
      "Batch 1075,  loss: 1.4117675304412842\n",
      "Batch 1080,  loss: 1.4798923492431642\n",
      "Batch 1085,  loss: 2.139460492134094\n",
      "Batch 1090,  loss: 1.6420063495635986\n",
      "Batch 1095,  loss: 1.6764014959335327\n",
      "Batch 1100,  loss: 1.841799831390381\n",
      "Batch 1105,  loss: 1.6623254299163819\n",
      "Batch 1110,  loss: 1.877950096130371\n",
      "Batch 1115,  loss: 1.6177393674850464\n",
      "Batch 1120,  loss: 2.1026435613632204\n",
      "Batch 1125,  loss: 1.7415239334106445\n",
      "Batch 1130,  loss: 1.5699923753738403\n",
      "Batch 1135,  loss: 1.8119616031646728\n",
      "Batch 1140,  loss: 1.621451997756958\n",
      "Batch 1145,  loss: 1.8139041662216187\n",
      "Batch 1150,  loss: 2.081769061088562\n",
      "Batch 1155,  loss: 1.8179487228393554\n",
      "Batch 1160,  loss: 1.9340975046157838\n",
      "Batch 1165,  loss: 1.1845150589942932\n",
      "Batch 1170,  loss: 1.941250991821289\n",
      "Batch 1175,  loss: 1.701616644859314\n",
      "Batch 1180,  loss: 1.5189836502075196\n",
      "Batch 1185,  loss: 1.4243208169937134\n",
      "Batch 1190,  loss: 1.5398410081863403\n",
      "Batch 1195,  loss: 1.429111361503601\n",
      "Batch 1200,  loss: 1.780202865600586\n",
      "Batch 1205,  loss: 1.6902087450027465\n",
      "Batch 1210,  loss: 1.6536609172821044\n",
      "Batch 1215,  loss: 1.4337990283966064\n",
      "Batch 1220,  loss: 1.9354331731796264\n",
      "Batch 1225,  loss: 1.637198567390442\n",
      "Batch 1230,  loss: 1.4538826942443848\n",
      "Batch 1235,  loss: 1.6021209955215454\n",
      "Batch 1240,  loss: 1.6000094652175902\n",
      "Batch 1245,  loss: 1.392011833190918\n",
      "Batch 1250,  loss: 1.8090181589126586\n",
      "Batch 1255,  loss: 1.6224772930145264\n",
      "Batch 1260,  loss: 1.676436221599579\n",
      "Batch 1265,  loss: 1.843859076499939\n",
      "Batch 1270,  loss: 1.4943244218826295\n",
      "Batch 1275,  loss: 2.0986427783966066\n",
      "Batch 1280,  loss: 1.7386557817459107\n",
      "Batch 1285,  loss: 1.3478899955749513\n",
      "Batch 1290,  loss: 1.8210822820663453\n",
      "Batch 1295,  loss: 1.971785068511963\n",
      "LOSS train 1.971785068511963. Validation loss: 2.194976917764655 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 28:\n",
      "Batch 5,  loss: 1.6357499599456786\n",
      "Batch 10,  loss: 1.7977080821990967\n",
      "Batch 15,  loss: 2.2659388065338133\n",
      "Batch 20,  loss: 1.544900608062744\n",
      "Batch 25,  loss: 1.561547541618347\n",
      "Batch 30,  loss: 1.6830676198005676\n",
      "Batch 35,  loss: 1.8883653163909913\n",
      "Batch 40,  loss: 1.7124634981155396\n",
      "Batch 45,  loss: 1.356614112854004\n",
      "Batch 50,  loss: 2.112844443321228\n",
      "Batch 55,  loss: 1.5347940444946289\n",
      "Batch 60,  loss: 2.2647250413894655\n",
      "Batch 65,  loss: 1.4994177579879762\n",
      "Batch 70,  loss: 1.4681833744049073\n",
      "Batch 75,  loss: 1.7848284006118775\n",
      "Batch 80,  loss: 1.7222997188568114\n",
      "Batch 85,  loss: 1.380468499660492\n",
      "Batch 90,  loss: 1.50697762966156\n",
      "Batch 95,  loss: 1.8360968589782716\n",
      "Batch 100,  loss: 1.725416612625122\n",
      "Batch 105,  loss: 1.8890058517456054\n",
      "Batch 110,  loss: 1.5500875473022462\n",
      "Batch 115,  loss: 1.9686846494674684\n",
      "Batch 120,  loss: 1.288835883140564\n",
      "Batch 125,  loss: 1.8737221002578734\n",
      "Batch 130,  loss: 1.6498085021972657\n",
      "Batch 135,  loss: 1.6628504991531372\n",
      "Batch 140,  loss: 1.5884669065475463\n",
      "Batch 145,  loss: 1.8311625003814698\n",
      "Batch 150,  loss: 1.5919707059860229\n",
      "Batch 155,  loss: 1.5980242252349854\n",
      "Batch 160,  loss: 1.48329439163208\n",
      "Batch 165,  loss: 1.7439034223556518\n",
      "Batch 170,  loss: 1.5615714550018311\n",
      "Batch 175,  loss: 1.8439043521881104\n",
      "Batch 180,  loss: 1.5868155717849732\n",
      "Batch 185,  loss: 1.6507002353668212\n",
      "Batch 190,  loss: 1.452399778366089\n",
      "Batch 195,  loss: 1.972577118873596\n",
      "Batch 200,  loss: 1.7896456956863402\n",
      "Batch 205,  loss: 1.8224043369293212\n",
      "Batch 210,  loss: 1.7590275764465333\n",
      "Batch 215,  loss: 1.7611061573028564\n",
      "Batch 220,  loss: 1.5005788803100586\n",
      "Batch 225,  loss: 1.6424328327178954\n",
      "Batch 230,  loss: 1.8068413615226746\n",
      "Batch 235,  loss: 1.9081687211990357\n",
      "Batch 240,  loss: 1.284748911857605\n",
      "Batch 245,  loss: 1.727609348297119\n",
      "Batch 250,  loss: 1.5807869911193848\n",
      "Batch 255,  loss: 1.7316993951797486\n",
      "Batch 260,  loss: 1.6120057821273803\n",
      "Batch 265,  loss: 1.808130145072937\n",
      "Batch 270,  loss: 1.5512991428375245\n",
      "Batch 275,  loss: 1.4458317637443543\n",
      "Batch 280,  loss: 1.5643772840499879\n",
      "Batch 285,  loss: 1.7804991006851196\n",
      "Batch 290,  loss: 1.6144415616989136\n",
      "Batch 295,  loss: 1.3727962493896484\n",
      "Batch 300,  loss: 1.5333155870437623\n",
      "Batch 305,  loss: 1.649433970451355\n",
      "Batch 310,  loss: 1.7804048776626586\n",
      "Batch 315,  loss: 1.424851894378662\n",
      "Batch 320,  loss: 1.474935269355774\n",
      "Batch 325,  loss: 1.633511233329773\n",
      "Batch 330,  loss: 1.9394605875015258\n",
      "Batch 335,  loss: 1.5685174942016602\n",
      "Batch 340,  loss: 1.7814189434051513\n",
      "Batch 345,  loss: 1.4960607051849366\n",
      "Batch 350,  loss: 1.2774479627609252\n",
      "Batch 355,  loss: 1.442397451400757\n",
      "Batch 360,  loss: 1.5848414897918701\n",
      "Batch 365,  loss: 1.4477547645568847\n",
      "Batch 370,  loss: 1.6242051601409913\n",
      "Batch 375,  loss: 1.638509702682495\n",
      "Batch 380,  loss: 1.84668128490448\n",
      "Batch 385,  loss: 2.141747736930847\n",
      "Batch 390,  loss: 1.3310359477996827\n",
      "Batch 395,  loss: 1.3619248151779175\n",
      "Batch 400,  loss: 1.5087378025054932\n",
      "Batch 405,  loss: 1.7015169620513917\n",
      "Batch 410,  loss: 1.732787585258484\n",
      "Batch 415,  loss: 1.5358871936798095\n",
      "Batch 420,  loss: 1.6782281160354615\n",
      "Batch 425,  loss: 1.2575894594192505\n",
      "Batch 430,  loss: 1.674042248725891\n",
      "Batch 435,  loss: 1.9570457696914674\n",
      "Batch 440,  loss: 1.5043049812316895\n",
      "Batch 445,  loss: 1.9483700513839721\n",
      "Batch 450,  loss: 1.6399580478668212\n",
      "Batch 455,  loss: 1.5733840227127076\n",
      "Batch 460,  loss: 1.9323480367660522\n",
      "Batch 465,  loss: 1.535129225254059\n",
      "Batch 470,  loss: 1.4210975408554076\n",
      "Batch 475,  loss: 1.128382635116577\n",
      "Batch 480,  loss: 1.4750511050224304\n",
      "Batch 485,  loss: 1.6878642559051513\n",
      "Batch 490,  loss: 1.831360673904419\n",
      "Batch 495,  loss: 1.4433111190795898\n",
      "Batch 500,  loss: 1.804042148590088\n",
      "Batch 505,  loss: 1.8551127433776855\n",
      "Batch 510,  loss: 1.7118326663970946\n",
      "Batch 515,  loss: 1.6267678260803222\n",
      "Batch 520,  loss: 2.0449031591415405\n",
      "Batch 525,  loss: 1.638578224182129\n",
      "Batch 530,  loss: 1.4924619436264037\n",
      "Batch 535,  loss: 1.5811079025268555\n",
      "Batch 540,  loss: 1.4612095594406127\n",
      "Batch 545,  loss: 1.6035582304000855\n",
      "Batch 550,  loss: 1.7150901794433593\n",
      "Batch 555,  loss: 2.0603497982025147\n",
      "Batch 560,  loss: 1.537449598312378\n",
      "Batch 565,  loss: 2.008294630050659\n",
      "Batch 570,  loss: 1.4726292610168457\n",
      "Batch 575,  loss: 1.5695322036743165\n",
      "Batch 580,  loss: 1.7860878109931946\n",
      "Batch 585,  loss: 1.5044039726257323\n",
      "Batch 590,  loss: 1.5458351135253907\n",
      "Batch 595,  loss: 1.370043969154358\n",
      "Batch 600,  loss: 1.8365721225738525\n",
      "Batch 605,  loss: 1.6709076166152954\n",
      "Batch 610,  loss: 1.3981364965438843\n",
      "Batch 615,  loss: 1.5542603492736817\n",
      "Batch 620,  loss: 1.4533663511276245\n",
      "Batch 625,  loss: 1.5832803010940553\n",
      "Batch 630,  loss: 2.060524249076843\n",
      "Batch 635,  loss: 1.8763188123703003\n",
      "Batch 640,  loss: 1.6080389976501466\n",
      "Batch 645,  loss: 1.482947540283203\n",
      "Batch 650,  loss: 1.5008769989013673\n",
      "Batch 655,  loss: 1.9263844251632691\n",
      "Batch 660,  loss: 1.7784580707550048\n",
      "Batch 665,  loss: 1.528484869003296\n",
      "Batch 670,  loss: 1.8314700841903686\n",
      "Batch 675,  loss: 1.5584251403808593\n",
      "Batch 680,  loss: 1.6786996126174927\n",
      "Batch 685,  loss: 1.3373336791992188\n",
      "Batch 690,  loss: 1.6653293132781983\n",
      "Batch 695,  loss: 1.6433440685272216\n",
      "Batch 700,  loss: 1.2232467889785767\n",
      "Batch 705,  loss: 1.6872915267944335\n",
      "Batch 710,  loss: 1.9082117080688477\n",
      "Batch 715,  loss: 1.9753251314163207\n",
      "Batch 720,  loss: 1.461535596847534\n",
      "Batch 725,  loss: 1.882828664779663\n",
      "Batch 730,  loss: 1.6752177953720093\n",
      "Batch 735,  loss: 1.6130906820297242\n",
      "Batch 740,  loss: 1.6723430633544922\n",
      "Batch 745,  loss: 1.5943110227584838\n",
      "Batch 750,  loss: 1.3336599826812745\n",
      "Batch 755,  loss: 1.1066285729408265\n",
      "Batch 760,  loss: 1.4652433514595031\n",
      "Batch 765,  loss: 1.5855517864227295\n",
      "Batch 770,  loss: 1.2677458763122558\n",
      "Batch 775,  loss: 1.3854889869689941\n",
      "Batch 780,  loss: 1.8998729467391968\n",
      "Batch 785,  loss: 1.6520787954330445\n",
      "Batch 790,  loss: 1.704274868965149\n",
      "Batch 795,  loss: 1.671001935005188\n",
      "Batch 800,  loss: 1.509337854385376\n",
      "Batch 805,  loss: 1.5034255504608154\n",
      "Batch 810,  loss: 1.4363893032073975\n",
      "Batch 815,  loss: 1.7704377174377441\n",
      "Batch 820,  loss: 1.710609245300293\n",
      "Batch 825,  loss: 1.4720341205596923\n",
      "Batch 830,  loss: 1.7978068470954895\n",
      "Batch 835,  loss: 1.6229510068893434\n",
      "Batch 840,  loss: 1.4719803094863892\n",
      "Batch 845,  loss: 1.6442399501800538\n",
      "Batch 850,  loss: 1.7402777194976806\n",
      "Batch 855,  loss: 1.3921791553497314\n",
      "Batch 860,  loss: 1.7766122102737427\n",
      "Batch 865,  loss: 1.8476820230484008\n",
      "Batch 870,  loss: 1.8525542259216308\n",
      "Batch 875,  loss: 1.8326569080352784\n",
      "Batch 880,  loss: 1.6155508518218995\n",
      "Batch 885,  loss: 1.6744673252105713\n",
      "Batch 890,  loss: 1.4002442598342895\n",
      "Batch 895,  loss: 1.5550177574157715\n",
      "Batch 900,  loss: 1.4538651823997497\n",
      "Batch 905,  loss: 1.6983301639556885\n",
      "Batch 910,  loss: 1.5635603427886964\n",
      "Batch 915,  loss: 1.4582367420196534\n",
      "Batch 920,  loss: 1.5290021657943726\n",
      "Batch 925,  loss: 1.3153688192367554\n",
      "Batch 930,  loss: 1.5916828870773316\n",
      "Batch 935,  loss: 1.3321157693862915\n",
      "Batch 940,  loss: 2.237220525741577\n",
      "Batch 945,  loss: 1.9602832078933716\n",
      "Batch 950,  loss: 1.880255103111267\n",
      "Batch 955,  loss: 1.950902795791626\n",
      "Batch 960,  loss: 1.5809836626052856\n",
      "Batch 965,  loss: 1.6502148509025574\n",
      "Batch 970,  loss: 1.6196969509124757\n",
      "Batch 975,  loss: 1.6102332592010498\n",
      "Batch 980,  loss: 1.763111662864685\n",
      "Batch 985,  loss: 1.399689245223999\n",
      "Batch 990,  loss: 1.9169230222702027\n",
      "Batch 995,  loss: 1.9317073822021484\n",
      "Batch 1000,  loss: 2.0325894117355348\n",
      "Batch 1005,  loss: 1.459529495239258\n",
      "Batch 1010,  loss: 1.699616050720215\n",
      "Batch 1015,  loss: 1.2888805150985718\n",
      "Batch 1020,  loss: 1.6383395671844483\n",
      "Batch 1025,  loss: 1.49156653881073\n",
      "Batch 1030,  loss: 1.5604281187057496\n",
      "Batch 1035,  loss: 1.6863841533660888\n",
      "Batch 1040,  loss: 1.5173097848892212\n",
      "Batch 1045,  loss: 1.5780131578445435\n",
      "Batch 1050,  loss: 1.6632033824920653\n",
      "Batch 1055,  loss: 1.673630380630493\n",
      "Batch 1060,  loss: 1.7081215023994445\n",
      "Batch 1065,  loss: 1.6697574615478517\n",
      "Batch 1070,  loss: 1.4463940382003784\n",
      "Batch 1075,  loss: 1.6680802583694458\n",
      "Batch 1080,  loss: 1.7853593826293945\n",
      "Batch 1085,  loss: 2.052181577682495\n",
      "Batch 1090,  loss: 1.7213951349258423\n",
      "Batch 1095,  loss: 1.393865942955017\n",
      "Batch 1100,  loss: 1.3348252534866334\n",
      "Batch 1105,  loss: 1.9258057594299316\n",
      "Batch 1110,  loss: 1.682967233657837\n",
      "Batch 1115,  loss: 1.6462947845458984\n",
      "Batch 1120,  loss: 1.6517136812210083\n",
      "Batch 1125,  loss: 1.6118751525878907\n",
      "Batch 1130,  loss: 1.6552551507949829\n",
      "Batch 1135,  loss: 1.3735837459564209\n",
      "Batch 1140,  loss: 1.517898941040039\n",
      "Batch 1145,  loss: 1.816535234451294\n",
      "Batch 1150,  loss: 1.5794612884521484\n",
      "Batch 1155,  loss: 1.7110599756240845\n",
      "Batch 1160,  loss: 1.8879544496536256\n",
      "Batch 1165,  loss: 1.4850858449935913\n",
      "Batch 1170,  loss: 1.6154802799224854\n",
      "Batch 1175,  loss: 2.207529306411743\n",
      "Batch 1180,  loss: 1.5510150909423828\n",
      "Batch 1185,  loss: 1.6204569578170775\n",
      "Batch 1190,  loss: 1.5037270069122315\n",
      "Batch 1195,  loss: 1.617648720741272\n",
      "Batch 1200,  loss: 1.7943703293800355\n",
      "Batch 1205,  loss: 1.6131218910217284\n",
      "Batch 1210,  loss: 1.660844087600708\n",
      "Batch 1215,  loss: 1.8398146867752074\n",
      "Batch 1220,  loss: 1.7531735301017761\n",
      "Batch 1225,  loss: 1.5629284381866455\n",
      "Batch 1230,  loss: 1.638834571838379\n",
      "Batch 1235,  loss: 1.5379427671432495\n",
      "Batch 1240,  loss: 1.5121088027954102\n",
      "Batch 1245,  loss: 1.5638230323791504\n",
      "Batch 1250,  loss: 1.7398314714431762\n",
      "Batch 1255,  loss: 1.7057344436645507\n",
      "Batch 1260,  loss: 1.4681279420852662\n",
      "Batch 1265,  loss: 1.7781360626220704\n",
      "Batch 1270,  loss: 2.189767622947693\n",
      "Batch 1275,  loss: 1.3392158150672913\n",
      "Batch 1280,  loss: 1.548866868019104\n",
      "Batch 1285,  loss: 1.6666820287704467\n",
      "Batch 1290,  loss: 1.8348678827285767\n",
      "Batch 1295,  loss: 1.5187489986419678\n",
      "LOSS train 1.5187489986419678. Validation loss: 2.184222220932996 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 29:\n",
      "Batch 5,  loss: 1.5952933311462403\n",
      "Batch 10,  loss: 1.9035033941268922\n",
      "Batch 15,  loss: 1.7524080514907836\n",
      "Batch 20,  loss: 1.9048474788665772\n",
      "Batch 25,  loss: 1.4989750146865846\n",
      "Batch 30,  loss: 1.7569372653961182\n",
      "Batch 35,  loss: 1.4119540452957153\n",
      "Batch 40,  loss: 1.6370280742645265\n",
      "Batch 45,  loss: 1.7291051626205445\n",
      "Batch 50,  loss: 1.5199425220489502\n",
      "Batch 55,  loss: 2.4984564542770387\n",
      "Batch 60,  loss: 1.638025951385498\n",
      "Batch 65,  loss: 1.8175543546676636\n",
      "Batch 70,  loss: 1.4533214688301086\n",
      "Batch 75,  loss: 1.1263448596000671\n",
      "Batch 80,  loss: 1.765443205833435\n",
      "Batch 85,  loss: 1.5246609449386597\n",
      "Batch 90,  loss: 1.4580622911453247\n",
      "Batch 95,  loss: 1.829885721206665\n",
      "Batch 100,  loss: 1.7899382352828979\n",
      "Batch 105,  loss: 1.5906880617141723\n",
      "Batch 110,  loss: 1.9893365621566772\n",
      "Batch 115,  loss: 1.68964626789093\n",
      "Batch 120,  loss: 2.0958401441574095\n",
      "Batch 125,  loss: 1.4134363174438476\n",
      "Batch 130,  loss: 1.4873281955718993\n",
      "Batch 135,  loss: 1.4708789587020874\n",
      "Batch 140,  loss: 1.668652892112732\n",
      "Batch 145,  loss: 1.3132185816764832\n",
      "Batch 150,  loss: 1.678132677078247\n",
      "Batch 155,  loss: 1.5477680444717408\n",
      "Batch 160,  loss: 1.7812064647674561\n",
      "Batch 165,  loss: 1.6725420236587525\n",
      "Batch 170,  loss: 1.3930555820465087\n",
      "Batch 175,  loss: 1.80465886592865\n",
      "Batch 180,  loss: 1.3765300273895265\n",
      "Batch 185,  loss: 1.842491579055786\n",
      "Batch 190,  loss: 1.4134419441223145\n",
      "Batch 195,  loss: 1.574973726272583\n",
      "Batch 200,  loss: 1.836812424659729\n",
      "Batch 205,  loss: 1.5760292768478394\n",
      "Batch 210,  loss: 1.5183638334274292\n",
      "Batch 215,  loss: 1.452047634124756\n",
      "Batch 220,  loss: 1.5194400072097778\n",
      "Batch 225,  loss: 1.4833446264266967\n",
      "Batch 230,  loss: 1.5455795049667358\n",
      "Batch 235,  loss: 1.5885167598724366\n",
      "Batch 240,  loss: 1.786375141143799\n",
      "Batch 245,  loss: 1.5708820343017578\n",
      "Batch 250,  loss: 1.5357635378837586\n",
      "Batch 255,  loss: 1.9631296873092652\n",
      "Batch 260,  loss: 1.7487232685089111\n",
      "Batch 265,  loss: 1.5477458953857421\n",
      "Batch 270,  loss: 1.5319822549819946\n",
      "Batch 275,  loss: 1.8087642192840576\n",
      "Batch 280,  loss: 1.5011447191238403\n",
      "Batch 285,  loss: 1.8772917985916138\n",
      "Batch 290,  loss: 1.8700245857238769\n",
      "Batch 295,  loss: 1.3082422971725465\n",
      "Batch 300,  loss: 1.8114318370819091\n",
      "Batch 305,  loss: 1.5468955039978027\n",
      "Batch 310,  loss: 1.5637780666351317\n",
      "Batch 315,  loss: 1.5986430764198303\n",
      "Batch 320,  loss: 1.3816246509552002\n",
      "Batch 325,  loss: 1.7048085451126098\n",
      "Batch 330,  loss: 1.87432963848114\n",
      "Batch 335,  loss: 1.6748695850372315\n",
      "Batch 340,  loss: 2.0337894439697264\n",
      "Batch 345,  loss: 1.3388042092323302\n",
      "Batch 350,  loss: 1.600399112701416\n",
      "Batch 355,  loss: 1.6776012182235718\n",
      "Batch 360,  loss: 1.6398324251174927\n",
      "Batch 365,  loss: 2.1544308423995973\n",
      "Batch 370,  loss: 1.6908584594726563\n",
      "Batch 375,  loss: 1.493014407157898\n",
      "Batch 380,  loss: 1.5000563383102417\n",
      "Batch 385,  loss: 1.477697205543518\n",
      "Batch 390,  loss: 2.050018572807312\n",
      "Batch 395,  loss: 1.4536345958709718\n",
      "Batch 400,  loss: 1.3755308389663696\n",
      "Batch 405,  loss: 1.528938388824463\n",
      "Batch 410,  loss: 1.66904296875\n",
      "Batch 415,  loss: 1.4340996265411377\n",
      "Batch 420,  loss: 1.3598931312561036\n",
      "Batch 425,  loss: 1.62109477519989\n",
      "Batch 430,  loss: 1.7571518182754517\n",
      "Batch 435,  loss: 1.6937793493270874\n",
      "Batch 440,  loss: 1.637203085422516\n",
      "Batch 445,  loss: 1.354565191268921\n",
      "Batch 450,  loss: 1.550549077987671\n",
      "Batch 455,  loss: 1.8171608209609986\n",
      "Batch 460,  loss: 1.2981364250183105\n",
      "Batch 465,  loss: 1.4648378610610961\n",
      "Batch 470,  loss: 1.3676722049713135\n",
      "Batch 475,  loss: 1.5351499319076538\n",
      "Batch 480,  loss: 1.535520887374878\n",
      "Batch 485,  loss: 1.6758215188980103\n",
      "Batch 490,  loss: 1.4895084857940675\n",
      "Batch 495,  loss: 1.7087540864944457\n",
      "Batch 500,  loss: 1.4390691995620728\n",
      "Batch 505,  loss: 1.9295727252960204\n",
      "Batch 510,  loss: 1.8828960418701173\n",
      "Batch 515,  loss: 1.737852644920349\n",
      "Batch 520,  loss: 1.6368643283843993\n",
      "Batch 525,  loss: 1.5958850145339967\n",
      "Batch 530,  loss: 1.5662847995758056\n",
      "Batch 535,  loss: 1.8476887226104737\n",
      "Batch 540,  loss: 1.7670621395111084\n",
      "Batch 545,  loss: 1.3398665189743042\n",
      "Batch 550,  loss: 1.466860795021057\n",
      "Batch 555,  loss: 1.469249951839447\n",
      "Batch 560,  loss: 1.4754500150680543\n",
      "Batch 565,  loss: 1.6922220826148986\n",
      "Batch 570,  loss: 1.556770133972168\n",
      "Batch 575,  loss: 1.6915152311325072\n",
      "Batch 580,  loss: 1.475834035873413\n",
      "Batch 585,  loss: 1.6295107364654542\n",
      "Batch 590,  loss: 1.4433253049850463\n",
      "Batch 595,  loss: 1.3970104932785035\n",
      "Batch 600,  loss: 1.3464159965515137\n",
      "Batch 605,  loss: 1.9091119050979615\n",
      "Batch 610,  loss: 1.5426234960556031\n",
      "Batch 615,  loss: 1.4420841217041016\n",
      "Batch 620,  loss: 2.002569890022278\n",
      "Batch 625,  loss: 1.7764204263687133\n",
      "Batch 630,  loss: 1.7648434162139892\n",
      "Batch 635,  loss: 1.711456847190857\n",
      "Batch 640,  loss: 1.535157859325409\n",
      "Batch 645,  loss: 1.7448251962661743\n",
      "Batch 650,  loss: 1.6839534997940064\n",
      "Batch 655,  loss: 1.4136804342269897\n",
      "Batch 660,  loss: 1.7087437629699707\n",
      "Batch 665,  loss: 1.9931676626205443\n",
      "Batch 670,  loss: 1.567380666732788\n",
      "Batch 675,  loss: 1.6608587980270386\n",
      "Batch 680,  loss: 1.454521107673645\n",
      "Batch 685,  loss: 1.9957517385482788\n",
      "Batch 690,  loss: 1.7580871343612672\n",
      "Batch 695,  loss: 1.519802951812744\n",
      "Batch 700,  loss: 1.7918078899383545\n",
      "Batch 705,  loss: 1.7200998067855835\n",
      "Batch 710,  loss: 1.6195940732955934\n",
      "Batch 715,  loss: 1.6726593971252441\n",
      "Batch 720,  loss: 2.0168549537658693\n",
      "Batch 725,  loss: 1.4910752296447753\n",
      "Batch 730,  loss: 1.8156063199043273\n",
      "Batch 735,  loss: 1.400707983970642\n",
      "Batch 740,  loss: 1.3478435754776001\n",
      "Batch 745,  loss: 1.430545449256897\n",
      "Batch 750,  loss: 1.662204623222351\n",
      "Batch 755,  loss: 1.4936392545700072\n",
      "Batch 760,  loss: 1.4542745113372804\n",
      "Batch 765,  loss: 1.874532127380371\n",
      "Batch 770,  loss: 1.6305106163024903\n",
      "Batch 775,  loss: 1.842984938621521\n",
      "Batch 780,  loss: 1.836245584487915\n",
      "Batch 785,  loss: 1.5348745822906493\n",
      "Batch 790,  loss: 1.722838544845581\n",
      "Batch 795,  loss: 1.7493463516235352\n",
      "Batch 800,  loss: 1.5377270460128785\n",
      "Batch 805,  loss: 1.5912835359573365\n",
      "Batch 810,  loss: 1.4184568643569946\n",
      "Batch 815,  loss: 1.6865251064300537\n",
      "Batch 820,  loss: 1.7690049886703492\n",
      "Batch 825,  loss: 1.5291271209716797\n",
      "Batch 830,  loss: 1.75668306350708\n",
      "Batch 835,  loss: 1.4262351751327516\n",
      "Batch 840,  loss: 1.7727797269821166\n",
      "Batch 845,  loss: 1.705090832710266\n",
      "Batch 850,  loss: 1.0848013281822204\n",
      "Batch 855,  loss: 1.4965903282165527\n",
      "Batch 860,  loss: 1.5990887403488159\n",
      "Batch 865,  loss: 1.4396904468536378\n",
      "Batch 870,  loss: 1.3171685457229614\n",
      "Batch 875,  loss: 1.5835670709609986\n",
      "Batch 880,  loss: 1.2871119737625123\n",
      "Batch 885,  loss: 2.26550133228302\n",
      "Batch 890,  loss: 1.6170625925064086\n",
      "Batch 895,  loss: 1.6006059885025024\n",
      "Batch 900,  loss: 1.5689353227615357\n",
      "Batch 905,  loss: 1.4651129245758057\n",
      "Batch 910,  loss: 0.9641579031944275\n",
      "Batch 915,  loss: 1.6959060192108155\n",
      "Batch 920,  loss: 1.321765959262848\n",
      "Batch 925,  loss: 1.3766845345497132\n",
      "Batch 930,  loss: 1.317847990989685\n",
      "Batch 935,  loss: 1.843452763557434\n",
      "Batch 940,  loss: 1.3515679836273193\n",
      "Batch 945,  loss: 1.6228936433792114\n",
      "Batch 950,  loss: 1.6477227210998535\n",
      "Batch 955,  loss: 1.5400147199630738\n",
      "Batch 960,  loss: 1.6317649602890014\n",
      "Batch 965,  loss: 1.5947976589202881\n",
      "Batch 970,  loss: 1.692791473865509\n",
      "Batch 975,  loss: 1.5522001028060912\n",
      "Batch 980,  loss: 1.7042001962661744\n",
      "Batch 985,  loss: 1.9431713104248047\n",
      "Batch 990,  loss: 1.464779257774353\n",
      "Batch 995,  loss: 1.4900967836380006\n",
      "Batch 1000,  loss: 1.2237503290176392\n",
      "Batch 1005,  loss: 1.467921757698059\n",
      "Batch 1010,  loss: 1.816284418106079\n",
      "Batch 1015,  loss: 1.7493314027786255\n",
      "Batch 1020,  loss: 1.6499411106109618\n",
      "Batch 1025,  loss: 1.4078393936157227\n",
      "Batch 1030,  loss: 1.7952316045761108\n",
      "Batch 1035,  loss: 2.0994412183761595\n",
      "Batch 1040,  loss: 1.3462002754211426\n",
      "Batch 1045,  loss: 1.3883407354354858\n",
      "Batch 1050,  loss: 1.502496314048767\n",
      "Batch 1055,  loss: 1.7192115306854248\n",
      "Batch 1060,  loss: 2.042038655281067\n",
      "Batch 1065,  loss: 1.4806178569793702\n",
      "Batch 1070,  loss: 1.4063323497772218\n",
      "Batch 1075,  loss: 1.7309423208236694\n",
      "Batch 1080,  loss: 1.577002191543579\n",
      "Batch 1085,  loss: 1.6129928588867188\n",
      "Batch 1090,  loss: 1.6297125816345215\n",
      "Batch 1095,  loss: 1.5932446241378784\n",
      "Batch 1100,  loss: 1.6892242670059203\n",
      "Batch 1105,  loss: 1.694625687599182\n",
      "Batch 1110,  loss: 1.6720747709274293\n",
      "Batch 1115,  loss: 1.6750036954879761\n",
      "Batch 1120,  loss: 1.8028043746948241\n",
      "Batch 1125,  loss: 1.3788610577583313\n",
      "Batch 1130,  loss: 1.2903597116470338\n",
      "Batch 1135,  loss: 1.5012107610702514\n",
      "Batch 1140,  loss: 1.5507811069488526\n",
      "Batch 1145,  loss: 1.325093388557434\n",
      "Batch 1150,  loss: 1.383937621116638\n",
      "Batch 1155,  loss: 1.398639690876007\n",
      "Batch 1160,  loss: 2.109996032714844\n",
      "Batch 1165,  loss: 1.7391865491867065\n",
      "Batch 1170,  loss: 1.6441944360733032\n",
      "Batch 1175,  loss: 1.4683881402015686\n",
      "Batch 1180,  loss: 1.8957969427108765\n",
      "Batch 1185,  loss: 1.5160209894180299\n",
      "Batch 1190,  loss: 1.273418927192688\n",
      "Batch 1195,  loss: 1.487564706802368\n",
      "Batch 1200,  loss: 1.4822681427001954\n",
      "Batch 1205,  loss: 1.6369965553283692\n",
      "Batch 1210,  loss: 1.7833691120147706\n",
      "Batch 1215,  loss: 1.5701844692230225\n",
      "Batch 1220,  loss: 1.3628740072250367\n",
      "Batch 1225,  loss: 1.878131055831909\n",
      "Batch 1230,  loss: 1.6816214799880982\n",
      "Batch 1235,  loss: 1.7321766376495362\n",
      "Batch 1240,  loss: 1.715230369567871\n",
      "Batch 1245,  loss: 1.9468862533569335\n",
      "Batch 1250,  loss: 1.7784993171691894\n",
      "Batch 1255,  loss: 1.5477357506752014\n",
      "Batch 1260,  loss: 1.5391155242919923\n",
      "Batch 1265,  loss: 1.4876346111297607\n",
      "Batch 1270,  loss: 1.6469854831695556\n",
      "Batch 1275,  loss: 1.6109297752380372\n",
      "Batch 1280,  loss: 1.490359377861023\n",
      "Batch 1285,  loss: 1.461917018890381\n",
      "Batch 1290,  loss: 1.6638841152191162\n",
      "Batch 1295,  loss: 1.8909497022628785\n",
      "LOSS train 1.8909497022628785. Validation loss: 2.0246768533907553 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 30:\n",
      "Batch 5,  loss: 1.5843552112579347\n",
      "Batch 10,  loss: 1.7655253291130066\n",
      "Batch 15,  loss: 1.6581489801406861\n",
      "Batch 20,  loss: 1.168379044532776\n",
      "Batch 25,  loss: 1.758820605278015\n",
      "Batch 30,  loss: 1.5954375982284545\n",
      "Batch 35,  loss: 1.5741646051406861\n",
      "Batch 40,  loss: 1.5701164484024048\n",
      "Batch 45,  loss: 1.5369025945663453\n",
      "Batch 50,  loss: 1.605573844909668\n",
      "Batch 55,  loss: 1.3365163326263427\n",
      "Batch 60,  loss: 1.6690715312957765\n",
      "Batch 65,  loss: 1.6751837253570556\n",
      "Batch 70,  loss: 1.367182683944702\n",
      "Batch 75,  loss: 1.574086332321167\n",
      "Batch 80,  loss: 1.4650265216827392\n",
      "Batch 85,  loss: 2.008492350578308\n",
      "Batch 90,  loss: 1.7610442638397217\n",
      "Batch 95,  loss: 1.7426913261413575\n",
      "Batch 100,  loss: 2.0419360399246216\n",
      "Batch 105,  loss: 1.4116816997528077\n",
      "Batch 110,  loss: 1.459801697731018\n",
      "Batch 115,  loss: 1.4503340244293212\n",
      "Batch 120,  loss: 1.412325882911682\n",
      "Batch 125,  loss: 1.7810071468353272\n",
      "Batch 130,  loss: 1.7045827627182006\n",
      "Batch 135,  loss: 1.6799614429473877\n",
      "Batch 140,  loss: 2.0352194786071776\n",
      "Batch 145,  loss: 1.5346396684646606\n",
      "Batch 150,  loss: 1.706864094734192\n",
      "Batch 155,  loss: 1.498997735977173\n",
      "Batch 160,  loss: 1.5301481008529663\n",
      "Batch 165,  loss: 1.8787801742553711\n",
      "Batch 170,  loss: 1.4383044719696045\n",
      "Batch 175,  loss: 1.4446300983428955\n",
      "Batch 180,  loss: 1.525016474723816\n",
      "Batch 185,  loss: 1.7139739274978638\n",
      "Batch 190,  loss: 1.432088267803192\n",
      "Batch 195,  loss: 1.541496181488037\n",
      "Batch 200,  loss: 1.7993098974227906\n",
      "Batch 205,  loss: 1.5878708362579346\n",
      "Batch 210,  loss: 1.7951236009597777\n",
      "Batch 215,  loss: 1.3422109365463257\n",
      "Batch 220,  loss: 1.5155381441116333\n",
      "Batch 225,  loss: 1.8641998767852783\n",
      "Batch 230,  loss: 1.6051243543624878\n",
      "Batch 235,  loss: 1.8178058862686157\n",
      "Batch 240,  loss: 1.4637195587158203\n",
      "Batch 245,  loss: 1.2792484521865846\n",
      "Batch 250,  loss: 1.545684313774109\n",
      "Batch 255,  loss: 1.7024053812026978\n",
      "Batch 260,  loss: 1.839623475074768\n",
      "Batch 265,  loss: 1.521592664718628\n",
      "Batch 270,  loss: 1.3785076856613159\n",
      "Batch 275,  loss: 1.5819295167922973\n",
      "Batch 280,  loss: 1.6219342470169067\n",
      "Batch 285,  loss: 1.697106623649597\n",
      "Batch 290,  loss: 1.8339454650878906\n",
      "Batch 295,  loss: 1.775610089302063\n",
      "Batch 300,  loss: 1.430545949935913\n",
      "Batch 305,  loss: 1.8435552597045899\n",
      "Batch 310,  loss: 1.631260633468628\n",
      "Batch 315,  loss: 1.500920057296753\n",
      "Batch 320,  loss: 1.3592765092849732\n",
      "Batch 325,  loss: 1.5469450950622559\n",
      "Batch 330,  loss: 1.5726078033447266\n",
      "Batch 335,  loss: 1.604528021812439\n",
      "Batch 340,  loss: 1.47755024433136\n",
      "Batch 345,  loss: 1.5810562133789063\n",
      "Batch 350,  loss: 1.5995679378509522\n",
      "Batch 355,  loss: 1.6398555755615234\n",
      "Batch 360,  loss: 1.6436788320541382\n",
      "Batch 365,  loss: 1.6231665372848512\n",
      "Batch 370,  loss: 1.3254843711853028\n",
      "Batch 375,  loss: 1.7458143949508667\n",
      "Batch 380,  loss: 1.847656011581421\n",
      "Batch 385,  loss: 1.7955753326416015\n",
      "Batch 390,  loss: 1.6238503575325012\n",
      "Batch 395,  loss: 1.459447455406189\n",
      "Batch 400,  loss: 1.5466727256774901\n",
      "Batch 405,  loss: 1.6427709817886353\n",
      "Batch 410,  loss: 1.6121747732162475\n",
      "Batch 415,  loss: 1.6906574487686157\n",
      "Batch 420,  loss: 1.526939582824707\n",
      "Batch 425,  loss: 1.815373992919922\n",
      "Batch 430,  loss: 1.22099826335907\n",
      "Batch 435,  loss: 1.7899549007415771\n",
      "Batch 440,  loss: 1.4980116605758667\n",
      "Batch 445,  loss: 1.588486623764038\n",
      "Batch 450,  loss: 1.7968958854675292\n",
      "Batch 455,  loss: 1.271826171875\n",
      "Batch 460,  loss: 1.5720463514328002\n",
      "Batch 465,  loss: 1.5577621459960938\n",
      "Batch 470,  loss: 1.6098217964172363\n",
      "Batch 475,  loss: 1.3749593496322632\n",
      "Batch 480,  loss: 1.9592968940734863\n",
      "Batch 485,  loss: 1.469945454597473\n",
      "Batch 490,  loss: 1.6922977209091186\n",
      "Batch 495,  loss: 1.6536187648773193\n",
      "Batch 500,  loss: 1.6366456985473632\n",
      "Batch 505,  loss: 1.3874850034713746\n",
      "Batch 510,  loss: 1.3241253614425659\n",
      "Batch 515,  loss: 1.406255805492401\n",
      "Batch 520,  loss: 1.3271275281906127\n",
      "Batch 525,  loss: 1.844945454597473\n",
      "Batch 530,  loss: 1.7747652769088744\n",
      "Batch 535,  loss: 1.7326523542404175\n",
      "Batch 540,  loss: 1.536890721321106\n",
      "Batch 545,  loss: 1.665515398979187\n",
      "Batch 550,  loss: 1.8243579387664794\n",
      "Batch 555,  loss: 1.7581818103790283\n",
      "Batch 560,  loss: 1.7757992267608642\n",
      "Batch 565,  loss: 1.4075679779052734\n",
      "Batch 570,  loss: 1.239058828353882\n",
      "Batch 575,  loss: 1.6020843267440796\n",
      "Batch 580,  loss: 1.6122647762298583\n",
      "Batch 585,  loss: 1.4519938945770263\n",
      "Batch 590,  loss: 1.6652570962905884\n",
      "Batch 595,  loss: 1.8704670906066894\n",
      "Batch 600,  loss: 1.5750413537025452\n",
      "Batch 605,  loss: 1.553546953201294\n",
      "Batch 610,  loss: 1.3972283840179442\n",
      "Batch 615,  loss: 1.704595947265625\n",
      "Batch 620,  loss: 1.3300567865371704\n",
      "Batch 625,  loss: 2.1872589111328127\n",
      "Batch 630,  loss: 1.3207061767578125\n",
      "Batch 635,  loss: 1.4519256830215455\n",
      "Batch 640,  loss: 1.4032356023788453\n",
      "Batch 645,  loss: 1.525256633758545\n",
      "Batch 650,  loss: 1.2835202932357788\n",
      "Batch 655,  loss: 1.540793251991272\n",
      "Batch 660,  loss: 1.5076443433761597\n",
      "Batch 665,  loss: 1.7215482473373414\n",
      "Batch 670,  loss: 1.465051293373108\n",
      "Batch 675,  loss: 1.6358444690704346\n",
      "Batch 680,  loss: 1.9593056321144104\n",
      "Batch 685,  loss: 1.3329458951950073\n",
      "Batch 690,  loss: 1.5147168159484863\n",
      "Batch 695,  loss: 1.364145040512085\n",
      "Batch 700,  loss: 1.44134464263916\n",
      "Batch 705,  loss: 1.6811709642410277\n",
      "Batch 710,  loss: 1.6098825216293335\n",
      "Batch 715,  loss: 1.3478312134742736\n",
      "Batch 720,  loss: 1.6859961986541747\n",
      "Batch 725,  loss: 1.7660351991653442\n",
      "Batch 730,  loss: 1.6195173978805542\n",
      "Batch 735,  loss: 1.5030665397644043\n",
      "Batch 740,  loss: 1.8292509078979493\n",
      "Batch 745,  loss: 1.854360008239746\n",
      "Batch 750,  loss: 1.2702402591705322\n",
      "Batch 755,  loss: 1.8979236125946044\n",
      "Batch 760,  loss: 1.8616315841674804\n",
      "Batch 765,  loss: 1.6212092399597169\n",
      "Batch 770,  loss: 1.8506798982620238\n",
      "Batch 775,  loss: 1.8763662815093993\n",
      "Batch 780,  loss: 1.5371634006500243\n",
      "Batch 785,  loss: 1.6154154062271118\n",
      "Batch 790,  loss: 1.6268247842788697\n",
      "Batch 795,  loss: 1.6536412954330444\n",
      "Batch 800,  loss: 1.7117584943771362\n",
      "Batch 805,  loss: 2.1905662059783935\n",
      "Batch 810,  loss: 1.7400757789611816\n",
      "Batch 815,  loss: 1.7191742420196534\n",
      "Batch 820,  loss: 1.4363801717758178\n",
      "Batch 825,  loss: 1.6470623254776\n",
      "Batch 830,  loss: 1.3289551734924316\n",
      "Batch 835,  loss: 1.5373621702194213\n",
      "Batch 840,  loss: 1.6671342134475708\n",
      "Batch 845,  loss: 1.6556118249893188\n",
      "Batch 850,  loss: 1.4547619581222535\n",
      "Batch 855,  loss: 1.646623182296753\n",
      "Batch 860,  loss: 1.1935420274734496\n",
      "Batch 865,  loss: 1.5778645038604737\n",
      "Batch 870,  loss: 1.3307831048965455\n",
      "Batch 875,  loss: 1.6592277050018311\n",
      "Batch 880,  loss: 1.376800775527954\n",
      "Batch 885,  loss: 1.656373882293701\n",
      "Batch 890,  loss: 1.6249616146087646\n",
      "Batch 895,  loss: 1.7039024829864502\n",
      "Batch 900,  loss: 1.723321557044983\n",
      "Batch 905,  loss: 1.302747654914856\n",
      "Batch 910,  loss: 1.5009538292884828\n",
      "Batch 915,  loss: 1.603769874572754\n",
      "Batch 920,  loss: 1.7461912393569947\n",
      "Batch 925,  loss: 1.7887033224105835\n",
      "Batch 930,  loss: 1.69199059009552\n",
      "Batch 935,  loss: 1.4979448318481445\n",
      "Batch 940,  loss: 1.6580276489257812\n",
      "Batch 945,  loss: 1.7094246387481689\n",
      "Batch 950,  loss: 1.7870548248291016\n",
      "Batch 955,  loss: 1.2541501879692079\n",
      "Batch 960,  loss: 1.1216218709945678\n",
      "Batch 965,  loss: 1.8121947526931763\n",
      "Batch 970,  loss: 1.688344693183899\n",
      "Batch 975,  loss: 1.3825955390930176\n",
      "Batch 980,  loss: 1.5877471208572387\n",
      "Batch 985,  loss: 1.3983524084091186\n",
      "Batch 990,  loss: 1.5047034978866578\n",
      "Batch 995,  loss: 1.3877504110336303\n",
      "Batch 1000,  loss: 1.6069608211517334\n",
      "Batch 1005,  loss: 1.6071085214614869\n",
      "Batch 1010,  loss: 2.051890468597412\n",
      "Batch 1015,  loss: 2.1627087116241457\n",
      "Batch 1020,  loss: 1.639110279083252\n",
      "Batch 1025,  loss: 1.388152539730072\n",
      "Batch 1030,  loss: 1.4408878087997437\n",
      "Batch 1035,  loss: 1.421853256225586\n",
      "Batch 1040,  loss: 1.210507833957672\n",
      "Batch 1045,  loss: 1.3189669013023377\n",
      "Batch 1050,  loss: 1.9571341514587401\n",
      "Batch 1055,  loss: 1.3427947521209718\n",
      "Batch 1060,  loss: 1.418828511238098\n",
      "Batch 1065,  loss: 1.6750338554382325\n",
      "Batch 1070,  loss: 1.7219346761703491\n",
      "Batch 1075,  loss: 2.175536799430847\n",
      "Batch 1080,  loss: 1.646549940109253\n",
      "Batch 1085,  loss: 1.599328351020813\n",
      "Batch 1090,  loss: 1.6609816789627074\n",
      "Batch 1095,  loss: 1.846218204498291\n",
      "Batch 1100,  loss: 1.661838674545288\n",
      "Batch 1105,  loss: 1.706659746170044\n",
      "Batch 1110,  loss: 1.7954490423202514\n",
      "Batch 1115,  loss: 1.6151908397674561\n",
      "Batch 1120,  loss: 1.2430177688598634\n",
      "Batch 1125,  loss: 1.5974260091781616\n",
      "Batch 1130,  loss: 1.7665034890174867\n",
      "Batch 1135,  loss: 1.7834200143814087\n",
      "Batch 1140,  loss: 1.9422010421752929\n",
      "Batch 1145,  loss: 1.5618493795394897\n",
      "Batch 1150,  loss: 1.7874860525131226\n",
      "Batch 1155,  loss: 1.4515718460083007\n",
      "Batch 1160,  loss: 1.4344754219055176\n",
      "Batch 1165,  loss: 1.6454841375350953\n",
      "Batch 1170,  loss: 2.0636216402053833\n",
      "Batch 1175,  loss: 1.6572614908218384\n",
      "Batch 1180,  loss: 1.4442018508911132\n",
      "Batch 1185,  loss: 1.6057348489761352\n",
      "Batch 1190,  loss: 1.3922985315322876\n",
      "Batch 1195,  loss: 1.333192563056946\n",
      "Batch 1200,  loss: 1.8320961952209474\n",
      "Batch 1205,  loss: 1.626997137069702\n",
      "Batch 1210,  loss: 2.0316980838775636\n",
      "Batch 1215,  loss: 1.6859782457351684\n",
      "Batch 1220,  loss: 1.4487308025360108\n",
      "Batch 1225,  loss: 1.5652488946914673\n",
      "Batch 1230,  loss: 1.3613504648208619\n",
      "Batch 1235,  loss: 1.7051360726356506\n",
      "Batch 1240,  loss: 1.9528469562530517\n",
      "Batch 1245,  loss: 1.4217208623886108\n",
      "Batch 1250,  loss: 1.4923435926437378\n",
      "Batch 1255,  loss: 1.8658890008926392\n",
      "Batch 1260,  loss: 1.4685171365737915\n",
      "Batch 1265,  loss: 1.7359941482543946\n",
      "Batch 1270,  loss: 2.0382783889770506\n",
      "Batch 1275,  loss: 1.9009626865386964\n",
      "Batch 1280,  loss: 1.579075574874878\n",
      "Batch 1285,  loss: 2.0244640588760374\n",
      "Batch 1290,  loss: 1.65771803855896\n",
      "Batch 1295,  loss: 1.3939173221588135\n",
      "LOSS train 1.3939173221588135. Validation loss: 2.010852140740112 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 31:\n",
      "Batch 5,  loss: 2.0600830793380736\n",
      "Batch 10,  loss: 1.3426591157913208\n",
      "Batch 15,  loss: 1.4322365760803222\n",
      "Batch 20,  loss: 1.5371996879577636\n",
      "Batch 25,  loss: 1.2236999988555908\n",
      "Batch 30,  loss: 1.6206306219100952\n",
      "Batch 35,  loss: 1.4965805053710937\n",
      "Batch 40,  loss: 1.5472511649131775\n",
      "Batch 45,  loss: 1.6484552860260009\n",
      "Batch 50,  loss: 1.4425497770309448\n",
      "Batch 55,  loss: 1.8446908950805665\n",
      "Batch 60,  loss: 1.4330134391784668\n",
      "Batch 65,  loss: 1.522227168083191\n",
      "Batch 70,  loss: 1.584040927886963\n",
      "Batch 75,  loss: 1.7035012722015381\n",
      "Batch 80,  loss: 1.7420041799545287\n",
      "Batch 85,  loss: 1.7268224000930785\n",
      "Batch 90,  loss: 1.3805075645446778\n",
      "Batch 95,  loss: 1.7158169746398926\n",
      "Batch 100,  loss: 1.5681156396865845\n",
      "Batch 105,  loss: 1.9031416654586792\n",
      "Batch 110,  loss: 1.614492702484131\n",
      "Batch 115,  loss: 1.5404510736465453\n",
      "Batch 120,  loss: 1.3220157384872437\n",
      "Batch 125,  loss: 1.7494497776031495\n",
      "Batch 130,  loss: 1.8536141157150268\n",
      "Batch 135,  loss: 1.5894728422164917\n",
      "Batch 140,  loss: 1.5915041446685791\n",
      "Batch 145,  loss: 1.4633325576782226\n",
      "Batch 150,  loss: 1.5182698726654054\n",
      "Batch 155,  loss: 1.5005561113357544\n",
      "Batch 160,  loss: 1.4103172779083253\n",
      "Batch 165,  loss: 1.722991132736206\n",
      "Batch 170,  loss: 1.6463468074798584\n",
      "Batch 175,  loss: 1.5466403484344482\n",
      "Batch 180,  loss: 1.1392095565795899\n",
      "Batch 185,  loss: 1.3034080743789673\n",
      "Batch 190,  loss: 1.632074272632599\n",
      "Batch 195,  loss: 1.5400669813156127\n",
      "Batch 200,  loss: 1.8399948120117187\n",
      "Batch 205,  loss: 1.4671616792678832\n",
      "Batch 210,  loss: 1.4544896602630615\n",
      "Batch 215,  loss: 1.9864907145500184\n",
      "Batch 220,  loss: 1.7283184289932252\n",
      "Batch 225,  loss: 1.7658999681472778\n",
      "Batch 230,  loss: 1.8598420381546021\n",
      "Batch 235,  loss: 1.6391496419906617\n",
      "Batch 240,  loss: 1.331437635421753\n",
      "Batch 245,  loss: 1.4177401542663575\n",
      "Batch 250,  loss: 1.388574516773224\n",
      "Batch 255,  loss: 1.280307912826538\n",
      "Batch 260,  loss: 1.50561842918396\n",
      "Batch 265,  loss: 1.5364978790283204\n",
      "Batch 270,  loss: 1.3015823364257812\n",
      "Batch 275,  loss: 1.6545190334320068\n",
      "Batch 280,  loss: 1.5272767066955566\n",
      "Batch 285,  loss: 1.5045176982879638\n",
      "Batch 290,  loss: 1.320809268951416\n",
      "Batch 295,  loss: 1.757309126853943\n",
      "Batch 300,  loss: 1.8783210515975952\n",
      "Batch 305,  loss: 1.7763989448547364\n",
      "Batch 310,  loss: 1.6188862800598145\n",
      "Batch 315,  loss: 1.8060508012771606\n",
      "Batch 320,  loss: 1.620524263381958\n",
      "Batch 325,  loss: 1.7259990453720093\n",
      "Batch 330,  loss: 1.5192644357681275\n",
      "Batch 335,  loss: 1.7550337076187135\n",
      "Batch 340,  loss: 1.3762957572937011\n",
      "Batch 345,  loss: 1.2942385911941527\n",
      "Batch 350,  loss: 1.4360357761383056\n",
      "Batch 355,  loss: 1.7955134630203247\n",
      "Batch 360,  loss: 1.7631553411483765\n",
      "Batch 365,  loss: 1.7242082595825194\n",
      "Batch 370,  loss: 1.4106574773788452\n",
      "Batch 375,  loss: 1.56428062915802\n",
      "Batch 380,  loss: 1.492494249343872\n",
      "Batch 385,  loss: 1.6660701274871825\n",
      "Batch 390,  loss: 1.9468507528305055\n",
      "Batch 395,  loss: 1.2959131717681884\n",
      "Batch 400,  loss: 1.370660376548767\n",
      "Batch 405,  loss: 1.853834080696106\n",
      "Batch 410,  loss: 1.4874963521957398\n",
      "Batch 415,  loss: 1.3543077945709228\n",
      "Batch 420,  loss: 1.5523787975311278\n",
      "Batch 425,  loss: 1.423899269104004\n",
      "Batch 430,  loss: 1.751719880104065\n",
      "Batch 435,  loss: 1.4528061628341675\n",
      "Batch 440,  loss: 1.8132562160491943\n",
      "Batch 445,  loss: 1.344906711578369\n",
      "Batch 450,  loss: 1.352594828605652\n",
      "Batch 455,  loss: 1.4653470754623412\n",
      "Batch 460,  loss: 1.8021710872650147\n",
      "Batch 465,  loss: 1.4121977090835571\n",
      "Batch 470,  loss: 1.8049531698226928\n",
      "Batch 475,  loss: 1.8484716415405273\n",
      "Batch 480,  loss: 1.5106833934783936\n",
      "Batch 485,  loss: 1.7423283815383912\n",
      "Batch 490,  loss: 1.2550130128860473\n",
      "Batch 495,  loss: 1.4333979129791259\n",
      "Batch 500,  loss: 1.7767053842544556\n",
      "Batch 505,  loss: 1.8679707765579223\n",
      "Batch 510,  loss: 1.4614634037017822\n",
      "Batch 515,  loss: 1.777791690826416\n",
      "Batch 520,  loss: 1.771442723274231\n",
      "Batch 525,  loss: 1.8417645215988159\n",
      "Batch 530,  loss: 1.3932817220687865\n",
      "Batch 535,  loss: 1.512800645828247\n",
      "Batch 540,  loss: 1.6316831827163696\n",
      "Batch 545,  loss: 1.4732875227928162\n",
      "Batch 550,  loss: 1.4291768550872803\n",
      "Batch 555,  loss: 1.5469270944595337\n",
      "Batch 560,  loss: 1.6868887424468995\n",
      "Batch 565,  loss: 1.4838459730148315\n",
      "Batch 570,  loss: 1.499664831161499\n",
      "Batch 575,  loss: 1.4524346351623536\n",
      "Batch 580,  loss: 1.3102132201194763\n",
      "Batch 585,  loss: 1.281747055053711\n",
      "Batch 590,  loss: 1.1949660658836365\n",
      "Batch 595,  loss: 2.042036247253418\n",
      "Batch 600,  loss: 1.8204698324203492\n",
      "Batch 605,  loss: 1.3184917449951172\n",
      "Batch 610,  loss: 1.8928062438964843\n",
      "Batch 615,  loss: 1.4732421875\n",
      "Batch 620,  loss: 1.701445460319519\n",
      "Batch 625,  loss: 1.600516390800476\n",
      "Batch 630,  loss: 1.4770486116409303\n",
      "Batch 635,  loss: 2.073268175125122\n",
      "Batch 640,  loss: 1.4885363101959228\n",
      "Batch 645,  loss: 1.6704323768615723\n",
      "Batch 650,  loss: 1.3341809511184692\n",
      "Batch 655,  loss: 1.4282328128814696\n",
      "Batch 660,  loss: 1.8443002462387086\n",
      "Batch 665,  loss: 1.9765985488891602\n",
      "Batch 670,  loss: 1.5149182081222534\n",
      "Batch 675,  loss: 1.7468579053878783\n",
      "Batch 680,  loss: 1.683061122894287\n",
      "Batch 685,  loss: 1.4461085796356201\n",
      "Batch 690,  loss: 1.6398064136505126\n",
      "Batch 695,  loss: 1.5059775471687318\n",
      "Batch 700,  loss: 1.3672610521316528\n",
      "Batch 705,  loss: 1.7507519960403441\n",
      "Batch 710,  loss: 1.4519115686416626\n",
      "Batch 715,  loss: 1.5829281330108642\n",
      "Batch 720,  loss: 1.6533194303512573\n",
      "Batch 725,  loss: 1.5313851594924928\n",
      "Batch 730,  loss: 1.3070001125335693\n",
      "Batch 735,  loss: 1.7168511629104615\n",
      "Batch 740,  loss: 1.3716152667999268\n",
      "Batch 745,  loss: 1.7573855638504028\n",
      "Batch 750,  loss: 2.0551671743392945\n",
      "Batch 755,  loss: 1.595172381401062\n",
      "Batch 760,  loss: 1.8740491628646851\n",
      "Batch 765,  loss: 1.4878502368927002\n",
      "Batch 770,  loss: 1.531124997138977\n",
      "Batch 775,  loss: 1.9508128404617309\n",
      "Batch 780,  loss: 1.6149284720420838\n",
      "Batch 785,  loss: 1.345641303062439\n",
      "Batch 790,  loss: 1.4759443521499633\n",
      "Batch 795,  loss: 1.6770771265029907\n",
      "Batch 800,  loss: 1.3475541353225708\n",
      "Batch 805,  loss: 1.293356680870056\n",
      "Batch 810,  loss: 1.703990650177002\n",
      "Batch 815,  loss: 2.0396061658859255\n",
      "Batch 820,  loss: 1.5517883062362672\n",
      "Batch 825,  loss: 1.9912939548492432\n",
      "Batch 830,  loss: 1.3331769704818726\n",
      "Batch 835,  loss: 1.5983030319213867\n",
      "Batch 840,  loss: 1.7199682474136353\n",
      "Batch 845,  loss: 1.9025546789169312\n",
      "Batch 850,  loss: 1.5324326992034911\n",
      "Batch 855,  loss: 2.1175145626068117\n",
      "Batch 860,  loss: 1.85579354763031\n",
      "Batch 865,  loss: 1.7208689212799073\n",
      "Batch 870,  loss: 1.804712176322937\n",
      "Batch 875,  loss: 1.4035297274589538\n",
      "Batch 880,  loss: 1.667255163192749\n",
      "Batch 885,  loss: 1.543035638332367\n",
      "Batch 890,  loss: 1.491370439529419\n",
      "Batch 895,  loss: 1.5167424201965332\n",
      "Batch 900,  loss: 1.6243229866027833\n",
      "Batch 905,  loss: 1.3049143075942993\n",
      "Batch 910,  loss: 1.661110782623291\n",
      "Batch 915,  loss: 1.7192811727523805\n",
      "Batch 920,  loss: 1.3530280828475951\n",
      "Batch 925,  loss: 1.707304334640503\n",
      "Batch 930,  loss: 1.8561708688735963\n",
      "Batch 935,  loss: 1.5428838014602662\n",
      "Batch 940,  loss: 1.607259178161621\n",
      "Batch 945,  loss: 1.488230013847351\n",
      "Batch 950,  loss: 1.5631953001022338\n",
      "Batch 955,  loss: 1.5156211853027344\n",
      "Batch 960,  loss: 1.4591257333755494\n",
      "Batch 965,  loss: 1.5858857870101928\n",
      "Batch 970,  loss: 1.2922783374786377\n",
      "Batch 975,  loss: 1.5810206651687622\n",
      "Batch 980,  loss: 1.5064825057983398\n",
      "Batch 985,  loss: 1.495984673500061\n",
      "Batch 990,  loss: 1.5663806915283203\n",
      "Batch 995,  loss: 1.4447924852371217\n",
      "Batch 1000,  loss: 1.6122677326202393\n",
      "Batch 1005,  loss: 1.5544039249420165\n",
      "Batch 1010,  loss: 1.2279929995536805\n",
      "Batch 1015,  loss: 1.248222279548645\n",
      "Batch 1020,  loss: 1.4992307424545288\n",
      "Batch 1025,  loss: 1.5749804019927978\n",
      "Batch 1030,  loss: 1.4353193998336793\n",
      "Batch 1035,  loss: 1.996166753768921\n",
      "Batch 1040,  loss: 1.717531108856201\n",
      "Batch 1045,  loss: 1.43289475440979\n",
      "Batch 1050,  loss: 1.7952446222305298\n",
      "Batch 1055,  loss: 1.512358045578003\n",
      "Batch 1060,  loss: 1.1802812576293946\n",
      "Batch 1065,  loss: 1.8228555202484131\n",
      "Batch 1070,  loss: 1.2032967686653138\n",
      "Batch 1075,  loss: 1.8045149564743042\n",
      "Batch 1080,  loss: 1.5153188943862914\n",
      "Batch 1085,  loss: 1.5875808000564575\n",
      "Batch 1090,  loss: 1.5614433765411377\n",
      "Batch 1095,  loss: 1.4228788614273071\n",
      "Batch 1100,  loss: 1.5140462875366212\n",
      "Batch 1105,  loss: 1.0222270846366883\n",
      "Batch 1110,  loss: 1.4412214040756226\n",
      "Batch 1115,  loss: 1.6884976863861083\n",
      "Batch 1120,  loss: 1.6947458744049073\n",
      "Batch 1125,  loss: 1.4264471054077148\n",
      "Batch 1130,  loss: 1.5917474985122682\n",
      "Batch 1135,  loss: 1.993447232246399\n",
      "Batch 1140,  loss: 1.7007104635238648\n",
      "Batch 1145,  loss: 1.852643609046936\n",
      "Batch 1150,  loss: 1.592560887336731\n",
      "Batch 1155,  loss: 1.5039708018302917\n",
      "Batch 1160,  loss: 1.7253551006317138\n",
      "Batch 1165,  loss: 1.417179560661316\n",
      "Batch 1170,  loss: 1.4157170295715331\n",
      "Batch 1175,  loss: 2.2681612730026246\n",
      "Batch 1180,  loss: 1.6746407508850099\n",
      "Batch 1185,  loss: 1.6117833375930786\n",
      "Batch 1190,  loss: 1.7367273569107056\n",
      "Batch 1195,  loss: 1.5200145959854126\n",
      "Batch 1200,  loss: 2.0903080463409425\n",
      "Batch 1205,  loss: 1.4892130374908448\n",
      "Batch 1210,  loss: 2.013335657119751\n",
      "Batch 1215,  loss: 1.8592227458953858\n",
      "Batch 1220,  loss: 1.34060697555542\n",
      "Batch 1225,  loss: 1.5810330152511596\n",
      "Batch 1230,  loss: 1.5121872663497924\n",
      "Batch 1235,  loss: 1.7519633293151855\n",
      "Batch 1240,  loss: 1.465846562385559\n",
      "Batch 1245,  loss: 1.6489356756210327\n",
      "Batch 1250,  loss: 2.084015154838562\n",
      "Batch 1255,  loss: 1.4938819408416748\n",
      "Batch 1260,  loss: 1.3349868178367614\n",
      "Batch 1265,  loss: 1.392156994342804\n",
      "Batch 1270,  loss: 1.8998539209365846\n",
      "Batch 1275,  loss: 1.4964112520217896\n",
      "Batch 1280,  loss: 1.8765086650848388\n",
      "Batch 1285,  loss: 2.0393434524536134\n",
      "Batch 1290,  loss: 1.8374431133270264\n",
      "Batch 1295,  loss: 1.4451624155044556\n",
      "LOSS train 1.4451624155044556. Validation loss: 2.0598856376690997 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 32:\n",
      "Batch 5,  loss: 2.070713973045349\n",
      "Batch 10,  loss: 1.592745018005371\n",
      "Batch 15,  loss: 1.7517020225524902\n",
      "Batch 20,  loss: 1.4537311315536499\n",
      "Batch 25,  loss: 1.318622350692749\n",
      "Batch 30,  loss: 1.7201522588729858\n",
      "Batch 35,  loss: 1.5034938335418702\n",
      "Batch 40,  loss: 1.6772338628768921\n",
      "Batch 45,  loss: 1.7797676563262939\n",
      "Batch 50,  loss: 1.4616963624954225\n",
      "Batch 55,  loss: 1.4550559282302857\n",
      "Batch 60,  loss: 1.8670827388763427\n",
      "Batch 65,  loss: 1.2341143369674683\n",
      "Batch 70,  loss: 1.7610054731369018\n",
      "Batch 75,  loss: 1.8246388912200928\n",
      "Batch 80,  loss: 1.4020869255065918\n",
      "Batch 85,  loss: 1.4765270233154297\n",
      "Batch 90,  loss: 1.7924672603607177\n",
      "Batch 95,  loss: 1.7580493688583374\n",
      "Batch 100,  loss: 1.8735191822052002\n",
      "Batch 105,  loss: 1.4491276502609254\n",
      "Batch 110,  loss: 1.5773443937301637\n",
      "Batch 115,  loss: 1.4776774644851685\n",
      "Batch 120,  loss: 1.3953279376029968\n",
      "Batch 125,  loss: 1.5787830114364625\n",
      "Batch 130,  loss: 1.6237191200256347\n",
      "Batch 135,  loss: 1.4174512386322022\n",
      "Batch 140,  loss: 1.6663284063339234\n",
      "Batch 145,  loss: 1.4981500387191773\n",
      "Batch 150,  loss: 1.381061577796936\n",
      "Batch 155,  loss: 1.6923843383789063\n",
      "Batch 160,  loss: 1.4079960584640503\n",
      "Batch 165,  loss: 1.5089033603668214\n",
      "Batch 170,  loss: 1.3604079246520997\n",
      "Batch 175,  loss: 1.5433100461959839\n",
      "Batch 180,  loss: 1.3121563911437988\n",
      "Batch 185,  loss: 1.6475316524505614\n",
      "Batch 190,  loss: 1.2457544088363648\n",
      "Batch 195,  loss: 1.476937961578369\n",
      "Batch 200,  loss: 1.6406949281692504\n",
      "Batch 205,  loss: 1.4178439378738403\n",
      "Batch 210,  loss: 1.4599481105804444\n",
      "Batch 215,  loss: 1.9183396816253662\n",
      "Batch 220,  loss: 1.7188575744628907\n",
      "Batch 225,  loss: 1.5812168836593627\n",
      "Batch 230,  loss: 1.6874117851257324\n",
      "Batch 235,  loss: 1.7029954671859742\n",
      "Batch 240,  loss: 1.6380388975143432\n",
      "Batch 245,  loss: 1.4796198725700378\n",
      "Batch 250,  loss: 1.3382201433181762\n",
      "Batch 255,  loss: 1.6544814348220824\n",
      "Batch 260,  loss: 1.5046782732009887\n",
      "Batch 265,  loss: 1.6611972808837892\n",
      "Batch 270,  loss: 1.8233638286590577\n",
      "Batch 275,  loss: 1.558978271484375\n",
      "Batch 280,  loss: 1.6210445046424866\n",
      "Batch 285,  loss: 1.7510235548019408\n",
      "Batch 290,  loss: 1.9116431713104247\n",
      "Batch 295,  loss: 1.3635806322097779\n",
      "Batch 300,  loss: 1.8020779371261597\n",
      "Batch 305,  loss: 1.4511021852493287\n",
      "Batch 310,  loss: 1.5454428911209106\n",
      "Batch 315,  loss: 1.6358752369880676\n",
      "Batch 320,  loss: 1.481308889389038\n",
      "Batch 325,  loss: 2.2691205024719237\n",
      "Batch 330,  loss: 1.5352181792259216\n",
      "Batch 335,  loss: 1.5518345832824707\n",
      "Batch 340,  loss: 1.762153196334839\n",
      "Batch 345,  loss: 1.741124725341797\n",
      "Batch 350,  loss: 1.6951465606689453\n",
      "Batch 355,  loss: 1.631592583656311\n",
      "Batch 360,  loss: 1.663130807876587\n",
      "Batch 365,  loss: 1.5345797538757324\n",
      "Batch 370,  loss: 1.77006938457489\n",
      "Batch 375,  loss: 1.6449572801589967\n",
      "Batch 380,  loss: 1.6348007678985597\n",
      "Batch 385,  loss: 2.06583309173584\n",
      "Batch 390,  loss: 1.5340725660324097\n",
      "Batch 395,  loss: 1.5856467247009278\n",
      "Batch 400,  loss: 1.4861332535743714\n",
      "Batch 405,  loss: 1.818609642982483\n",
      "Batch 410,  loss: 1.4810206770896912\n",
      "Batch 415,  loss: 1.7467647790908813\n",
      "Batch 420,  loss: 1.4213529109954834\n",
      "Batch 425,  loss: 1.6893940448760987\n",
      "Batch 430,  loss: 1.6112661838531495\n",
      "Batch 435,  loss: 1.306596612930298\n",
      "Batch 440,  loss: 1.867109751701355\n",
      "Batch 445,  loss: 1.4050851225852967\n",
      "Batch 450,  loss: 1.425435769557953\n",
      "Batch 455,  loss: 1.4945042610168457\n",
      "Batch 460,  loss: 1.886165237426758\n",
      "Batch 465,  loss: 1.73413405418396\n",
      "Batch 470,  loss: 1.673394775390625\n",
      "Batch 475,  loss: 1.2706345319747925\n",
      "Batch 480,  loss: 1.4859233856201173\n",
      "Batch 485,  loss: 1.9018876791000365\n",
      "Batch 490,  loss: 1.5099320650100707\n",
      "Batch 495,  loss: 1.5026316046714783\n",
      "Batch 500,  loss: 1.83969247341156\n",
      "Batch 505,  loss: 1.8169060230255127\n",
      "Batch 510,  loss: 1.5407691717147827\n",
      "Batch 515,  loss: 1.6206401109695434\n",
      "Batch 520,  loss: 1.5155241250991822\n",
      "Batch 525,  loss: 1.715188217163086\n",
      "Batch 530,  loss: 1.5859514713287353\n",
      "Batch 535,  loss: 1.5442659378051757\n",
      "Batch 540,  loss: 1.5988804817199707\n",
      "Batch 545,  loss: 1.4165750741958618\n",
      "Batch 550,  loss: 1.5554317831993103\n",
      "Batch 555,  loss: 1.682957935333252\n",
      "Batch 560,  loss: 1.3535739660263062\n",
      "Batch 565,  loss: 1.5682894706726074\n",
      "Batch 570,  loss: 1.3447931289672852\n",
      "Batch 575,  loss: 1.533737564086914\n",
      "Batch 580,  loss: 1.3579515933990478\n",
      "Batch 585,  loss: 1.546086025238037\n",
      "Batch 590,  loss: 1.4184488534927369\n",
      "Batch 595,  loss: 1.3755227088928224\n",
      "Batch 600,  loss: 1.7183101892471313\n",
      "Batch 605,  loss: 1.74752938747406\n",
      "Batch 610,  loss: 1.532561469078064\n",
      "Batch 615,  loss: 1.3738049030303956\n",
      "Batch 620,  loss: 1.834661889076233\n",
      "Batch 625,  loss: 1.7296336889266968\n",
      "Batch 630,  loss: 1.4372589111328125\n",
      "Batch 635,  loss: 1.6081766605377197\n",
      "Batch 640,  loss: 1.4817838430404664\n",
      "Batch 645,  loss: 1.6809022188186646\n",
      "Batch 650,  loss: 1.645250678062439\n",
      "Batch 655,  loss: 1.7535409688949586\n",
      "Batch 660,  loss: 1.75220787525177\n",
      "Batch 665,  loss: 1.4723026514053346\n",
      "Batch 670,  loss: 1.7957932472229003\n",
      "Batch 675,  loss: 1.7381349086761475\n",
      "Batch 680,  loss: 1.5370994091033936\n",
      "Batch 685,  loss: 1.4608954548835755\n",
      "Batch 690,  loss: 1.6784912586212157\n",
      "Batch 695,  loss: 1.565150833129883\n",
      "Batch 700,  loss: 1.4180782318115235\n",
      "Batch 705,  loss: 1.595347237586975\n",
      "Batch 710,  loss: 1.7874797344207765\n",
      "Batch 715,  loss: 1.7978585243225098\n",
      "Batch 720,  loss: 1.400605356693268\n",
      "Batch 725,  loss: 1.8215400457382203\n",
      "Batch 730,  loss: 1.2522868752479552\n",
      "Batch 735,  loss: 1.4463095903396606\n",
      "Batch 740,  loss: 1.7706936120986938\n",
      "Batch 745,  loss: 1.5472903966903686\n",
      "Batch 750,  loss: 1.605018639564514\n",
      "Batch 755,  loss: 1.7575381994247437\n",
      "Batch 760,  loss: 1.58865327835083\n",
      "Batch 765,  loss: 1.3293686151504516\n",
      "Batch 770,  loss: 1.7399692058563232\n",
      "Batch 775,  loss: 1.6370368003845215\n",
      "Batch 780,  loss: 1.6713555335998536\n",
      "Batch 785,  loss: 1.8118849515914917\n",
      "Batch 790,  loss: 1.9596543550491332\n",
      "Batch 795,  loss: 1.5485925436019898\n",
      "Batch 800,  loss: 1.6786335706710815\n",
      "Batch 805,  loss: 1.5929342746734618\n",
      "Batch 810,  loss: 1.6169793009757996\n",
      "Batch 815,  loss: 1.7949403762817382\n",
      "Batch 820,  loss: 1.3607274174690247\n",
      "Batch 825,  loss: 1.4109431266784669\n",
      "Batch 830,  loss: 1.4208470582962036\n",
      "Batch 835,  loss: 1.480356478691101\n",
      "Batch 840,  loss: 1.8132290840148926\n",
      "Batch 845,  loss: 1.6387421369552613\n",
      "Batch 850,  loss: 1.3748318433761597\n",
      "Batch 855,  loss: 1.24795982837677\n",
      "Batch 860,  loss: 1.470117735862732\n",
      "Batch 865,  loss: 1.8888766527175904\n",
      "Batch 870,  loss: 1.3312063813209534\n",
      "Batch 875,  loss: 1.6064254283905028\n",
      "Batch 880,  loss: 1.7347583055496216\n",
      "Batch 885,  loss: 1.4743805408477784\n",
      "Batch 890,  loss: 1.4739961624145508\n",
      "Batch 895,  loss: 1.3119002819061278\n",
      "Batch 900,  loss: 1.164650285243988\n",
      "Batch 905,  loss: 1.3702093601226806\n",
      "Batch 910,  loss: 1.707890510559082\n",
      "Batch 915,  loss: 1.467123818397522\n",
      "Batch 920,  loss: 1.6720582485198974\n",
      "Batch 925,  loss: 1.5629000663757324\n",
      "Batch 930,  loss: 1.2826928853988648\n",
      "Batch 935,  loss: 1.362394118309021\n",
      "Batch 940,  loss: 1.5158249616622925\n",
      "Batch 945,  loss: 1.6167594194412231\n",
      "Batch 950,  loss: 1.654918384552002\n",
      "Batch 955,  loss: 1.6831101655960083\n",
      "Batch 960,  loss: 1.6153823852539062\n",
      "Batch 965,  loss: 1.3462767124176025\n",
      "Batch 970,  loss: 1.539686369895935\n",
      "Batch 975,  loss: 1.8557397842407226\n",
      "Batch 980,  loss: 1.5345090866088866\n",
      "Batch 985,  loss: 1.7032299995422364\n",
      "Batch 990,  loss: 1.3739365100860597\n",
      "Batch 995,  loss: 1.4541685581207275\n",
      "Batch 1000,  loss: 1.5933226823806763\n",
      "Batch 1005,  loss: 1.451183295249939\n",
      "Batch 1010,  loss: 1.6842625141143799\n",
      "Batch 1015,  loss: 1.623810338973999\n",
      "Batch 1020,  loss: 1.641463327407837\n",
      "Batch 1025,  loss: 1.801803994178772\n",
      "Batch 1030,  loss: 1.6368071794509889\n",
      "Batch 1035,  loss: 1.8823846578598022\n",
      "Batch 1040,  loss: 1.7316486835479736\n",
      "Batch 1045,  loss: 1.380489468574524\n",
      "Batch 1050,  loss: 1.7003971576690673\n",
      "Batch 1055,  loss: 1.3589216232299806\n",
      "Batch 1060,  loss: 1.3867701530456542\n",
      "Batch 1065,  loss: 1.6826124906539917\n",
      "Batch 1070,  loss: 1.4830814599990845\n",
      "Batch 1075,  loss: 1.3350050687789916\n",
      "Batch 1080,  loss: 1.403914713859558\n",
      "Batch 1085,  loss: 1.290170168876648\n",
      "Batch 1090,  loss: 1.4554991841316223\n",
      "Batch 1095,  loss: 1.5490334987640382\n",
      "Batch 1100,  loss: 1.513217520713806\n",
      "Batch 1105,  loss: 1.8162472009658814\n",
      "Batch 1110,  loss: 1.6240995407104493\n",
      "Batch 1115,  loss: 1.1973761796951294\n",
      "Batch 1120,  loss: 1.5307502508163453\n",
      "Batch 1125,  loss: 1.7116678476333618\n",
      "Batch 1130,  loss: 1.6982292413711548\n",
      "Batch 1135,  loss: 1.8231120109558105\n",
      "Batch 1140,  loss: 1.2128191709518432\n",
      "Batch 1145,  loss: 1.8309781193733214\n",
      "Batch 1150,  loss: 1.6611857175827027\n",
      "Batch 1155,  loss: 1.753525733947754\n",
      "Batch 1160,  loss: 1.2670052528381348\n",
      "Batch 1165,  loss: 1.344104814529419\n",
      "Batch 1170,  loss: 1.5924270868301391\n",
      "Batch 1175,  loss: 1.3609392166137695\n",
      "Batch 1180,  loss: 1.5221905708312988\n",
      "Batch 1185,  loss: 1.1068521976470946\n",
      "Batch 1190,  loss: 2.0675541162490845\n",
      "Batch 1195,  loss: 1.6301342964172363\n",
      "Batch 1200,  loss: 1.409851038455963\n",
      "Batch 1205,  loss: 1.651351761817932\n",
      "Batch 1210,  loss: 1.5716160893440247\n",
      "Batch 1215,  loss: 1.4451090335845946\n",
      "Batch 1220,  loss: 2.0305628776550293\n",
      "Batch 1225,  loss: 1.3896376848220826\n",
      "Batch 1230,  loss: 1.4752326011657715\n",
      "Batch 1235,  loss: 1.7990755319595337\n",
      "Batch 1240,  loss: 1.4239198207855224\n",
      "Batch 1245,  loss: 1.7290040731430054\n",
      "Batch 1250,  loss: 1.2803037524223329\n",
      "Batch 1255,  loss: 1.5084226846694946\n",
      "Batch 1260,  loss: 1.7907772302627563\n",
      "Batch 1265,  loss: 1.9319690704345702\n",
      "Batch 1270,  loss: 1.5806270837783813\n",
      "Batch 1275,  loss: 1.6981390714645386\n",
      "Batch 1280,  loss: 1.4263854026794434\n",
      "Batch 1285,  loss: 1.8328050374984741\n",
      "Batch 1290,  loss: 1.45883150100708\n",
      "Batch 1295,  loss: 1.4134597301483154\n",
      "LOSS train 1.4134597301483154. Validation loss: 2.271887446663998 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 33:\n",
      "Batch 5,  loss: 1.4460230827331544\n",
      "Batch 10,  loss: 1.5425294160842895\n",
      "Batch 15,  loss: 1.9212972164154052\n",
      "Batch 20,  loss: 1.36787291765213\n",
      "Batch 25,  loss: 1.536143946647644\n",
      "Batch 30,  loss: 1.3897787570953368\n",
      "Batch 35,  loss: 1.7209432601928711\n",
      "Batch 40,  loss: 1.593188738822937\n",
      "Batch 45,  loss: 1.6509387016296386\n",
      "Batch 50,  loss: 1.6399921894073486\n",
      "Batch 55,  loss: 1.636736559867859\n",
      "Batch 60,  loss: 1.3452966213226318\n",
      "Batch 65,  loss: 1.6038722395896912\n",
      "Batch 70,  loss: 1.5058550119400025\n",
      "Batch 75,  loss: 1.7030341863632201\n",
      "Batch 80,  loss: 1.654413390159607\n",
      "Batch 85,  loss: 1.9604302644729614\n",
      "Batch 90,  loss: 1.8430572032928467\n",
      "Batch 95,  loss: 1.4679773807525636\n",
      "Batch 100,  loss: 1.6128945112228394\n",
      "Batch 105,  loss: 1.3437352895736694\n",
      "Batch 110,  loss: 1.5222934484481812\n",
      "Batch 115,  loss: 1.3211921453475952\n",
      "Batch 120,  loss: 1.3508068561553954\n",
      "Batch 125,  loss: 1.7262593030929565\n",
      "Batch 130,  loss: 1.55260272026062\n",
      "Batch 135,  loss: 1.5623902797698974\n",
      "Batch 140,  loss: 1.6453222990036012\n",
      "Batch 145,  loss: 1.614266610145569\n",
      "Batch 150,  loss: 1.4160365104675292\n",
      "Batch 155,  loss: 1.4913652896881104\n",
      "Batch 160,  loss: 1.4857435941696167\n",
      "Batch 165,  loss: 1.4349501848220825\n",
      "Batch 170,  loss: 1.43777893781662\n",
      "Batch 175,  loss: 1.3821584701538085\n",
      "Batch 180,  loss: 1.436184287071228\n",
      "Batch 185,  loss: 1.6124903678894043\n",
      "Batch 190,  loss: 1.3377963781356812\n",
      "Batch 195,  loss: 1.3095799207687377\n",
      "Batch 200,  loss: 1.6175705909729003\n",
      "Batch 205,  loss: 2.0445462465286255\n",
      "Batch 210,  loss: 1.590145468711853\n",
      "Batch 215,  loss: 1.5746306419372558\n",
      "Batch 220,  loss: 1.533798837661743\n",
      "Batch 225,  loss: 1.5380131244659423\n",
      "Batch 230,  loss: 1.6478042602539062\n",
      "Batch 235,  loss: 1.6435327768325805\n",
      "Batch 240,  loss: 1.7748535394668579\n",
      "Batch 245,  loss: 1.5472495317459107\n",
      "Batch 250,  loss: 1.618276262283325\n",
      "Batch 255,  loss: 1.636427891254425\n",
      "Batch 260,  loss: 1.513574242591858\n",
      "Batch 265,  loss: 1.5246266841888427\n",
      "Batch 270,  loss: 1.83828706741333\n",
      "Batch 275,  loss: 1.7473459005355836\n",
      "Batch 280,  loss: 1.1911539793014527\n",
      "Batch 285,  loss: 1.736003828048706\n",
      "Batch 290,  loss: 1.17253737449646\n",
      "Batch 295,  loss: 1.5578990817070006\n",
      "Batch 300,  loss: 1.6551316738128663\n",
      "Batch 305,  loss: 1.7610525131225585\n",
      "Batch 310,  loss: 1.5836834192276001\n",
      "Batch 315,  loss: 1.474434757232666\n",
      "Batch 320,  loss: 1.546303129196167\n",
      "Batch 325,  loss: 1.6089586019515991\n",
      "Batch 330,  loss: 1.5703959226608277\n",
      "Batch 335,  loss: 1.6774598360061646\n",
      "Batch 340,  loss: 1.3803802967071532\n",
      "Batch 345,  loss: 1.597643756866455\n",
      "Batch 350,  loss: 1.5939788579940797\n",
      "Batch 355,  loss: 1.6751299619674682\n",
      "Batch 360,  loss: 1.6192212343215941\n",
      "Batch 365,  loss: 1.4264231204986573\n",
      "Batch 370,  loss: 1.4147438764572144\n",
      "Batch 375,  loss: 1.4108191967010497\n",
      "Batch 380,  loss: 1.6426664590835571\n",
      "Batch 385,  loss: 1.6257296562194825\n",
      "Batch 390,  loss: 1.5610622406005858\n",
      "Batch 395,  loss: 1.3308812856674195\n",
      "Batch 400,  loss: 1.6202601194381714\n",
      "Batch 405,  loss: 1.7114102840423584\n",
      "Batch 410,  loss: 1.7533390045166015\n",
      "Batch 415,  loss: 1.7512896060943604\n",
      "Batch 420,  loss: 1.672744870185852\n",
      "Batch 425,  loss: 2.0319780111312866\n",
      "Batch 430,  loss: 1.4602597236633301\n",
      "Batch 435,  loss: 1.406176543235779\n",
      "Batch 440,  loss: 1.4671619892120362\n",
      "Batch 445,  loss: 1.2895421385765076\n",
      "Batch 450,  loss: 1.4539321660995483\n",
      "Batch 455,  loss: 1.483700442314148\n",
      "Batch 460,  loss: 1.4234702587127686\n",
      "Batch 465,  loss: 1.545287299156189\n",
      "Batch 470,  loss: 1.426565408706665\n",
      "Batch 475,  loss: 1.2239389538764953\n",
      "Batch 480,  loss: 1.6053277492523192\n",
      "Batch 485,  loss: 1.6888579368591308\n",
      "Batch 490,  loss: 1.624243474006653\n",
      "Batch 495,  loss: 1.7947372913360595\n",
      "Batch 500,  loss: 1.4779208183288575\n",
      "Batch 505,  loss: 1.304035496711731\n",
      "Batch 510,  loss: 1.3400875806808472\n",
      "Batch 515,  loss: 1.3856922149658204\n",
      "Batch 520,  loss: 1.5547735929489135\n",
      "Batch 525,  loss: 1.4579237699508667\n",
      "Batch 530,  loss: 1.2700093626976012\n",
      "Batch 535,  loss: 1.5181947946548462\n",
      "Batch 540,  loss: 1.6255200862884522\n",
      "Batch 545,  loss: 1.7534727096557616\n",
      "Batch 550,  loss: 1.7351591825485229\n",
      "Batch 555,  loss: 1.7091169834136963\n",
      "Batch 560,  loss: 1.427394413948059\n",
      "Batch 565,  loss: 1.1514146089553834\n",
      "Batch 570,  loss: 1.5646398544311524\n",
      "Batch 575,  loss: 1.705591368675232\n",
      "Batch 580,  loss: 1.438322949409485\n",
      "Batch 585,  loss: 1.4788768529891967\n",
      "Batch 590,  loss: 1.5425804615020753\n",
      "Batch 595,  loss: 1.8587619304656982\n",
      "Batch 600,  loss: 1.7615504026412965\n",
      "Batch 605,  loss: 1.9789846181869506\n",
      "Batch 610,  loss: 1.482705569267273\n",
      "Batch 615,  loss: 1.659478497505188\n",
      "Batch 620,  loss: 1.581165623664856\n",
      "Batch 625,  loss: 1.7404675006866455\n",
      "Batch 630,  loss: 1.5412461757659912\n",
      "Batch 635,  loss: 1.395349097251892\n",
      "Batch 640,  loss: 1.5780897617340088\n",
      "Batch 645,  loss: 1.582642912864685\n",
      "Batch 650,  loss: 1.6244447946548461\n",
      "Batch 655,  loss: 2.092522144317627\n",
      "Batch 660,  loss: 1.429934298992157\n",
      "Batch 665,  loss: 1.412187886238098\n",
      "Batch 670,  loss: 1.712640142440796\n",
      "Batch 675,  loss: 1.6849129438400268\n",
      "Batch 680,  loss: 1.4918809175491332\n",
      "Batch 685,  loss: 1.620556104183197\n",
      "Batch 690,  loss: 1.6458781480789184\n",
      "Batch 695,  loss: 1.1326526165008546\n",
      "Batch 700,  loss: 1.5973218917846679\n",
      "Batch 705,  loss: 1.3635735273361207\n",
      "Batch 710,  loss: 1.4256893277168274\n",
      "Batch 715,  loss: 1.4296902656555175\n",
      "Batch 720,  loss: 1.655821180343628\n",
      "Batch 725,  loss: 1.6374437808990479\n",
      "Batch 730,  loss: 1.6217500686645507\n",
      "Batch 735,  loss: 1.6771403551101685\n",
      "Batch 740,  loss: 1.6886853456497193\n",
      "Batch 745,  loss: 1.982080864906311\n",
      "Batch 750,  loss: 1.5820239067077637\n",
      "Batch 755,  loss: 1.6306398630142211\n",
      "Batch 760,  loss: 1.5269205570220947\n",
      "Batch 765,  loss: 1.553470253944397\n",
      "Batch 770,  loss: 1.6457947492599487\n",
      "Batch 775,  loss: 1.3439905643463135\n",
      "Batch 780,  loss: 1.9120868682861327\n",
      "Batch 785,  loss: 1.6634158372879029\n",
      "Batch 790,  loss: 1.4756667375564576\n",
      "Batch 795,  loss: 1.4975452423095703\n",
      "Batch 800,  loss: 1.3832878351211548\n",
      "Batch 805,  loss: 1.4487772703170776\n",
      "Batch 810,  loss: 1.730616855621338\n",
      "Batch 815,  loss: 1.5496634721755982\n",
      "Batch 820,  loss: 1.482451581954956\n",
      "Batch 825,  loss: 1.5437360763549806\n",
      "Batch 830,  loss: 1.4804274559020996\n",
      "Batch 835,  loss: 1.418908166885376\n",
      "Batch 840,  loss: 1.6474045991897583\n",
      "Batch 845,  loss: 1.6603765487670898\n",
      "Batch 850,  loss: 1.4780479669570923\n",
      "Batch 855,  loss: 2.019035291671753\n",
      "Batch 860,  loss: 1.5432757139205933\n",
      "Batch 865,  loss: 1.4957523465156555\n",
      "Batch 870,  loss: 1.3022554874420167\n",
      "Batch 875,  loss: 2.047268509864807\n",
      "Batch 880,  loss: 1.8612867593765259\n",
      "Batch 885,  loss: 1.3816399812698363\n",
      "Batch 890,  loss: 1.4032546997070312\n",
      "Batch 895,  loss: 1.7685461044311523\n",
      "Batch 900,  loss: 1.5080045938491822\n",
      "Batch 905,  loss: 1.674877405166626\n",
      "Batch 910,  loss: 1.369451892375946\n",
      "Batch 915,  loss: 1.7032742500305176\n",
      "Batch 920,  loss: 1.6458203077316285\n",
      "Batch 925,  loss: 1.5856659412384033\n",
      "Batch 930,  loss: 1.2195588588714599\n",
      "Batch 935,  loss: 1.6946711540222168\n",
      "Batch 940,  loss: 1.6621474027633667\n",
      "Batch 945,  loss: 1.3874372482299804\n",
      "Batch 950,  loss: 1.3621026277542114\n",
      "Batch 955,  loss: 1.9569040298461915\n",
      "Batch 960,  loss: 1.1893500089645386\n",
      "Batch 965,  loss: 1.3777600526809692\n",
      "Batch 970,  loss: 1.972467303276062\n",
      "Batch 975,  loss: 1.8289434671401978\n",
      "Batch 980,  loss: 1.5480802536010743\n",
      "Batch 985,  loss: 1.7169358015060425\n",
      "Batch 990,  loss: 1.6055912017822265\n",
      "Batch 995,  loss: 2.1015985012054443\n",
      "Batch 1000,  loss: 1.3795599937438965\n",
      "Batch 1005,  loss: 1.5816646814346313\n",
      "Batch 1010,  loss: 1.8991161823272704\n",
      "Batch 1015,  loss: 1.2941716194152832\n",
      "Batch 1020,  loss: 1.5529261350631713\n",
      "Batch 1025,  loss: 1.6147461175918578\n",
      "Batch 1030,  loss: 1.474339485168457\n",
      "Batch 1035,  loss: 1.4819945096969604\n",
      "Batch 1040,  loss: 1.3905750274658204\n",
      "Batch 1045,  loss: 1.5345618009567261\n",
      "Batch 1050,  loss: 1.574836826324463\n",
      "Batch 1055,  loss: 1.4728078365325927\n",
      "Batch 1060,  loss: 1.7830267190933227\n",
      "Batch 1065,  loss: 1.607868242263794\n",
      "Batch 1070,  loss: 1.5967621803283691\n",
      "Batch 1075,  loss: 1.8665350198745727\n",
      "Batch 1080,  loss: 1.5563074827194214\n",
      "Batch 1085,  loss: 1.645442247390747\n",
      "Batch 1090,  loss: 1.1078773617744446\n",
      "Batch 1095,  loss: 1.712909746170044\n",
      "Batch 1100,  loss: 1.8397557258605957\n",
      "Batch 1105,  loss: 1.355388355255127\n",
      "Batch 1110,  loss: 1.7850191235542296\n",
      "Batch 1115,  loss: 1.7096152305603027\n",
      "Batch 1120,  loss: 1.8379813432693481\n",
      "Batch 1125,  loss: 1.4044296741485596\n",
      "Batch 1130,  loss: 1.36633403301239\n",
      "Batch 1135,  loss: 1.7196285247802734\n",
      "Batch 1140,  loss: 1.5547544717788697\n",
      "Batch 1145,  loss: 1.6839367270469665\n",
      "Batch 1150,  loss: 1.4633078575134277\n",
      "Batch 1155,  loss: 1.9773900270462037\n",
      "Batch 1160,  loss: 1.3481337308883667\n",
      "Batch 1165,  loss: 1.5083792448043822\n",
      "Batch 1170,  loss: 1.4179264783859253\n",
      "Batch 1175,  loss: 1.9711560010910034\n",
      "Batch 1180,  loss: 1.7868518590927125\n",
      "Batch 1185,  loss: 1.830516767501831\n",
      "Batch 1190,  loss: 1.500371527671814\n",
      "Batch 1195,  loss: 1.284857738018036\n",
      "Batch 1200,  loss: 1.6800102710723877\n",
      "Batch 1205,  loss: 1.914305067062378\n",
      "Batch 1210,  loss: 1.488811230659485\n",
      "Batch 1215,  loss: 1.6456389665603637\n",
      "Batch 1220,  loss: 1.7356968879699708\n",
      "Batch 1225,  loss: 1.4961749196052552\n",
      "Batch 1230,  loss: 1.4654019594192504\n",
      "Batch 1235,  loss: 1.4194692850112915\n",
      "Batch 1240,  loss: 1.1998896479606629\n",
      "Batch 1245,  loss: 1.4365331649780273\n",
      "Batch 1250,  loss: 1.470334267616272\n",
      "Batch 1255,  loss: 1.5052937507629394\n",
      "Batch 1260,  loss: 1.4704185247421264\n",
      "Batch 1265,  loss: 1.649946928024292\n",
      "Batch 1270,  loss: 1.4778664588928223\n",
      "Batch 1275,  loss: 1.2582218527793885\n",
      "Batch 1280,  loss: 1.8638047695159912\n",
      "Batch 1285,  loss: 1.2315414309501649\n",
      "Batch 1290,  loss: 1.8294424295425415\n",
      "Batch 1295,  loss: 1.251219391822815\n",
      "LOSS train 1.251219391822815. Validation loss: 1.9305309900669037 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 34:\n",
      "Batch 5,  loss: 1.3851304411888123\n",
      "Batch 10,  loss: 1.5215657949447632\n",
      "Batch 15,  loss: 1.6849332809448243\n",
      "Batch 20,  loss: 1.20164235830307\n",
      "Batch 25,  loss: 1.5442304611206055\n",
      "Batch 30,  loss: 1.9628470182418822\n",
      "Batch 35,  loss: 1.6204967021942138\n",
      "Batch 40,  loss: 1.762752318382263\n",
      "Batch 45,  loss: 1.5564244747161866\n",
      "Batch 50,  loss: 1.454292321205139\n",
      "Batch 55,  loss: 1.3277947664260865\n",
      "Batch 60,  loss: 1.391444778442383\n",
      "Batch 65,  loss: 1.7035647630691528\n",
      "Batch 70,  loss: 1.2989024639129638\n",
      "Batch 75,  loss: 1.4241294622421266\n",
      "Batch 80,  loss: 1.5880708932876586\n",
      "Batch 85,  loss: 1.4852174997329712\n",
      "Batch 90,  loss: 1.8258613109588624\n",
      "Batch 95,  loss: 1.6886536121368407\n",
      "Batch 100,  loss: 1.7030025005340577\n",
      "Batch 105,  loss: 1.3123425722122193\n",
      "Batch 110,  loss: 1.4441931962966919\n",
      "Batch 115,  loss: 1.6968338251113892\n",
      "Batch 120,  loss: 1.759105372428894\n",
      "Batch 125,  loss: 1.2034257531166077\n",
      "Batch 130,  loss: 1.754375147819519\n",
      "Batch 135,  loss: 1.5677402019500732\n",
      "Batch 140,  loss: 1.7517886400222777\n",
      "Batch 145,  loss: 1.4022908687591553\n",
      "Batch 150,  loss: 1.8121110677719117\n",
      "Batch 155,  loss: 1.6472161889076233\n",
      "Batch 160,  loss: 1.360753846168518\n",
      "Batch 165,  loss: 1.7260167360305787\n",
      "Batch 170,  loss: 1.3652724266052245\n",
      "Batch 175,  loss: 1.6528613090515136\n",
      "Batch 180,  loss: 1.5302529335021973\n",
      "Batch 185,  loss: 1.270058274269104\n",
      "Batch 190,  loss: 1.7136910438537598\n",
      "Batch 195,  loss: 1.523913526535034\n",
      "Batch 200,  loss: 1.557982039451599\n",
      "Batch 205,  loss: 1.3058078289031982\n",
      "Batch 210,  loss: 1.3631237626075745\n",
      "Batch 215,  loss: 1.5821593642234801\n",
      "Batch 220,  loss: 1.230846071243286\n",
      "Batch 225,  loss: 1.5204089522361754\n",
      "Batch 230,  loss: 1.6576021194458008\n",
      "Batch 235,  loss: 1.7699149131774903\n",
      "Batch 240,  loss: 1.4197983503341676\n",
      "Batch 245,  loss: 1.6780295372009277\n",
      "Batch 250,  loss: 1.641284966468811\n",
      "Batch 255,  loss: 1.500956130027771\n",
      "Batch 260,  loss: 1.9328827857971191\n",
      "Batch 265,  loss: 1.460937213897705\n",
      "Batch 270,  loss: 1.8769678592681884\n",
      "Batch 275,  loss: 1.2666717767715454\n",
      "Batch 280,  loss: 1.2587064266204835\n",
      "Batch 285,  loss: 1.7861268281936646\n",
      "Batch 290,  loss: 1.6550259351730348\n",
      "Batch 295,  loss: 1.7520256996154786\n",
      "Batch 300,  loss: 1.2599339723587035\n",
      "Batch 305,  loss: 1.6385465502738952\n",
      "Batch 310,  loss: 1.877635645866394\n",
      "Batch 315,  loss: 1.4662879705429077\n",
      "Batch 320,  loss: 1.403858470916748\n",
      "Batch 325,  loss: 1.3492581367492675\n",
      "Batch 330,  loss: 1.7115757942199707\n",
      "Batch 335,  loss: 1.2035231113433837\n",
      "Batch 340,  loss: 1.590711736679077\n",
      "Batch 345,  loss: 1.2959596157073974\n",
      "Batch 350,  loss: 1.5077160835266112\n",
      "Batch 355,  loss: 1.6187466144561768\n",
      "Batch 360,  loss: 1.70640869140625\n",
      "Batch 365,  loss: 1.5349927186965941\n",
      "Batch 370,  loss: 1.881560206413269\n",
      "Batch 375,  loss: 1.4420213937759399\n",
      "Batch 380,  loss: 1.5154542565345763\n",
      "Batch 385,  loss: 1.395507001876831\n",
      "Batch 390,  loss: 1.2948565840721131\n",
      "Batch 395,  loss: 2.0477221727371218\n",
      "Batch 400,  loss: 1.6711925268173218\n",
      "Batch 405,  loss: 1.893556499481201\n",
      "Batch 410,  loss: 1.510978627204895\n",
      "Batch 415,  loss: 1.9579200744628906\n",
      "Batch 420,  loss: 1.4844781637191773\n",
      "Batch 425,  loss: 1.598751425743103\n",
      "Batch 430,  loss: 1.6690263032913208\n",
      "Batch 435,  loss: 1.343186378479004\n",
      "Batch 440,  loss: 1.8286338567733764\n",
      "Batch 445,  loss: 1.8822583913803101\n",
      "Batch 450,  loss: 1.4853687524795531\n",
      "Batch 455,  loss: 1.2079275846481323\n",
      "Batch 460,  loss: 1.5374007940292358\n",
      "Batch 465,  loss: 1.826528549194336\n",
      "Batch 470,  loss: 1.5354610919952392\n",
      "Batch 475,  loss: 1.4529366970062256\n",
      "Batch 480,  loss: 1.5493704319000243\n",
      "Batch 485,  loss: 1.536926507949829\n",
      "Batch 490,  loss: 1.5122388839721679\n",
      "Batch 495,  loss: 1.9238795518875123\n",
      "Batch 500,  loss: 1.5157037258148194\n",
      "Batch 505,  loss: 1.3104744672775268\n",
      "Batch 510,  loss: 1.6852926731109619\n",
      "Batch 515,  loss: 1.6344994068145753\n",
      "Batch 520,  loss: 1.2750078916549683\n",
      "Batch 525,  loss: 1.6100044965744018\n",
      "Batch 530,  loss: 1.5399266719818114\n",
      "Batch 535,  loss: 1.5723742723464966\n",
      "Batch 540,  loss: 1.645429754257202\n",
      "Batch 545,  loss: 1.2103017330169679\n",
      "Batch 550,  loss: 1.6508108377456665\n",
      "Batch 555,  loss: 1.7895970582962035\n",
      "Batch 560,  loss: 1.6356306552886963\n",
      "Batch 565,  loss: 1.6967478752136231\n",
      "Batch 570,  loss: 1.655088758468628\n",
      "Batch 575,  loss: 1.4666269063949584\n",
      "Batch 580,  loss: 1.5058717489242555\n",
      "Batch 585,  loss: 1.947779130935669\n",
      "Batch 590,  loss: 1.3424823999404907\n",
      "Batch 595,  loss: 1.2325569152832032\n",
      "Batch 600,  loss: 1.5080920457839966\n",
      "Batch 605,  loss: 1.5691935896873475\n",
      "Batch 610,  loss: 1.259862756729126\n",
      "Batch 615,  loss: 1.8539881944656371\n",
      "Batch 620,  loss: 1.71090829372406\n",
      "Batch 625,  loss: 1.3349576354026795\n",
      "Batch 630,  loss: 1.418686032295227\n",
      "Batch 635,  loss: 1.318117380142212\n",
      "Batch 640,  loss: 1.1639349102973937\n",
      "Batch 645,  loss: 1.6991530418395997\n",
      "Batch 650,  loss: 1.5203553915023804\n",
      "Batch 655,  loss: 1.4082489728927612\n",
      "Batch 660,  loss: 1.5161290645599366\n",
      "Batch 665,  loss: 1.5322350025177003\n",
      "Batch 670,  loss: 1.4690027952194213\n",
      "Batch 675,  loss: 1.7337674140930175\n",
      "Batch 680,  loss: 1.6346137523651123\n",
      "Batch 685,  loss: 1.5303333520889282\n",
      "Batch 690,  loss: 1.4059990406036378\n",
      "Batch 695,  loss: 1.7721887111663819\n",
      "Batch 700,  loss: 1.8862462997436524\n",
      "Batch 705,  loss: 1.3595901250839233\n",
      "Batch 710,  loss: 1.6372243881225585\n",
      "Batch 715,  loss: 1.40968976020813\n",
      "Batch 720,  loss: 1.737805223464966\n",
      "Batch 725,  loss: 1.446556067466736\n",
      "Batch 730,  loss: 1.7322336196899415\n",
      "Batch 735,  loss: 1.3709705829620362\n",
      "Batch 740,  loss: 1.4868635654449462\n",
      "Batch 745,  loss: 1.5647799730300904\n",
      "Batch 750,  loss: 1.383040761947632\n",
      "Batch 755,  loss: 1.742337965965271\n",
      "Batch 760,  loss: 1.6759827613830567\n",
      "Batch 765,  loss: 1.4430338382720946\n",
      "Batch 770,  loss: 1.4268219709396361\n",
      "Batch 775,  loss: 1.3708439469337463\n",
      "Batch 780,  loss: 1.642199730873108\n",
      "Batch 785,  loss: 1.536142635345459\n",
      "Batch 790,  loss: 1.2404779195785522\n",
      "Batch 795,  loss: 1.5624198198318482\n",
      "Batch 800,  loss: 1.5335409879684447\n",
      "Batch 805,  loss: 1.4991518974304199\n",
      "Batch 810,  loss: 1.4997803926467896\n",
      "Batch 815,  loss: 1.4965160131454467\n",
      "Batch 820,  loss: 1.3531337261199952\n",
      "Batch 825,  loss: 1.394216239452362\n",
      "Batch 830,  loss: 1.513249111175537\n",
      "Batch 835,  loss: 1.653910756111145\n",
      "Batch 840,  loss: 1.6565967082977295\n",
      "Batch 845,  loss: 1.9397650480270385\n",
      "Batch 850,  loss: 1.6073545217514038\n",
      "Batch 855,  loss: 1.9898519277572633\n",
      "Batch 860,  loss: 1.8161952257156373\n",
      "Batch 865,  loss: 1.678673768043518\n",
      "Batch 870,  loss: 1.923731279373169\n",
      "Batch 875,  loss: 1.4404353857040406\n",
      "Batch 880,  loss: 1.8374861001968383\n",
      "Batch 885,  loss: 1.1869957447052002\n",
      "Batch 890,  loss: 1.407682979106903\n",
      "Batch 895,  loss: 1.6377588510513306\n",
      "Batch 900,  loss: 1.4508923530578612\n",
      "Batch 905,  loss: 1.5141242265701294\n",
      "Batch 910,  loss: 1.6474578142166139\n",
      "Batch 915,  loss: 1.2311667919158935\n",
      "Batch 920,  loss: 1.6484440326690675\n",
      "Batch 925,  loss: 1.5406930208206178\n",
      "Batch 930,  loss: 1.843094253540039\n",
      "Batch 935,  loss: 1.5000266313552857\n",
      "Batch 940,  loss: 1.5969956159591674\n",
      "Batch 945,  loss: 1.6361869335174561\n",
      "Batch 950,  loss: 1.8107011318206787\n",
      "Batch 955,  loss: 1.408585751056671\n",
      "Batch 960,  loss: 1.2800137042999267\n",
      "Batch 965,  loss: 1.31203852891922\n",
      "Batch 970,  loss: 2.2676676750183105\n",
      "Batch 975,  loss: 1.3563584566116333\n",
      "Batch 980,  loss: 1.3721212506294251\n",
      "Batch 985,  loss: 1.4967157125473023\n",
      "Batch 990,  loss: 1.8026354789733887\n",
      "Batch 995,  loss: 1.6451353549957275\n",
      "Batch 1000,  loss: 1.6270864248275756\n",
      "Batch 1005,  loss: 1.5348978996276856\n",
      "Batch 1010,  loss: 1.4164693832397461\n",
      "Batch 1015,  loss: 1.5545564413070678\n",
      "Batch 1020,  loss: 1.5046092510223388\n",
      "Batch 1025,  loss: 1.6458372235298158\n",
      "Batch 1030,  loss: 1.440159296989441\n",
      "Batch 1035,  loss: 1.4103015184402465\n",
      "Batch 1040,  loss: 1.6120830059051514\n",
      "Batch 1045,  loss: 1.5663093328475952\n",
      "Batch 1050,  loss: 1.6036332368850708\n",
      "Batch 1055,  loss: 1.4890031814575195\n",
      "Batch 1060,  loss: 1.6452970504760742\n",
      "Batch 1065,  loss: 1.6720185041427613\n",
      "Batch 1070,  loss: 1.440191888809204\n",
      "Batch 1075,  loss: 1.4397483348846436\n",
      "Batch 1080,  loss: 1.5348412275314331\n",
      "Batch 1085,  loss: 1.2440319061279297\n",
      "Batch 1090,  loss: 1.5424963235855103\n",
      "Batch 1095,  loss: 1.5107307434082031\n",
      "Batch 1100,  loss: 1.725841999053955\n",
      "Batch 1105,  loss: 1.7732424974441527\n",
      "Batch 1110,  loss: 1.4943429470062255\n",
      "Batch 1115,  loss: 1.2331251978874207\n",
      "Batch 1120,  loss: 1.5180444359779357\n",
      "Batch 1125,  loss: 1.5546528577804566\n",
      "Batch 1130,  loss: 2.1099613189697264\n",
      "Batch 1135,  loss: 1.7534951210021972\n",
      "Batch 1140,  loss: 1.8145599365234375\n",
      "Batch 1145,  loss: 1.3360275745391845\n",
      "Batch 1150,  loss: 1.671116828918457\n",
      "Batch 1155,  loss: 1.3378084659576417\n",
      "Batch 1160,  loss: 1.4974212884902953\n",
      "Batch 1165,  loss: 1.5698616981506348\n",
      "Batch 1170,  loss: 1.701730227470398\n",
      "Batch 1175,  loss: 1.5717666625976563\n",
      "Batch 1180,  loss: 1.7676549911499024\n",
      "Batch 1185,  loss: 1.5675809860229493\n",
      "Batch 1190,  loss: 1.4074884295463561\n",
      "Batch 1195,  loss: 1.3784632325172423\n",
      "Batch 1200,  loss: 1.3949698567390443\n",
      "Batch 1205,  loss: 1.5547725915908814\n",
      "Batch 1210,  loss: 1.34727623462677\n",
      "Batch 1215,  loss: 1.888461446762085\n",
      "Batch 1220,  loss: 1.4899226903915406\n",
      "Batch 1225,  loss: 1.915842056274414\n",
      "Batch 1230,  loss: 1.8024761199951171\n",
      "Batch 1235,  loss: 1.4739981651306153\n",
      "Batch 1240,  loss: 1.3325464248657226\n",
      "Batch 1245,  loss: 1.6869847536087037\n",
      "Batch 1250,  loss: 1.5044281721115111\n",
      "Batch 1255,  loss: 1.7671790361404418\n",
      "Batch 1260,  loss: 1.6047261476516723\n",
      "Batch 1265,  loss: 1.2872972249984742\n",
      "Batch 1270,  loss: 1.5945095777511598\n",
      "Batch 1275,  loss: 1.57210373878479\n",
      "Batch 1280,  loss: 1.8479908943176269\n",
      "Batch 1285,  loss: 1.974292516708374\n",
      "Batch 1290,  loss: 1.4039698362350463\n",
      "Batch 1295,  loss: 1.6282935380935668\n",
      "LOSS train 1.6282935380935668. Validation loss: 2.117481427363775 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 35:\n",
      "Batch 5,  loss: 1.6464761734008788\n",
      "Batch 10,  loss: 1.6330686807632446\n",
      "Batch 15,  loss: 1.50195631980896\n",
      "Batch 20,  loss: 1.6102246999740601\n",
      "Batch 25,  loss: 1.4360382080078125\n",
      "Batch 30,  loss: 1.7446950912475585\n",
      "Batch 35,  loss: 1.36414475440979\n",
      "Batch 40,  loss: 1.977517580986023\n",
      "Batch 45,  loss: 1.5281654834747314\n",
      "Batch 50,  loss: 1.5127782106399537\n",
      "Batch 55,  loss: 1.4716143608093262\n",
      "Batch 60,  loss: 1.752137041091919\n",
      "Batch 65,  loss: 1.1585200786590577\n",
      "Batch 70,  loss: 1.582339882850647\n",
      "Batch 75,  loss: 1.6215118408203124\n",
      "Batch 80,  loss: 1.4420809030532837\n",
      "Batch 85,  loss: 1.509504222869873\n",
      "Batch 90,  loss: 1.3725337982177734\n",
      "Batch 95,  loss: 1.5095422267913818\n",
      "Batch 100,  loss: 1.745743465423584\n",
      "Batch 105,  loss: 1.6595901012420655\n",
      "Batch 110,  loss: 1.9725011825561523\n",
      "Batch 115,  loss: 1.558229660987854\n",
      "Batch 120,  loss: 1.5968087911605835\n",
      "Batch 125,  loss: 1.5920657157897948\n",
      "Batch 130,  loss: 1.4597657203674317\n",
      "Batch 135,  loss: 1.584535503387451\n",
      "Batch 140,  loss: 1.4805466651916503\n",
      "Batch 145,  loss: 1.8206137895584107\n",
      "Batch 150,  loss: 1.6016722917556763\n",
      "Batch 155,  loss: 1.654318594932556\n",
      "Batch 160,  loss: 1.322001576423645\n",
      "Batch 165,  loss: 1.6228118419647217\n",
      "Batch 170,  loss: 1.6001362323760986\n",
      "Batch 175,  loss: 1.5000054121017456\n",
      "Batch 180,  loss: 2.1244038343429565\n",
      "Batch 185,  loss: 1.4521924257278442\n",
      "Batch 190,  loss: 1.4603330850601197\n",
      "Batch 195,  loss: 1.4513920068740844\n",
      "Batch 200,  loss: 1.517336916923523\n",
      "Batch 205,  loss: 1.579465663433075\n",
      "Batch 210,  loss: 1.2582665085792542\n",
      "Batch 215,  loss: 1.5546202659606934\n",
      "Batch 220,  loss: 1.6010636329650878\n",
      "Batch 225,  loss: 1.3444321155548096\n",
      "Batch 230,  loss: 1.9445587158203126\n",
      "Batch 235,  loss: 1.632364773750305\n",
      "Batch 240,  loss: 1.9558512210845946\n",
      "Batch 245,  loss: 1.6822454452514648\n",
      "Batch 250,  loss: 1.3424628257751465\n",
      "Batch 255,  loss: 1.624300742149353\n",
      "Batch 260,  loss: 1.5450428962707519\n",
      "Batch 265,  loss: 1.4779494047164916\n",
      "Batch 270,  loss: 1.478031873703003\n",
      "Batch 275,  loss: 1.5036271333694458\n",
      "Batch 280,  loss: 1.4295738697052003\n",
      "Batch 285,  loss: 1.4454188823699952\n",
      "Batch 290,  loss: 1.1888107657432556\n",
      "Batch 295,  loss: 2.058683693408966\n",
      "Batch 300,  loss: 1.999234676361084\n",
      "Batch 305,  loss: 1.142467486858368\n",
      "Batch 310,  loss: 1.356316041946411\n",
      "Batch 315,  loss: 1.78369243144989\n",
      "Batch 320,  loss: 1.1990954637527467\n",
      "Batch 325,  loss: 1.7103201389312743\n",
      "Batch 330,  loss: 1.7008768558502196\n",
      "Batch 335,  loss: 1.9384311437606812\n",
      "Batch 340,  loss: 1.397583794593811\n",
      "Batch 345,  loss: 1.6189074039459228\n",
      "Batch 350,  loss: 1.6142367601394654\n",
      "Batch 355,  loss: 1.4827811241149902\n",
      "Batch 360,  loss: 1.595759916305542\n",
      "Batch 365,  loss: 1.599959659576416\n",
      "Batch 370,  loss: 1.4522947549819947\n",
      "Batch 375,  loss: 1.5283354759216308\n",
      "Batch 380,  loss: 1.7212878465652466\n",
      "Batch 385,  loss: 1.4396996736526488\n",
      "Batch 390,  loss: 1.4278815507888794\n",
      "Batch 395,  loss: 1.3539897203445435\n",
      "Batch 400,  loss: 1.3852360725402832\n",
      "Batch 405,  loss: 1.4407188653945924\n",
      "Batch 410,  loss: 1.8239965915679932\n",
      "Batch 415,  loss: 1.8277817010879516\n",
      "Batch 420,  loss: 1.7030029296875\n",
      "Batch 425,  loss: 1.4966323852539063\n",
      "Batch 430,  loss: 1.3420582294464112\n",
      "Batch 435,  loss: 1.562331199645996\n",
      "Batch 440,  loss: 1.460386538505554\n",
      "Batch 445,  loss: 1.6996338844299317\n",
      "Batch 450,  loss: 1.2378533363342286\n",
      "Batch 455,  loss: 1.635405969619751\n",
      "Batch 460,  loss: 1.4783441543579101\n",
      "Batch 465,  loss: 1.7303433179855348\n",
      "Batch 470,  loss: 1.5723320722579956\n",
      "Batch 475,  loss: 1.5150681614875794\n",
      "Batch 480,  loss: 1.7302805423736571\n",
      "Batch 485,  loss: 1.4831629276275635\n",
      "Batch 490,  loss: 1.3547333240509034\n",
      "Batch 495,  loss: 1.573096013069153\n",
      "Batch 500,  loss: 2.029907751083374\n",
      "Batch 505,  loss: 1.616322898864746\n",
      "Batch 510,  loss: 1.2131286144256592\n",
      "Batch 515,  loss: 1.528778600692749\n",
      "Batch 520,  loss: 1.3090263485908509\n",
      "Batch 525,  loss: 1.4859554767608643\n",
      "Batch 530,  loss: 1.777260684967041\n",
      "Batch 535,  loss: 2.236932873725891\n",
      "Batch 540,  loss: 2.3635783433914184\n",
      "Batch 545,  loss: 1.68312509059906\n",
      "Batch 550,  loss: 1.3893252849578857\n",
      "Batch 555,  loss: 1.3671405076980592\n",
      "Batch 560,  loss: 1.4661182403564452\n",
      "Batch 565,  loss: 1.4127303838729859\n",
      "Batch 570,  loss: 1.4008180141448974\n",
      "Batch 575,  loss: 1.593215250968933\n",
      "Batch 580,  loss: 1.26560320854187\n",
      "Batch 585,  loss: 1.6331477642059327\n",
      "Batch 590,  loss: 1.5974603414535522\n",
      "Batch 595,  loss: 1.3595670580863952\n",
      "Batch 600,  loss: 1.4034585237503052\n",
      "Batch 605,  loss: 1.6447896003723144\n",
      "Batch 610,  loss: 1.4296895265579224\n",
      "Batch 615,  loss: 1.1555071353912354\n",
      "Batch 620,  loss: 0.9700069189071655\n",
      "Batch 625,  loss: 1.9700313091278077\n",
      "Batch 630,  loss: 1.491041612625122\n",
      "Batch 635,  loss: 1.9115369081497193\n",
      "Batch 640,  loss: 1.438847303390503\n",
      "Batch 645,  loss: 1.5008344650268555\n",
      "Batch 650,  loss: 1.929215145111084\n",
      "Batch 655,  loss: 1.4943397045135498\n",
      "Batch 660,  loss: 1.7713796138763427\n",
      "Batch 665,  loss: 1.3205851554870605\n",
      "Batch 670,  loss: 1.7208609580993652\n",
      "Batch 675,  loss: 1.3830814838409424\n",
      "Batch 680,  loss: 1.6646901845932007\n",
      "Batch 685,  loss: 1.440298891067505\n",
      "Batch 690,  loss: 1.095520293712616\n",
      "Batch 695,  loss: 1.564669418334961\n",
      "Batch 700,  loss: 1.3062839388847352\n",
      "Batch 705,  loss: 1.5363141775131226\n",
      "Batch 710,  loss: 1.2642289876937867\n",
      "Batch 715,  loss: 1.5600417137145997\n",
      "Batch 720,  loss: 1.4344545602798462\n",
      "Batch 725,  loss: 1.6697226285934448\n",
      "Batch 730,  loss: 1.4745527982711792\n",
      "Batch 735,  loss: 1.9741838455200196\n",
      "Batch 740,  loss: 1.5854088544845581\n",
      "Batch 745,  loss: 1.5998768091201783\n",
      "Batch 750,  loss: 1.818448042869568\n",
      "Batch 755,  loss: 1.4788774728775025\n",
      "Batch 760,  loss: 1.3315444707870483\n",
      "Batch 765,  loss: 1.45139878988266\n",
      "Batch 770,  loss: 1.5952305793762207\n",
      "Batch 775,  loss: 1.4837435245513917\n",
      "Batch 780,  loss: 1.5352152585983276\n",
      "Batch 785,  loss: 1.44091295003891\n",
      "Batch 790,  loss: 1.8307812213897705\n",
      "Batch 795,  loss: 1.3637369871139526\n",
      "Batch 800,  loss: 1.4889618635177613\n",
      "Batch 805,  loss: 1.5753213882446289\n",
      "Batch 810,  loss: 1.4854559659957887\n",
      "Batch 815,  loss: 1.458100640773773\n",
      "Batch 820,  loss: 1.420920467376709\n",
      "Batch 825,  loss: 1.6403851509094238\n",
      "Batch 830,  loss: 1.8498921871185303\n",
      "Batch 835,  loss: 1.2900233507156371\n",
      "Batch 840,  loss: 1.4777015686035155\n",
      "Batch 845,  loss: 1.850783920288086\n",
      "Batch 850,  loss: 1.3472227215766908\n",
      "Batch 855,  loss: 1.5856528997421264\n",
      "Batch 860,  loss: 1.5893197536468506\n",
      "Batch 865,  loss: 1.411072325706482\n",
      "Batch 870,  loss: 1.7057604312896728\n",
      "Batch 875,  loss: 1.5709449768066406\n",
      "Batch 880,  loss: 1.3625226616859436\n",
      "Batch 885,  loss: 1.468614435195923\n",
      "Batch 890,  loss: 1.4925459623336792\n",
      "Batch 895,  loss: 1.6906202793121339\n",
      "Batch 900,  loss: 1.5138369083404541\n",
      "Batch 905,  loss: 1.573830485343933\n",
      "Batch 910,  loss: 1.6788630485534668\n",
      "Batch 915,  loss: 1.653895378112793\n",
      "Batch 920,  loss: 1.5260998487472535\n",
      "Batch 925,  loss: 1.351033878326416\n",
      "Batch 930,  loss: 1.4532701969146729\n",
      "Batch 935,  loss: 1.8854776859283446\n",
      "Batch 940,  loss: 1.7860931158065796\n",
      "Batch 945,  loss: 1.4584678649902343\n",
      "Batch 950,  loss: 1.4501861333847046\n",
      "Batch 955,  loss: 1.3713483333587646\n",
      "Batch 960,  loss: 1.3044766187667847\n",
      "Batch 965,  loss: 1.6970219612121582\n",
      "Batch 970,  loss: 1.2879964590072632\n",
      "Batch 975,  loss: 1.722132921218872\n",
      "Batch 980,  loss: 1.3907956600189209\n",
      "Batch 985,  loss: 1.4629862308502197\n",
      "Batch 990,  loss: 1.4523814797401429\n",
      "Batch 995,  loss: 1.383808422088623\n",
      "Batch 1000,  loss: 1.9445461273193358\n",
      "Batch 1005,  loss: 1.5338604688644408\n",
      "Batch 1010,  loss: 1.7461615324020385\n",
      "Batch 1015,  loss: 1.455486035346985\n",
      "Batch 1020,  loss: 1.4422569751739502\n",
      "Batch 1025,  loss: 1.6257148027420043\n",
      "Batch 1030,  loss: 1.3127667903900146\n",
      "Batch 1035,  loss: 1.4692867040634154\n",
      "Batch 1040,  loss: 1.189042091369629\n",
      "Batch 1045,  loss: 1.7957521677017212\n",
      "Batch 1050,  loss: 1.561586618423462\n",
      "Batch 1055,  loss: 1.2717170715332031\n",
      "Batch 1060,  loss: 1.5059174299240112\n",
      "Batch 1065,  loss: 1.4604568958282471\n",
      "Batch 1070,  loss: 1.2226983666419984\n",
      "Batch 1075,  loss: 1.671533203125\n",
      "Batch 1080,  loss: 1.5861226797103882\n",
      "Batch 1085,  loss: 1.3861664533615112\n",
      "Batch 1090,  loss: 1.7496775388717651\n",
      "Batch 1095,  loss: 1.8284881591796875\n",
      "Batch 1100,  loss: 1.7808921098709107\n",
      "Batch 1105,  loss: 1.5733580589294434\n",
      "Batch 1110,  loss: 1.4969787120819091\n",
      "Batch 1115,  loss: 1.8917986154556274\n",
      "Batch 1120,  loss: 1.6365259408950805\n",
      "Batch 1125,  loss: 1.3752070188522338\n",
      "Batch 1130,  loss: 1.3642483472824096\n",
      "Batch 1135,  loss: 1.4627440929412843\n",
      "Batch 1140,  loss: 1.4639649868011475\n",
      "Batch 1145,  loss: 1.7340955972671508\n",
      "Batch 1150,  loss: 1.5638365983963012\n",
      "Batch 1155,  loss: 1.3510324239730835\n",
      "Batch 1160,  loss: 1.279997134208679\n",
      "Batch 1165,  loss: 1.6978421449661254\n",
      "Batch 1170,  loss: 1.7827717781066894\n",
      "Batch 1175,  loss: 1.5368952989578246\n",
      "Batch 1180,  loss: 1.5717342138290404\n",
      "Batch 1185,  loss: 1.45265953540802\n",
      "Batch 1190,  loss: 1.6180932998657227\n",
      "Batch 1195,  loss: 1.5831264972686767\n",
      "Batch 1200,  loss: 1.5778601169586182\n",
      "Batch 1205,  loss: 1.2931729078292846\n",
      "Batch 1210,  loss: 1.4592723846435547\n",
      "Batch 1215,  loss: 1.5296324014663696\n",
      "Batch 1220,  loss: 1.6174307584762573\n",
      "Batch 1225,  loss: 1.4992339372634889\n",
      "Batch 1230,  loss: 1.3944783449172973\n",
      "Batch 1235,  loss: 1.5088818550109864\n",
      "Batch 1240,  loss: 1.4500732660293578\n",
      "Batch 1245,  loss: 1.40053288936615\n",
      "Batch 1250,  loss: 1.5967119693756104\n",
      "Batch 1255,  loss: 1.7193974494934081\n",
      "Batch 1260,  loss: 1.583683431148529\n",
      "Batch 1265,  loss: 1.6233400583267212\n",
      "Batch 1270,  loss: 1.4176012754440308\n",
      "Batch 1275,  loss: 1.5867844104766846\n",
      "Batch 1280,  loss: 1.2428243994712829\n",
      "Batch 1285,  loss: 1.6882382154464721\n",
      "Batch 1290,  loss: 1.8201467037200927\n",
      "Batch 1295,  loss: 1.336191964149475\n",
      "LOSS train 1.336191964149475. Validation loss: 1.9697406712919474 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 36:\n",
      "Batch 5,  loss: 1.2204599976539612\n",
      "Batch 10,  loss: 1.27596595287323\n",
      "Batch 15,  loss: 1.4099018096923828\n",
      "Batch 20,  loss: 1.511506986618042\n",
      "Batch 25,  loss: 1.445322370529175\n",
      "Batch 30,  loss: 1.464145827293396\n",
      "Batch 35,  loss: 1.718494987487793\n",
      "Batch 40,  loss: 1.624095606803894\n",
      "Batch 45,  loss: 1.5376813650131225\n",
      "Batch 50,  loss: 1.638173508644104\n",
      "Batch 55,  loss: 1.7498610496520997\n",
      "Batch 60,  loss: 1.47419114112854\n",
      "Batch 65,  loss: 1.353460168838501\n",
      "Batch 70,  loss: 1.5241634607315064\n",
      "Batch 75,  loss: 2.1680333852767943\n",
      "Batch 80,  loss: 1.4348661422729492\n",
      "Batch 85,  loss: 1.4033597707748413\n",
      "Batch 90,  loss: 1.5831909894943237\n",
      "Batch 95,  loss: 1.1445804953575134\n",
      "Batch 100,  loss: 1.613688850402832\n",
      "Batch 105,  loss: 1.2995951533317567\n",
      "Batch 110,  loss: 1.7809183835983275\n",
      "Batch 115,  loss: 1.4163663506507873\n",
      "Batch 120,  loss: 1.8015657901763915\n",
      "Batch 125,  loss: 1.2819927334785461\n",
      "Batch 130,  loss: 1.5690210819244386\n",
      "Batch 135,  loss: 1.3779189586639404\n",
      "Batch 140,  loss: 1.5403841018676758\n",
      "Batch 145,  loss: 1.361955165863037\n",
      "Batch 150,  loss: 1.3795436143875122\n",
      "Batch 155,  loss: 1.5412413597106933\n",
      "Batch 160,  loss: 1.2301440834999084\n",
      "Batch 165,  loss: 1.6460339307785035\n",
      "Batch 170,  loss: 1.4339178085327149\n",
      "Batch 175,  loss: 1.216647744178772\n",
      "Batch 180,  loss: 1.5533435821533204\n",
      "Batch 185,  loss: 1.6888903617858886\n",
      "Batch 190,  loss: 1.630174732208252\n",
      "Batch 195,  loss: 1.7294827222824096\n",
      "Batch 200,  loss: 2.1400208711624145\n",
      "Batch 205,  loss: 1.7019042015075683\n",
      "Batch 210,  loss: 1.536312174797058\n",
      "Batch 215,  loss: 1.5162046432495118\n",
      "Batch 220,  loss: 1.1407848358154298\n",
      "Batch 225,  loss: 1.3807533979415894\n",
      "Batch 230,  loss: 1.6693896055221558\n",
      "Batch 235,  loss: 1.5286476850509643\n",
      "Batch 240,  loss: 1.285492968559265\n",
      "Batch 245,  loss: 1.4573948383331299\n",
      "Batch 250,  loss: 1.6901054382324219\n",
      "Batch 255,  loss: 1.3652682781219483\n",
      "Batch 260,  loss: 1.6056084632873535\n",
      "Batch 265,  loss: 1.342376947402954\n",
      "Batch 270,  loss: 1.2869123101234436\n",
      "Batch 275,  loss: 1.5404695987701416\n",
      "Batch 280,  loss: 1.718449306488037\n",
      "Batch 285,  loss: 1.4321250200271607\n",
      "Batch 290,  loss: 1.5667722463607787\n",
      "Batch 295,  loss: 1.5410927772521972\n",
      "Batch 300,  loss: 1.2636339902877807\n",
      "Batch 305,  loss: 1.4242012262344361\n",
      "Batch 310,  loss: 1.7915090084075929\n",
      "Batch 315,  loss: 1.8027924299240112\n",
      "Batch 320,  loss: 1.4948296308517457\n",
      "Batch 325,  loss: 1.5516775608062745\n",
      "Batch 330,  loss: 1.703449511528015\n",
      "Batch 335,  loss: 1.3682114362716675\n",
      "Batch 340,  loss: 1.4634329199790954\n",
      "Batch 345,  loss: 1.5580102920532226\n",
      "Batch 350,  loss: 1.5241789817810059\n",
      "Batch 355,  loss: 1.548004412651062\n",
      "Batch 360,  loss: 1.4290498495101929\n",
      "Batch 365,  loss: 1.505750060081482\n",
      "Batch 370,  loss: 1.9331735372543335\n",
      "Batch 375,  loss: 1.824521040916443\n",
      "Batch 380,  loss: 1.389104151725769\n",
      "Batch 385,  loss: 1.744569516181946\n",
      "Batch 390,  loss: 1.4431073546409607\n",
      "Batch 395,  loss: 2.02744300365448\n",
      "Batch 400,  loss: 1.5283554792404175\n",
      "Batch 405,  loss: 1.6891319036483765\n",
      "Batch 410,  loss: 1.6405319452285767\n",
      "Batch 415,  loss: 1.5516380310058593\n",
      "Batch 420,  loss: 1.582346296310425\n",
      "Batch 425,  loss: 1.9359834432601928\n",
      "Batch 430,  loss: 1.386046552658081\n",
      "Batch 435,  loss: 1.769069242477417\n",
      "Batch 440,  loss: 1.519948124885559\n",
      "Batch 445,  loss: 1.6840147495269775\n",
      "Batch 450,  loss: 1.468844962120056\n",
      "Batch 455,  loss: 1.5393478393554687\n",
      "Batch 460,  loss: 1.230112361907959\n",
      "Batch 465,  loss: 1.5103820323944093\n",
      "Batch 470,  loss: 1.645757293701172\n",
      "Batch 475,  loss: 1.6935600638389587\n",
      "Batch 480,  loss: 1.5883301496505737\n",
      "Batch 485,  loss: 2.011714553833008\n",
      "Batch 490,  loss: 1.3259750485420227\n",
      "Batch 495,  loss: 1.9130110025405884\n",
      "Batch 500,  loss: 1.4958309412002564\n",
      "Batch 505,  loss: 1.2942941427230834\n",
      "Batch 510,  loss: 1.3112557888031007\n",
      "Batch 515,  loss: 1.513720202445984\n",
      "Batch 520,  loss: 1.566315197944641\n",
      "Batch 525,  loss: 1.3785285234451294\n",
      "Batch 530,  loss: 1.4325503349304198\n",
      "Batch 535,  loss: 1.4425883531570434\n",
      "Batch 540,  loss: 1.457432174682617\n",
      "Batch 545,  loss: 2.0054337978363037\n",
      "Batch 550,  loss: 1.5466031789779664\n",
      "Batch 555,  loss: 1.751569104194641\n",
      "Batch 560,  loss: 1.3467649459838866\n",
      "Batch 565,  loss: 1.30378897190094\n",
      "Batch 570,  loss: 1.4330557107925415\n",
      "Batch 575,  loss: 1.341461753845215\n",
      "Batch 580,  loss: 1.5717117547988892\n",
      "Batch 585,  loss: 1.7617399215698242\n",
      "Batch 590,  loss: 1.3712846755981445\n",
      "Batch 595,  loss: 1.4513197898864747\n",
      "Batch 600,  loss: 1.3298304557800293\n",
      "Batch 605,  loss: 1.4088626623153686\n",
      "Batch 610,  loss: 1.4204582214355468\n",
      "Batch 615,  loss: 1.3280557632446288\n",
      "Batch 620,  loss: 1.6349211931228638\n",
      "Batch 625,  loss: 1.575626015663147\n",
      "Batch 630,  loss: 1.7433764457702636\n",
      "Batch 635,  loss: 1.4414270401000977\n",
      "Batch 640,  loss: 1.304073977470398\n",
      "Batch 645,  loss: 1.5922110319137572\n",
      "Batch 650,  loss: 1.4811692237854004\n",
      "Batch 655,  loss: 1.2924001216888428\n",
      "Batch 660,  loss: 1.534732723236084\n",
      "Batch 665,  loss: 1.5458892822265624\n",
      "Batch 670,  loss: 1.2378951549530028\n",
      "Batch 675,  loss: 1.632595944404602\n",
      "Batch 680,  loss: 1.3995121955871581\n",
      "Batch 685,  loss: 1.460490918159485\n",
      "Batch 690,  loss: 1.503025484085083\n",
      "Batch 695,  loss: 1.3846741437911987\n",
      "Batch 700,  loss: 1.685239315032959\n",
      "Batch 705,  loss: 1.3451719164848328\n",
      "Batch 710,  loss: 1.439630913734436\n",
      "Batch 715,  loss: 2.1702795028686523\n",
      "Batch 720,  loss: 1.5972471475601195\n",
      "Batch 725,  loss: 1.418003749847412\n",
      "Batch 730,  loss: 1.9261781215667724\n",
      "Batch 735,  loss: 1.6664828777313232\n",
      "Batch 740,  loss: 1.481278157234192\n",
      "Batch 745,  loss: 1.6312321186065675\n",
      "Batch 750,  loss: 1.6117554187774659\n",
      "Batch 755,  loss: 1.5631643772125243\n",
      "Batch 760,  loss: 1.687861943244934\n",
      "Batch 765,  loss: 1.481294536590576\n",
      "Batch 770,  loss: 1.5815949440002441\n",
      "Batch 775,  loss: 1.4490362644195556\n",
      "Batch 780,  loss: 1.2076762437820434\n",
      "Batch 785,  loss: 1.5544960498809814\n",
      "Batch 790,  loss: 1.7778329610824586\n",
      "Batch 795,  loss: 1.476795780658722\n",
      "Batch 800,  loss: 1.3761683464050294\n",
      "Batch 805,  loss: 1.608028769493103\n",
      "Batch 810,  loss: 1.5403687953948975\n",
      "Batch 815,  loss: 1.1432831645011903\n",
      "Batch 820,  loss: 1.6782086372375489\n",
      "Batch 825,  loss: 1.3598976850509643\n",
      "Batch 830,  loss: 1.4859230995178223\n",
      "Batch 835,  loss: 1.7905540227890016\n",
      "Batch 840,  loss: 1.7321441531181336\n",
      "Batch 845,  loss: 1.4951626062393188\n",
      "Batch 850,  loss: 2.4140485286712647\n",
      "Batch 855,  loss: 1.594323420524597\n",
      "Batch 860,  loss: 1.3940231323242187\n",
      "Batch 865,  loss: 1.6711096048355103\n",
      "Batch 870,  loss: 1.500772976875305\n",
      "Batch 875,  loss: 1.1957792043685913\n",
      "Batch 880,  loss: 1.5175216436386108\n",
      "Batch 885,  loss: 1.4725661516189574\n",
      "Batch 890,  loss: 1.517575740814209\n",
      "Batch 895,  loss: 1.7690635919570923\n",
      "Batch 900,  loss: 1.526108407974243\n",
      "Batch 905,  loss: 1.696497631072998\n",
      "Batch 910,  loss: 1.3981817960739136\n",
      "Batch 915,  loss: 1.5902184009552003\n",
      "Batch 920,  loss: 1.3911848068237305\n",
      "Batch 925,  loss: 1.5503247737884522\n",
      "Batch 930,  loss: 1.4237202882766724\n",
      "Batch 935,  loss: 1.5477433443069457\n",
      "Batch 940,  loss: 1.3939257860183716\n",
      "Batch 945,  loss: 1.5874881029129029\n",
      "Batch 950,  loss: 1.1985991954803468\n",
      "Batch 955,  loss: 1.3503918170928955\n",
      "Batch 960,  loss: 1.764695167541504\n",
      "Batch 965,  loss: 1.7701420307159423\n",
      "Batch 970,  loss: 1.5226729393005372\n",
      "Batch 975,  loss: 1.2496492385864257\n",
      "Batch 980,  loss: 1.624050211906433\n",
      "Batch 985,  loss: 1.6871284008026124\n",
      "Batch 990,  loss: 1.5407057762145997\n",
      "Batch 995,  loss: 1.8123337984085084\n",
      "Batch 1000,  loss: 1.5152094602584838\n",
      "Batch 1005,  loss: 1.4825313091278076\n",
      "Batch 1010,  loss: 1.4583402395248413\n",
      "Batch 1015,  loss: 1.5179763793945313\n",
      "Batch 1020,  loss: 1.1849955677986146\n",
      "Batch 1025,  loss: 1.4709027528762817\n",
      "Batch 1030,  loss: 1.5323615312576293\n",
      "Batch 1035,  loss: 1.6339364051818848\n",
      "Batch 1040,  loss: 1.825145173072815\n",
      "Batch 1045,  loss: 1.7128726720809937\n",
      "Batch 1050,  loss: 1.3693809032440185\n",
      "Batch 1055,  loss: 1.9536678314208984\n",
      "Batch 1060,  loss: 1.2523980855941772\n",
      "Batch 1065,  loss: 1.3899598836898803\n",
      "Batch 1070,  loss: 1.7308698654174806\n",
      "Batch 1075,  loss: 1.7066458225250245\n",
      "Batch 1080,  loss: 1.4796918869018554\n",
      "Batch 1085,  loss: 1.4495195150375366\n",
      "Batch 1090,  loss: 1.2679425001144409\n",
      "Batch 1095,  loss: 1.4915466070175172\n",
      "Batch 1100,  loss: 1.603702449798584\n",
      "Batch 1105,  loss: 1.7750498533248902\n",
      "Batch 1110,  loss: 1.5095604181289672\n",
      "Batch 1115,  loss: 1.9099397659301758\n",
      "Batch 1120,  loss: 1.529080295562744\n",
      "Batch 1125,  loss: 1.2926254987716674\n",
      "Batch 1130,  loss: 1.8071303606033324\n",
      "Batch 1135,  loss: 1.6035765647888183\n",
      "Batch 1140,  loss: 1.4739235639572144\n",
      "Batch 1145,  loss: 1.061735737323761\n",
      "Batch 1150,  loss: 1.254516053199768\n",
      "Batch 1155,  loss: 1.275135087966919\n",
      "Batch 1160,  loss: 1.5711233139038085\n",
      "Batch 1165,  loss: 1.5724281549453736\n",
      "Batch 1170,  loss: 1.6644044637680053\n",
      "Batch 1175,  loss: 1.5519834518432618\n",
      "Batch 1180,  loss: 1.5447209358215332\n",
      "Batch 1185,  loss: 1.750591230392456\n",
      "Batch 1190,  loss: 1.2558334589004516\n",
      "Batch 1195,  loss: 1.8204413175582885\n",
      "Batch 1200,  loss: 1.3187128543853759\n",
      "Batch 1205,  loss: 1.8241067409515381\n",
      "Batch 1210,  loss: 1.6602041959762572\n",
      "Batch 1215,  loss: 1.5088117003440857\n",
      "Batch 1220,  loss: 1.2483540534973145\n",
      "Batch 1225,  loss: 1.4716115713119506\n",
      "Batch 1230,  loss: 2.0413401126861572\n",
      "Batch 1235,  loss: 1.7605047464370727\n",
      "Batch 1240,  loss: 1.771368932723999\n",
      "Batch 1245,  loss: 1.4999791622161864\n",
      "Batch 1250,  loss: 1.4761961698532104\n",
      "Batch 1255,  loss: 1.664605975151062\n",
      "Batch 1260,  loss: 1.737680149078369\n",
      "Batch 1265,  loss: 1.7801290273666381\n",
      "Batch 1270,  loss: 1.337230134010315\n",
      "Batch 1275,  loss: 1.2335904955863952\n",
      "Batch 1280,  loss: 1.5163527965545653\n",
      "Batch 1285,  loss: 1.8528237342834473\n",
      "Batch 1290,  loss: 1.5707791566848754\n",
      "Batch 1295,  loss: 1.6110737562179565\n",
      "LOSS train 1.6110737562179565. Validation loss: 2.0569722742118217 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 37:\n",
      "Batch 5,  loss: 1.478139078617096\n",
      "Batch 10,  loss: 1.2993391394615172\n",
      "Batch 15,  loss: 1.6880506992340087\n",
      "Batch 20,  loss: 1.594474768638611\n",
      "Batch 25,  loss: 1.6091111183166504\n",
      "Batch 30,  loss: 1.6172208070755005\n",
      "Batch 35,  loss: 1.4540523529052733\n",
      "Batch 40,  loss: 1.3210744619369508\n",
      "Batch 45,  loss: 1.6293178796768188\n",
      "Batch 50,  loss: 1.7323172569274903\n",
      "Batch 55,  loss: 1.6767780303955078\n",
      "Batch 60,  loss: 1.7885217666625977\n",
      "Batch 65,  loss: 1.725516128540039\n",
      "Batch 70,  loss: 1.4984407424926758\n",
      "Batch 75,  loss: 1.9486345291137694\n",
      "Batch 80,  loss: 1.5422712802886962\n",
      "Batch 85,  loss: 1.4641311883926391\n",
      "Batch 90,  loss: 1.8224114656448365\n",
      "Batch 95,  loss: 1.278084945678711\n",
      "Batch 100,  loss: 1.605884861946106\n",
      "Batch 105,  loss: 1.594740343093872\n",
      "Batch 110,  loss: 1.884463357925415\n",
      "Batch 115,  loss: 1.3823018789291381\n",
      "Batch 120,  loss: 1.378163492679596\n",
      "Batch 125,  loss: 1.5346363067626954\n",
      "Batch 130,  loss: 1.4580251455307007\n",
      "Batch 135,  loss: 1.5232809066772461\n",
      "Batch 140,  loss: 1.3562027931213378\n",
      "Batch 145,  loss: 1.5087908625602722\n",
      "Batch 150,  loss: 1.3941473245620728\n",
      "Batch 155,  loss: 1.2907723665237427\n",
      "Batch 160,  loss: 1.645113229751587\n",
      "Batch 165,  loss: 1.931713843345642\n",
      "Batch 170,  loss: 1.5565573215484618\n",
      "Batch 175,  loss: 1.6115524053573609\n",
      "Batch 180,  loss: 1.6299534797668458\n",
      "Batch 185,  loss: 1.4738323211669921\n",
      "Batch 190,  loss: 1.6402186036109925\n",
      "Batch 195,  loss: 1.7196794033050538\n",
      "Batch 200,  loss: 1.5840413331985475\n",
      "Batch 205,  loss: 1.6075428962707519\n",
      "Batch 210,  loss: 1.4667364478111267\n",
      "Batch 215,  loss: 1.4966784715652466\n",
      "Batch 220,  loss: 1.5717339038848877\n",
      "Batch 225,  loss: 1.569381093978882\n",
      "Batch 230,  loss: 1.327638840675354\n",
      "Batch 235,  loss: 1.6201322317123412\n",
      "Batch 240,  loss: 1.3949181675910949\n",
      "Batch 245,  loss: 1.267813014984131\n",
      "Batch 250,  loss: 1.4056325912475587\n",
      "Batch 255,  loss: 1.645743727684021\n",
      "Batch 260,  loss: 1.4037655353546143\n",
      "Batch 265,  loss: 1.7116063356399536\n",
      "Batch 270,  loss: 1.2843807697296143\n",
      "Batch 275,  loss: 1.5775023460388184\n",
      "Batch 280,  loss: 1.503217601776123\n",
      "Batch 285,  loss: 1.6600768566131592\n",
      "Batch 290,  loss: 1.4413262605667114\n",
      "Batch 295,  loss: 1.8125282764434814\n",
      "Batch 300,  loss: 1.4495697736740112\n",
      "Batch 305,  loss: 1.5440510034561157\n",
      "Batch 310,  loss: 1.337526226043701\n",
      "Batch 315,  loss: 1.5872849702835083\n",
      "Batch 320,  loss: 1.3364301204681397\n",
      "Batch 325,  loss: 1.4579584717750549\n",
      "Batch 330,  loss: 1.5883681058883667\n",
      "Batch 335,  loss: 1.744143795967102\n",
      "Batch 340,  loss: 1.783916163444519\n",
      "Batch 345,  loss: 1.633931612968445\n",
      "Batch 350,  loss: 1.7016806125640869\n",
      "Batch 355,  loss: 1.5427625894546508\n",
      "Batch 360,  loss: 1.7529550075531006\n",
      "Batch 365,  loss: 1.626637864112854\n",
      "Batch 370,  loss: 1.6090359210968017\n",
      "Batch 375,  loss: 1.2924991369247436\n",
      "Batch 380,  loss: 1.5423362970352172\n",
      "Batch 385,  loss: 1.4909920573234559\n",
      "Batch 390,  loss: 1.4906118631362915\n",
      "Batch 395,  loss: 2.05697124004364\n",
      "Batch 400,  loss: 1.3454148530960084\n",
      "Batch 405,  loss: 1.521621537208557\n",
      "Batch 410,  loss: 2.060517430305481\n",
      "Batch 415,  loss: 1.6718321561813354\n",
      "Batch 420,  loss: 1.5353785514831544\n",
      "Batch 425,  loss: 1.6600013732910157\n",
      "Batch 430,  loss: 1.8952500104904175\n",
      "Batch 435,  loss: 1.3903397798538208\n",
      "Batch 440,  loss: 1.4885576486587524\n",
      "Batch 445,  loss: 1.4752021551132202\n",
      "Batch 450,  loss: 1.7083916902542113\n",
      "Batch 455,  loss: 1.5070878505706786\n",
      "Batch 460,  loss: 1.5835699319839478\n",
      "Batch 465,  loss: 1.299552845954895\n",
      "Batch 470,  loss: 1.6596478462219237\n",
      "Batch 475,  loss: 1.1552863836288452\n",
      "Batch 480,  loss: 1.6527082681655885\n",
      "Batch 485,  loss: 1.2913100600242615\n",
      "Batch 490,  loss: 1.4229259967803956\n",
      "Batch 495,  loss: 1.4597431659698485\n",
      "Batch 500,  loss: 1.589381456375122\n",
      "Batch 505,  loss: 1.8236718654632569\n",
      "Batch 510,  loss: 1.5574992656707765\n",
      "Batch 515,  loss: 1.4063095808029176\n",
      "Batch 520,  loss: 1.4372236490249635\n",
      "Batch 525,  loss: 1.6703161478042603\n",
      "Batch 530,  loss: 1.5202372789382934\n",
      "Batch 535,  loss: 1.3682465195655822\n",
      "Batch 540,  loss: 1.5173002004623413\n",
      "Batch 545,  loss: 1.5568658828735351\n",
      "Batch 550,  loss: 1.5297104597091675\n",
      "Batch 555,  loss: 1.5295533657073974\n",
      "Batch 560,  loss: 1.4816318511962892\n",
      "Batch 565,  loss: 1.8900193452835083\n",
      "Batch 570,  loss: 1.5020195960998535\n",
      "Batch 575,  loss: 1.5253472805023194\n",
      "Batch 580,  loss: 1.2174991011619567\n",
      "Batch 585,  loss: 1.5881914615631103\n",
      "Batch 590,  loss: 1.495382046699524\n",
      "Batch 595,  loss: 1.5459372520446777\n",
      "Batch 600,  loss: 1.3544780969619752\n",
      "Batch 605,  loss: 1.624188446998596\n",
      "Batch 610,  loss: 1.3223682403564454\n",
      "Batch 615,  loss: 1.5896946907043457\n",
      "Batch 620,  loss: 1.4765248775482178\n",
      "Batch 625,  loss: 1.1087953805923463\n",
      "Batch 630,  loss: 1.8198342084884644\n",
      "Batch 635,  loss: 1.3117039799690247\n",
      "Batch 640,  loss: 1.3883842468261718\n",
      "Batch 645,  loss: 1.6906609296798707\n",
      "Batch 650,  loss: 1.4416930317878722\n",
      "Batch 655,  loss: 1.4027928829193115\n",
      "Batch 660,  loss: 1.3716201305389404\n",
      "Batch 665,  loss: 1.2101676225662232\n",
      "Batch 670,  loss: 1.6508397817611695\n",
      "Batch 675,  loss: 1.3497409343719482\n",
      "Batch 680,  loss: 1.4380661964416503\n",
      "Batch 685,  loss: 1.6035464525222778\n",
      "Batch 690,  loss: 1.3326123476028442\n",
      "Batch 695,  loss: 1.3805335283279419\n",
      "Batch 700,  loss: 1.4700250387191773\n",
      "Batch 705,  loss: 1.7296263456344605\n",
      "Batch 710,  loss: 1.4455805540084838\n",
      "Batch 715,  loss: 1.4751933932304382\n",
      "Batch 720,  loss: 1.5485710382461548\n",
      "Batch 725,  loss: 1.4565537452697754\n",
      "Batch 730,  loss: 1.360452699661255\n",
      "Batch 735,  loss: 1.715878176689148\n",
      "Batch 740,  loss: 1.6430537939071654\n",
      "Batch 745,  loss: 1.414979338645935\n",
      "Batch 750,  loss: 1.7299353361129761\n",
      "Batch 755,  loss: 1.4249568700790405\n",
      "Batch 760,  loss: 1.6963608503341674\n",
      "Batch 765,  loss: 1.6178154230117798\n",
      "Batch 770,  loss: 1.4240145802497863\n",
      "Batch 775,  loss: 1.4600651383399963\n",
      "Batch 780,  loss: 1.6141271114349365\n",
      "Batch 785,  loss: 1.3668243646621705\n",
      "Batch 790,  loss: 1.4862455606460572\n",
      "Batch 795,  loss: 1.6939970970153808\n",
      "Batch 800,  loss: 1.5468679189682006\n",
      "Batch 805,  loss: 1.6705941677093505\n",
      "Batch 810,  loss: 1.4568170070648194\n",
      "Batch 815,  loss: 1.3504896998405456\n",
      "Batch 820,  loss: 1.5206832408905029\n",
      "Batch 825,  loss: 1.3174012064933778\n",
      "Batch 830,  loss: 1.4482302904129027\n",
      "Batch 835,  loss: 1.4367897748947143\n",
      "Batch 840,  loss: 1.348209834098816\n",
      "Batch 845,  loss: 1.8256314277648926\n",
      "Batch 850,  loss: 1.285461437702179\n",
      "Batch 855,  loss: 1.8479751586914062\n",
      "Batch 860,  loss: 1.6933370351791381\n",
      "Batch 865,  loss: 1.5522137403488159\n",
      "Batch 870,  loss: 1.4381549119949342\n",
      "Batch 875,  loss: 1.6870089054107666\n",
      "Batch 880,  loss: 1.5029199600219727\n",
      "Batch 885,  loss: 1.5794796705245973\n",
      "Batch 890,  loss: 1.4881988406181335\n",
      "Batch 895,  loss: 1.4855562210083009\n",
      "Batch 900,  loss: 1.60142240524292\n",
      "Batch 905,  loss: 1.3638473987579345\n",
      "Batch 910,  loss: 1.6304094076156617\n",
      "Batch 915,  loss: 1.2803345918655396\n",
      "Batch 920,  loss: 1.7174194574356079\n",
      "Batch 925,  loss: 1.6396172046661377\n",
      "Batch 930,  loss: 1.3010222434997558\n",
      "Batch 935,  loss: 1.4518370628356934\n",
      "Batch 940,  loss: 1.7444531202316285\n",
      "Batch 945,  loss: 1.4769930362701416\n",
      "Batch 950,  loss: 1.7280455827713013\n",
      "Batch 955,  loss: 1.2498948335647584\n",
      "Batch 960,  loss: 1.3735340356826782\n",
      "Batch 965,  loss: 1.5365967988967895\n",
      "Batch 970,  loss: 1.4648393630981444\n",
      "Batch 975,  loss: 1.7559662580490112\n",
      "Batch 980,  loss: 1.5312142610549926\n",
      "Batch 985,  loss: 1.6923115491867065\n",
      "Batch 990,  loss: 1.523109745979309\n",
      "Batch 995,  loss: 1.539324688911438\n",
      "Batch 1000,  loss: 1.1398985505104064\n",
      "Batch 1005,  loss: 1.514745545387268\n",
      "Batch 1010,  loss: 1.1731151580810546\n",
      "Batch 1015,  loss: 1.608811616897583\n",
      "Batch 1020,  loss: 1.4181524515151978\n",
      "Batch 1025,  loss: 1.5159126281738282\n",
      "Batch 1030,  loss: 1.5142120122909546\n",
      "Batch 1035,  loss: 1.6328986406326294\n",
      "Batch 1040,  loss: 1.4784814596176148\n",
      "Batch 1045,  loss: 1.1346040606498717\n",
      "Batch 1050,  loss: 1.4607158184051514\n",
      "Batch 1055,  loss: 1.896754789352417\n",
      "Batch 1060,  loss: 2.0747601985931396\n",
      "Batch 1065,  loss: 1.6477230787277222\n",
      "Batch 1070,  loss: 1.2220473289489746\n",
      "Batch 1075,  loss: 1.5579816102981567\n",
      "Batch 1080,  loss: 1.4123826026916504\n",
      "Batch 1085,  loss: 1.1922952651977539\n",
      "Batch 1090,  loss: 1.4013920068740844\n",
      "Batch 1095,  loss: 1.700564432144165\n",
      "Batch 1100,  loss: 1.5688888788223267\n",
      "Batch 1105,  loss: 1.609126877784729\n",
      "Batch 1110,  loss: 1.3231632947921752\n",
      "Batch 1115,  loss: 1.3888651847839355\n",
      "Batch 1120,  loss: 1.782110905647278\n",
      "Batch 1125,  loss: 1.5084689140319825\n",
      "Batch 1130,  loss: 1.70193612575531\n",
      "Batch 1135,  loss: 1.8640010833740235\n",
      "Batch 1140,  loss: 1.5350986003875733\n",
      "Batch 1145,  loss: 2.0274983167648317\n",
      "Batch 1150,  loss: 1.8889922380447388\n",
      "Batch 1155,  loss: 1.4139314889907837\n",
      "Batch 1160,  loss: 1.317763102054596\n",
      "Batch 1165,  loss: 1.5862197160720826\n",
      "Batch 1170,  loss: 1.68259859085083\n",
      "Batch 1175,  loss: 1.5912213325500488\n",
      "Batch 1180,  loss: 1.6952014446258545\n",
      "Batch 1185,  loss: 1.7982411146163941\n",
      "Batch 1190,  loss: 1.3728051662445069\n",
      "Batch 1195,  loss: 1.282858145236969\n",
      "Batch 1200,  loss: 1.5803341150283814\n",
      "Batch 1205,  loss: 1.3104347944259644\n",
      "Batch 1210,  loss: 1.3795658111572267\n",
      "Batch 1215,  loss: 1.2875167369842528\n",
      "Batch 1220,  loss: 1.654136633872986\n",
      "Batch 1225,  loss: 1.6606464624404906\n",
      "Batch 1230,  loss: 1.244365406036377\n",
      "Batch 1235,  loss: 1.4436739206314086\n",
      "Batch 1240,  loss: 1.1256201982498169\n",
      "Batch 1245,  loss: 1.8437024354934692\n",
      "Batch 1250,  loss: 1.6660119533538817\n",
      "Batch 1255,  loss: 1.6039265632629394\n",
      "Batch 1260,  loss: 1.4188216209411622\n",
      "Batch 1265,  loss: 1.4332163333892822\n",
      "Batch 1270,  loss: 1.516321873664856\n",
      "Batch 1275,  loss: 1.6070253372192382\n",
      "Batch 1280,  loss: 1.8385927438735963\n",
      "Batch 1285,  loss: 1.4109620332717896\n",
      "Batch 1290,  loss: 1.4433366775512695\n",
      "Batch 1295,  loss: 1.3319920539855956\n",
      "LOSS train 1.3319920539855956. Validation loss: 1.9500212052974988 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 38:\n",
      "Batch 5,  loss: 1.6590113401412965\n",
      "Batch 10,  loss: 1.9306828737258912\n",
      "Batch 15,  loss: 1.6273580074310303\n",
      "Batch 20,  loss: 1.4589359998703002\n",
      "Batch 25,  loss: 1.643041968345642\n",
      "Batch 30,  loss: 1.3748989820480346\n",
      "Batch 35,  loss: 1.4602940797805786\n",
      "Batch 40,  loss: 1.2926768779754638\n",
      "Batch 45,  loss: 1.4185834407806397\n",
      "Batch 50,  loss: 1.6056742191314697\n",
      "Batch 55,  loss: 1.487012791633606\n",
      "Batch 60,  loss: 1.3639711618423462\n",
      "Batch 65,  loss: 1.4308045864105225\n",
      "Batch 70,  loss: 1.6025089979171754\n",
      "Batch 75,  loss: 1.5119762301445008\n",
      "Batch 80,  loss: 1.4667759656906127\n",
      "Batch 85,  loss: 1.5994158744812013\n",
      "Batch 90,  loss: 1.6933003187179565\n",
      "Batch 95,  loss: 1.5032424092292787\n",
      "Batch 100,  loss: 1.4684189796447753\n",
      "Batch 105,  loss: 1.3867792963981629\n",
      "Batch 110,  loss: 1.4705316305160523\n",
      "Batch 115,  loss: 1.2401365399360658\n",
      "Batch 120,  loss: 1.6013397693634033\n",
      "Batch 125,  loss: 1.2854395866394044\n",
      "Batch 130,  loss: 1.117252027988434\n",
      "Batch 135,  loss: 1.1459843397140503\n",
      "Batch 140,  loss: 1.598222827911377\n",
      "Batch 145,  loss: 1.492323112487793\n",
      "Batch 150,  loss: 1.427283525466919\n",
      "Batch 155,  loss: 1.5132400274276734\n",
      "Batch 160,  loss: 1.6918938159942627\n",
      "Batch 165,  loss: 1.0383509039878844\n",
      "Batch 170,  loss: 1.6026122570037842\n",
      "Batch 175,  loss: 1.1500763058662415\n",
      "Batch 180,  loss: 1.536232352256775\n",
      "Batch 185,  loss: 1.5571207284927369\n",
      "Batch 190,  loss: 1.5122989654541015\n",
      "Batch 195,  loss: 1.765425968170166\n",
      "Batch 200,  loss: 1.4515892386436462\n",
      "Batch 205,  loss: 1.8113639116287232\n",
      "Batch 210,  loss: 1.2372994661331176\n",
      "Batch 215,  loss: 1.5519964456558228\n",
      "Batch 220,  loss: 1.973655891418457\n",
      "Batch 225,  loss: 1.8052680253982545\n",
      "Batch 230,  loss: 1.2327507257461547\n",
      "Batch 235,  loss: 1.4716097116470337\n",
      "Batch 240,  loss: 1.5090825319290162\n",
      "Batch 245,  loss: 2.0777111768722536\n",
      "Batch 250,  loss: 1.5297431945800781\n",
      "Batch 255,  loss: 1.5294177651405334\n",
      "Batch 260,  loss: 1.8297384262084961\n",
      "Batch 265,  loss: 1.3395278692245483\n",
      "Batch 270,  loss: 1.3044119358062745\n",
      "Batch 275,  loss: 1.6031567096710204\n",
      "Batch 280,  loss: 1.5376190423965455\n",
      "Batch 285,  loss: 1.4179096937179565\n",
      "Batch 290,  loss: 1.2800296545028687\n",
      "Batch 295,  loss: 1.4969314098358155\n",
      "Batch 300,  loss: 1.2898820400238038\n",
      "Batch 305,  loss: 1.7217424869537354\n",
      "Batch 310,  loss: 2.015721583366394\n",
      "Batch 315,  loss: 1.4244537830352784\n",
      "Batch 320,  loss: 1.5762370586395265\n",
      "Batch 325,  loss: 1.5153220653533936\n",
      "Batch 330,  loss: 1.9406713485717773\n",
      "Batch 335,  loss: 1.6613301277160644\n",
      "Batch 340,  loss: 1.7102367281913757\n",
      "Batch 345,  loss: 1.4506416082382203\n",
      "Batch 350,  loss: 1.4895911455154418\n",
      "Batch 355,  loss: 1.557990562915802\n",
      "Batch 360,  loss: 1.520182466506958\n",
      "Batch 365,  loss: 1.6474493026733399\n",
      "Batch 370,  loss: 1.2484121680259705\n",
      "Batch 375,  loss: 1.567752480506897\n",
      "Batch 380,  loss: 1.2917405843734742\n",
      "Batch 385,  loss: 1.5243770599365234\n",
      "Batch 390,  loss: 1.444491481781006\n",
      "Batch 395,  loss: 1.391906476020813\n",
      "Batch 400,  loss: 1.1585864067077636\n",
      "Batch 405,  loss: 1.5394541382789613\n",
      "Batch 410,  loss: 1.4329572319984436\n",
      "Batch 415,  loss: 1.740753412246704\n",
      "Batch 420,  loss: 1.353002166748047\n",
      "Batch 425,  loss: 1.4443778157234193\n",
      "Batch 430,  loss: 1.259412956237793\n",
      "Batch 435,  loss: 1.8439278602600098\n",
      "Batch 440,  loss: 1.5870627403259276\n",
      "Batch 445,  loss: 1.5228213548660279\n",
      "Batch 450,  loss: 1.419219422340393\n",
      "Batch 455,  loss: 1.558763313293457\n",
      "Batch 460,  loss: 1.5403053522109986\n",
      "Batch 465,  loss: 1.4950687408447265\n",
      "Batch 470,  loss: 1.574674868583679\n",
      "Batch 475,  loss: 1.443036961555481\n",
      "Batch 480,  loss: 1.3478508353233338\n",
      "Batch 485,  loss: 1.451234793663025\n",
      "Batch 490,  loss: 1.2639921307563782\n",
      "Batch 495,  loss: 1.7828725814819335\n",
      "Batch 500,  loss: 1.651340413093567\n",
      "Batch 505,  loss: 1.507324481010437\n",
      "Batch 510,  loss: 1.7920110464096068\n",
      "Batch 515,  loss: 1.3476093769073487\n",
      "Batch 520,  loss: 1.4064341545104981\n",
      "Batch 525,  loss: 1.6216941833496095\n",
      "Batch 530,  loss: 1.4378497123718261\n",
      "Batch 535,  loss: 1.255837881565094\n",
      "Batch 540,  loss: 1.4085685849189757\n",
      "Batch 545,  loss: 1.7497509002685547\n",
      "Batch 550,  loss: 1.37073757648468\n",
      "Batch 555,  loss: 1.4610591173171996\n",
      "Batch 560,  loss: 1.6434548854827882\n",
      "Batch 565,  loss: 1.693372130393982\n",
      "Batch 570,  loss: 1.3712304830551147\n",
      "Batch 575,  loss: 1.2925594568252563\n",
      "Batch 580,  loss: 1.2760941863059998\n",
      "Batch 585,  loss: 1.742682647705078\n",
      "Batch 590,  loss: 1.205548310279846\n",
      "Batch 595,  loss: 1.304625105857849\n",
      "Batch 600,  loss: 1.9141194820404053\n",
      "Batch 605,  loss: 1.5575987815856933\n",
      "Batch 610,  loss: 1.4443803787231446\n",
      "Batch 615,  loss: 1.2121919512748718\n",
      "Batch 620,  loss: 1.3581815242767334\n",
      "Batch 625,  loss: 1.177186369895935\n",
      "Batch 630,  loss: 1.6519714832305907\n",
      "Batch 635,  loss: 1.3548341512680053\n",
      "Batch 640,  loss: 1.5826029300689697\n",
      "Batch 645,  loss: 1.5607056975364686\n",
      "Batch 650,  loss: 1.2637434720993042\n",
      "Batch 655,  loss: 1.5394312620162964\n",
      "Batch 660,  loss: 1.4100815773010253\n",
      "Batch 665,  loss: 1.5514477491378784\n",
      "Batch 670,  loss: 1.4945235013961793\n",
      "Batch 675,  loss: 1.5381805181503296\n",
      "Batch 680,  loss: 1.7785493373870849\n",
      "Batch 685,  loss: 1.534374475479126\n",
      "Batch 690,  loss: 1.3577395915985107\n",
      "Batch 695,  loss: 1.575346255302429\n",
      "Batch 700,  loss: 1.505276870727539\n",
      "Batch 705,  loss: 1.3506870865821838\n",
      "Batch 710,  loss: 1.2730196237564086\n",
      "Batch 715,  loss: 1.5568894863128662\n",
      "Batch 720,  loss: 1.416141414642334\n",
      "Batch 725,  loss: 1.7032680988311768\n",
      "Batch 730,  loss: 1.3555439949035644\n",
      "Batch 735,  loss: 2.044209909439087\n",
      "Batch 740,  loss: 1.3913349509239197\n",
      "Batch 745,  loss: 1.3565119981765748\n",
      "Batch 750,  loss: 1.5696621894836427\n",
      "Batch 755,  loss: 1.7037281751632691\n",
      "Batch 760,  loss: 1.6208157300949098\n",
      "Batch 765,  loss: 1.5071256637573243\n",
      "Batch 770,  loss: 1.44150869846344\n",
      "Batch 775,  loss: 1.378684115409851\n",
      "Batch 780,  loss: 1.7156569242477417\n",
      "Batch 785,  loss: 1.4501412749290465\n",
      "Batch 790,  loss: 1.5107805013656617\n",
      "Batch 795,  loss: 1.7916367530822754\n",
      "Batch 800,  loss: 1.6202128171920775\n",
      "Batch 805,  loss: 1.6368064880371094\n",
      "Batch 810,  loss: 1.5986159086227416\n",
      "Batch 815,  loss: 1.8060996532440186\n",
      "Batch 820,  loss: 1.4275835633277894\n",
      "Batch 825,  loss: 1.3779820680618287\n",
      "Batch 830,  loss: 1.4616745710372925\n",
      "Batch 835,  loss: 1.3705710887908935\n",
      "Batch 840,  loss: 1.5561075687408448\n",
      "Batch 845,  loss: 1.5779626369476318\n",
      "Batch 850,  loss: 1.5014600515365601\n",
      "Batch 855,  loss: 1.3465148687362671\n",
      "Batch 860,  loss: 1.4108862400054931\n",
      "Batch 865,  loss: 1.7581645011901856\n",
      "Batch 870,  loss: 1.7690018653869628\n",
      "Batch 875,  loss: 1.5110985040664673\n",
      "Batch 880,  loss: 1.204244327545166\n",
      "Batch 885,  loss: 1.7799823045730592\n",
      "Batch 890,  loss: 1.5800547838211059\n",
      "Batch 895,  loss: 2.047377896308899\n",
      "Batch 900,  loss: 1.5862677097320557\n",
      "Batch 905,  loss: 1.504814863204956\n",
      "Batch 910,  loss: 1.4902388334274292\n",
      "Batch 915,  loss: 1.4664530754089355\n",
      "Batch 920,  loss: 1.749783492088318\n",
      "Batch 925,  loss: 1.5204490423202515\n",
      "Batch 930,  loss: 1.6278481006622314\n",
      "Batch 935,  loss: 1.618914794921875\n",
      "Batch 940,  loss: 1.3792582035064698\n",
      "Batch 945,  loss: 1.8950639963150024\n",
      "Batch 950,  loss: 1.6217667937278748\n",
      "Batch 955,  loss: 1.216062068939209\n",
      "Batch 960,  loss: 1.4320666551589967\n",
      "Batch 965,  loss: 1.7757988691329956\n",
      "Batch 970,  loss: 1.8400821685791016\n",
      "Batch 975,  loss: 1.3717674732208252\n",
      "Batch 980,  loss: 1.258308720588684\n",
      "Batch 985,  loss: 1.4747764706611632\n",
      "Batch 990,  loss: 1.5153232097625733\n",
      "Batch 995,  loss: 1.3356890678405762\n",
      "Batch 1000,  loss: 1.3594314098358153\n",
      "Batch 1005,  loss: 1.525265145301819\n",
      "Batch 1010,  loss: 1.6108922481536865\n",
      "Batch 1015,  loss: 1.4218969821929932\n",
      "Batch 1020,  loss: 1.124018633365631\n",
      "Batch 1025,  loss: 1.2960279703140258\n",
      "Batch 1030,  loss: 1.5576317071914674\n",
      "Batch 1035,  loss: 1.3862263441085816\n",
      "Batch 1040,  loss: 1.7351281642913818\n",
      "Batch 1045,  loss: 1.3156930685043335\n",
      "Batch 1050,  loss: 1.7341091632843018\n",
      "Batch 1055,  loss: 1.4271243810653687\n",
      "Batch 1060,  loss: 1.5321542978286744\n",
      "Batch 1065,  loss: 1.409249496459961\n",
      "Batch 1070,  loss: 1.3926395416259765\n",
      "Batch 1075,  loss: 1.5710421562194825\n",
      "Batch 1080,  loss: 1.40229971408844\n",
      "Batch 1085,  loss: 1.167164146900177\n",
      "Batch 1090,  loss: 1.9790979623794556\n",
      "Batch 1095,  loss: 1.6049198150634765\n",
      "Batch 1100,  loss: 1.5028969764709472\n",
      "Batch 1105,  loss: 1.463707160949707\n",
      "Batch 1110,  loss: 1.76489577293396\n",
      "Batch 1115,  loss: 1.7616999626159668\n",
      "Batch 1120,  loss: 1.3784286260604859\n",
      "Batch 1125,  loss: 1.710471224784851\n",
      "Batch 1130,  loss: 1.4870578527450562\n",
      "Batch 1135,  loss: 1.271198558807373\n",
      "Batch 1140,  loss: 1.346170973777771\n",
      "Batch 1145,  loss: 1.5886819124221803\n",
      "Batch 1150,  loss: 1.229185652732849\n",
      "Batch 1155,  loss: 1.7185014963150025\n",
      "Batch 1160,  loss: 1.4006635427474976\n",
      "Batch 1165,  loss: 1.302514910697937\n",
      "Batch 1170,  loss: 1.6173218488693237\n",
      "Batch 1175,  loss: 1.394377064704895\n",
      "Batch 1180,  loss: 1.2615171432495118\n",
      "Batch 1185,  loss: 1.4736228466033936\n",
      "Batch 1190,  loss: 1.2729117155075074\n",
      "Batch 1195,  loss: 1.7164148569107056\n",
      "Batch 1200,  loss: 1.6947924613952636\n",
      "Batch 1205,  loss: 1.6238860368728638\n",
      "Batch 1210,  loss: 1.4563021183013916\n",
      "Batch 1215,  loss: 1.672388482093811\n",
      "Batch 1220,  loss: 1.6926413536071778\n",
      "Batch 1225,  loss: 1.7059180736541748\n",
      "Batch 1230,  loss: 1.5843868732452393\n",
      "Batch 1235,  loss: 1.4149756908416748\n",
      "Batch 1240,  loss: 1.2891111850738526\n",
      "Batch 1245,  loss: 1.6268645524978638\n",
      "Batch 1250,  loss: 1.5701743602752685\n",
      "Batch 1255,  loss: 1.4883267164230347\n",
      "Batch 1260,  loss: 1.2986639499664308\n",
      "Batch 1265,  loss: 1.8137443780899047\n",
      "Batch 1270,  loss: 1.585964822769165\n",
      "Batch 1275,  loss: 1.3821148872375488\n",
      "Batch 1280,  loss: 1.3432473182678222\n",
      "Batch 1285,  loss: 1.8237988948822021\n",
      "Batch 1290,  loss: 1.5790785551071167\n",
      "Batch 1295,  loss: 1.519195556640625\n",
      "LOSS train 1.519195556640625. Validation loss: 2.0543946453301167 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 39:\n",
      "Batch 5,  loss: 1.3541814088821411\n",
      "Batch 10,  loss: 1.4113038301467895\n",
      "Batch 15,  loss: 1.86440749168396\n",
      "Batch 20,  loss: 1.825312614440918\n",
      "Batch 25,  loss: 1.4116680145263671\n",
      "Batch 30,  loss: 1.1511585116386414\n",
      "Batch 35,  loss: 2.065163326263428\n",
      "Batch 40,  loss: 1.556099820137024\n",
      "Batch 45,  loss: 1.345124363899231\n",
      "Batch 50,  loss: 1.4212921857833862\n",
      "Batch 55,  loss: 1.4523109436035155\n",
      "Batch 60,  loss: 1.421583390235901\n",
      "Batch 65,  loss: 1.4598981857299804\n",
      "Batch 70,  loss: 1.4999316692352296\n",
      "Batch 75,  loss: 1.553064227104187\n",
      "Batch 80,  loss: 1.702940034866333\n",
      "Batch 85,  loss: 1.4782641649246215\n",
      "Batch 90,  loss: 1.498224401473999\n",
      "Batch 95,  loss: 1.442020320892334\n",
      "Batch 100,  loss: 1.3132383584976197\n",
      "Batch 105,  loss: 1.6351302862167358\n",
      "Batch 110,  loss: 1.3456759929656983\n",
      "Batch 115,  loss: 1.4550540685653686\n",
      "Batch 120,  loss: 1.4561313033103942\n",
      "Batch 125,  loss: 1.4141767740249633\n",
      "Batch 130,  loss: 1.3821624040603637\n",
      "Batch 135,  loss: 1.547074794769287\n",
      "Batch 140,  loss: 1.5267978191375733\n",
      "Batch 145,  loss: 1.7143941879272462\n",
      "Batch 150,  loss: 1.291489839553833\n",
      "Batch 155,  loss: 1.4576077222824098\n",
      "Batch 160,  loss: 1.2587499260902404\n",
      "Batch 165,  loss: 1.5872567653656007\n",
      "Batch 170,  loss: 1.5746692299842835\n",
      "Batch 175,  loss: 1.3153008460998534\n",
      "Batch 180,  loss: 1.7363116979598998\n",
      "Batch 185,  loss: 1.6344287395477295\n",
      "Batch 190,  loss: 1.643070125579834\n",
      "Batch 195,  loss: 1.498190975189209\n",
      "Batch 200,  loss: 1.3866382122039795\n",
      "Batch 205,  loss: 1.567072606086731\n",
      "Batch 210,  loss: 1.4260063886642456\n",
      "Batch 215,  loss: 1.4809908390045166\n",
      "Batch 220,  loss: 1.5853193283081055\n",
      "Batch 225,  loss: 1.3100217580795288\n",
      "Batch 230,  loss: 1.2973552227020264\n",
      "Batch 235,  loss: 1.8090930938720704\n",
      "Batch 240,  loss: 1.3951531887054442\n",
      "Batch 245,  loss: 1.428359031677246\n",
      "Batch 250,  loss: 1.6454620361328125\n",
      "Batch 255,  loss: 1.563204288482666\n",
      "Batch 260,  loss: 1.2764487743377686\n",
      "Batch 265,  loss: 1.4091654539108276\n",
      "Batch 270,  loss: 1.8281634330749512\n",
      "Batch 275,  loss: 1.253113031387329\n",
      "Batch 280,  loss: 1.3084520816802978\n",
      "Batch 285,  loss: 1.3901893854141236\n",
      "Batch 290,  loss: 1.584579300880432\n",
      "Batch 295,  loss: 1.290562105178833\n",
      "Batch 300,  loss: 1.4243895769119264\n",
      "Batch 305,  loss: 1.306890606880188\n",
      "Batch 310,  loss: 1.4087048172950745\n",
      "Batch 315,  loss: 1.6024890661239624\n",
      "Batch 320,  loss: 1.5434418678283692\n",
      "Batch 325,  loss: 1.7311240196228028\n",
      "Batch 330,  loss: 1.586060643196106\n",
      "Batch 335,  loss: 1.2094568848609923\n",
      "Batch 340,  loss: 1.7587000608444214\n",
      "Batch 345,  loss: 1.4447947978973388\n",
      "Batch 350,  loss: 1.7953244686126708\n",
      "Batch 355,  loss: 1.3965290784835815\n",
      "Batch 360,  loss: 1.5280853986740113\n",
      "Batch 365,  loss: 1.760126543045044\n",
      "Batch 370,  loss: 2.0644041538238525\n",
      "Batch 375,  loss: 1.6934121370315551\n",
      "Batch 380,  loss: 1.3416435241699218\n",
      "Batch 385,  loss: 1.7483412742614746\n",
      "Batch 390,  loss: 1.3711896419525147\n",
      "Batch 395,  loss: 1.4631736636161805\n",
      "Batch 400,  loss: 1.6290096998214723\n",
      "Batch 405,  loss: 1.740237045288086\n",
      "Batch 410,  loss: 1.4245099306106568\n",
      "Batch 415,  loss: 1.4090389251708983\n",
      "Batch 420,  loss: 1.5251840591430663\n",
      "Batch 425,  loss: 1.6874349594116211\n",
      "Batch 430,  loss: 1.4324139833450318\n",
      "Batch 435,  loss: 1.1427314043045045\n",
      "Batch 440,  loss: 1.5481095433235168\n",
      "Batch 445,  loss: 1.680165433883667\n",
      "Batch 450,  loss: 1.1631420612335206\n",
      "Batch 455,  loss: 1.793008852005005\n",
      "Batch 460,  loss: 1.593969464302063\n",
      "Batch 465,  loss: 1.5665135622024535\n",
      "Batch 470,  loss: 1.4599651098251343\n",
      "Batch 475,  loss: 1.663406491279602\n",
      "Batch 480,  loss: 1.4689525723457337\n",
      "Batch 485,  loss: 1.43205668926239\n",
      "Batch 490,  loss: 1.5407861709594726\n",
      "Batch 495,  loss: 1.4357352018356324\n",
      "Batch 500,  loss: 1.6239016771316528\n",
      "Batch 505,  loss: 1.520680832862854\n",
      "Batch 510,  loss: 1.80330171585083\n",
      "Batch 515,  loss: 1.5845407009124757\n",
      "Batch 520,  loss: 1.6266151189804077\n",
      "Batch 525,  loss: 1.513004755973816\n",
      "Batch 530,  loss: 1.4258667945861816\n",
      "Batch 535,  loss: 1.5175790309906005\n",
      "Batch 540,  loss: 1.584563386440277\n",
      "Batch 545,  loss: 1.2161484122276307\n",
      "Batch 550,  loss: 1.6138019323349\n",
      "Batch 555,  loss: 1.2946335554122925\n",
      "Batch 560,  loss: 1.4245224952697755\n",
      "Batch 565,  loss: 1.3832602500915527\n",
      "Batch 570,  loss: 1.6360843658447266\n",
      "Batch 575,  loss: 1.39480299949646\n",
      "Batch 580,  loss: 1.4244258165359498\n",
      "Batch 585,  loss: 1.7515750408172608\n",
      "Batch 590,  loss: 1.457521915435791\n",
      "Batch 595,  loss: 1.6102775096893311\n",
      "Batch 600,  loss: 1.591368818283081\n",
      "Batch 605,  loss: 1.5842590093612672\n",
      "Batch 610,  loss: 1.6341891527175902\n",
      "Batch 615,  loss: 1.7799873113632203\n",
      "Batch 620,  loss: 1.2292977571487427\n",
      "Batch 625,  loss: 1.2724671363830566\n",
      "Batch 630,  loss: 1.5354061841964721\n",
      "Batch 635,  loss: 1.3396306037902832\n",
      "Batch 640,  loss: 1.5251423001289368\n",
      "Batch 645,  loss: 1.687955379486084\n",
      "Batch 650,  loss: 1.5237269163131715\n",
      "Batch 655,  loss: 1.5578685998916626\n",
      "Batch 660,  loss: 1.8887266397476197\n",
      "Batch 665,  loss: 1.4816523790359497\n",
      "Batch 670,  loss: 1.4719176292419434\n",
      "Batch 675,  loss: 1.5566888093948363\n",
      "Batch 680,  loss: 1.6552107334136963\n",
      "Batch 685,  loss: 1.5522579908370973\n",
      "Batch 690,  loss: 1.2158247828483582\n",
      "Batch 695,  loss: 1.1860360264778138\n",
      "Batch 700,  loss: 1.706614875793457\n",
      "Batch 705,  loss: 1.4359589099884034\n",
      "Batch 710,  loss: 1.5134508848190307\n",
      "Batch 715,  loss: 1.8688817501068116\n",
      "Batch 720,  loss: 1.5732361316680907\n",
      "Batch 725,  loss: 1.8397082328796386\n",
      "Batch 730,  loss: 1.565274143218994\n",
      "Batch 735,  loss: 1.4768054485321045\n",
      "Batch 740,  loss: 1.5778068542480468\n",
      "Batch 745,  loss: 1.3747326374053954\n",
      "Batch 750,  loss: 1.2880938291549682\n",
      "Batch 755,  loss: 1.8978623151779175\n",
      "Batch 760,  loss: 1.5975718259811402\n",
      "Batch 765,  loss: 1.536627435684204\n",
      "Batch 770,  loss: 1.40818247795105\n",
      "Batch 775,  loss: 1.3733463525772094\n",
      "Batch 780,  loss: 1.4182105541229248\n",
      "Batch 785,  loss: 1.335650658607483\n",
      "Batch 790,  loss: 1.431458306312561\n",
      "Batch 795,  loss: 1.2846257448196412\n",
      "Batch 800,  loss: 1.5805325508117676\n",
      "Batch 805,  loss: 1.5775376081466674\n",
      "Batch 810,  loss: 1.5371098518371582\n",
      "Batch 815,  loss: 1.919247579574585\n",
      "Batch 820,  loss: 1.6221377372741699\n",
      "Batch 825,  loss: 1.3407125234603883\n",
      "Batch 830,  loss: 1.5082592487335205\n",
      "Batch 835,  loss: 1.5884435772895813\n",
      "Batch 840,  loss: 1.1792436122894288\n",
      "Batch 845,  loss: 1.5129690170288086\n",
      "Batch 850,  loss: 1.9043152332305908\n",
      "Batch 855,  loss: 1.7887303829193115\n",
      "Batch 860,  loss: 1.614849817752838\n",
      "Batch 865,  loss: 1.5696583032608031\n",
      "Batch 870,  loss: 1.6038115978240968\n",
      "Batch 875,  loss: 1.529882264137268\n",
      "Batch 880,  loss: 1.1975244522094726\n",
      "Batch 885,  loss: 1.5793982982635497\n",
      "Batch 890,  loss: 1.6523165225982666\n",
      "Batch 895,  loss: 1.2916105270385743\n",
      "Batch 900,  loss: 1.5800194025039673\n",
      "Batch 905,  loss: 1.4203267335891723\n",
      "Batch 910,  loss: 1.2142253637313842\n",
      "Batch 915,  loss: 1.6102885365486146\n",
      "Batch 920,  loss: 1.6635334968566895\n",
      "Batch 925,  loss: 1.3886557817459106\n",
      "Batch 930,  loss: 1.5606536149978638\n",
      "Batch 935,  loss: 1.400578498840332\n",
      "Batch 940,  loss: 1.7130733728408813\n",
      "Batch 945,  loss: 1.4429414510726928\n",
      "Batch 950,  loss: 1.7489951133728028\n",
      "Batch 955,  loss: 2.0510134935379027\n",
      "Batch 960,  loss: 1.5093387126922608\n",
      "Batch 965,  loss: 1.5501192331314086\n",
      "Batch 970,  loss: 1.8016285657882691\n",
      "Batch 975,  loss: 1.6690268993377686\n",
      "Batch 980,  loss: 1.529753279685974\n",
      "Batch 985,  loss: 1.613288116455078\n",
      "Batch 990,  loss: 1.4704712629318237\n",
      "Batch 995,  loss: 1.2588907599449157\n",
      "Batch 1000,  loss: 1.4599945545196533\n",
      "Batch 1005,  loss: 1.3177391290664673\n",
      "Batch 1010,  loss: 1.2717286109924317\n",
      "Batch 1015,  loss: 1.252731239795685\n",
      "Batch 1020,  loss: 1.113944947719574\n",
      "Batch 1025,  loss: 1.7481610298156738\n",
      "Batch 1030,  loss: 1.8501637935638429\n",
      "Batch 1035,  loss: 1.1486960649490356\n",
      "Batch 1040,  loss: 1.5632386207580566\n",
      "Batch 1045,  loss: 1.5134860277175903\n",
      "Batch 1050,  loss: 1.4711808681488037\n",
      "Batch 1055,  loss: 1.362791907787323\n",
      "Batch 1060,  loss: 1.5699848413467408\n",
      "Batch 1065,  loss: 1.3796947717666626\n",
      "Batch 1070,  loss: 1.2860710859298705\n",
      "Batch 1075,  loss: 1.4967391729354858\n",
      "Batch 1080,  loss: 1.548146867752075\n",
      "Batch 1085,  loss: 1.496474027633667\n",
      "Batch 1090,  loss: 1.3354616284370422\n",
      "Batch 1095,  loss: 1.3838682413101195\n",
      "Batch 1100,  loss: 1.3578122973442077\n",
      "Batch 1105,  loss: 1.5133209586143495\n",
      "Batch 1110,  loss: 1.524095845222473\n",
      "Batch 1115,  loss: 1.4116763114929198\n",
      "Batch 1120,  loss: 1.8810537338256836\n",
      "Batch 1125,  loss: 1.4387850522994996\n",
      "Batch 1130,  loss: 1.359339714050293\n",
      "Batch 1135,  loss: 1.5092142343521118\n",
      "Batch 1140,  loss: 1.3375483751296997\n",
      "Batch 1145,  loss: 1.675935196876526\n",
      "Batch 1150,  loss: 1.57847501039505\n",
      "Batch 1155,  loss: 1.5746243238449096\n",
      "Batch 1160,  loss: 1.5813156604766845\n",
      "Batch 1165,  loss: 1.6224642753601075\n",
      "Batch 1170,  loss: 1.3812734365463257\n",
      "Batch 1175,  loss: 1.363760280609131\n",
      "Batch 1180,  loss: 1.425647234916687\n",
      "Batch 1185,  loss: 1.6145829916000367\n",
      "Batch 1190,  loss: 1.4218358516693115\n",
      "Batch 1195,  loss: 2.077332377433777\n",
      "Batch 1200,  loss: 1.6661213159561157\n",
      "Batch 1205,  loss: 1.292311930656433\n",
      "Batch 1210,  loss: 1.864611029624939\n",
      "Batch 1215,  loss: 1.5393213748931884\n",
      "Batch 1220,  loss: 1.3452281832695008\n",
      "Batch 1225,  loss: 1.6502181053161622\n",
      "Batch 1230,  loss: 1.4407705068588257\n",
      "Batch 1235,  loss: 1.432575511932373\n",
      "Batch 1240,  loss: 1.3919353008270263\n",
      "Batch 1245,  loss: 1.4842377662658692\n",
      "Batch 1250,  loss: 1.4299620866775513\n",
      "Batch 1255,  loss: 1.7816685438156128\n",
      "Batch 1260,  loss: 1.7141184091567994\n",
      "Batch 1265,  loss: 1.3785520315170288\n",
      "Batch 1270,  loss: 1.3906523942947389\n",
      "Batch 1275,  loss: 1.3354179620742799\n",
      "Batch 1280,  loss: 1.5450719594955444\n",
      "Batch 1285,  loss: 1.534987211227417\n",
      "Batch 1290,  loss: 1.5765718698501587\n",
      "Batch 1295,  loss: 1.4814766883850097\n",
      "LOSS train 1.4814766883850097. Validation loss: 2.108298248935629 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 40:\n",
      "Batch 5,  loss: 1.5630756855010985\n",
      "Batch 10,  loss: 1.4758169889450072\n",
      "Batch 15,  loss: 1.64277503490448\n",
      "Batch 20,  loss: 1.7001704454421998\n",
      "Batch 25,  loss: 1.5719278573989868\n",
      "Batch 30,  loss: 1.4381244659423829\n",
      "Batch 35,  loss: 1.6691138982772826\n",
      "Batch 40,  loss: 1.5914201259613037\n",
      "Batch 45,  loss: 1.4185355663299561\n",
      "Batch 50,  loss: 1.9896678924560547\n",
      "Batch 55,  loss: 1.6653466939926147\n",
      "Batch 60,  loss: 1.6486526250839233\n",
      "Batch 65,  loss: 1.4149937152862548\n",
      "Batch 70,  loss: 1.333074951171875\n",
      "Batch 75,  loss: 1.5670251727104187\n",
      "Batch 80,  loss: 1.6110857009887696\n",
      "Batch 85,  loss: 1.456407380104065\n",
      "Batch 90,  loss: 1.5617534160614013\n",
      "Batch 95,  loss: 1.6449771881103517\n",
      "Batch 100,  loss: 1.4988940477371215\n",
      "Batch 105,  loss: 1.2686358451843263\n",
      "Batch 110,  loss: 1.4839734792709351\n",
      "Batch 115,  loss: 1.618562602996826\n",
      "Batch 120,  loss: 1.9453655242919923\n",
      "Batch 125,  loss: 1.591609787940979\n",
      "Batch 130,  loss: 1.714479112625122\n",
      "Batch 135,  loss: 1.600423765182495\n",
      "Batch 140,  loss: 1.2612622261047364\n",
      "Batch 145,  loss: 1.730001974105835\n",
      "Batch 150,  loss: 1.6071043491363526\n",
      "Batch 155,  loss: 1.319638442993164\n",
      "Batch 160,  loss: 1.288090968132019\n",
      "Batch 165,  loss: 1.5585251092910766\n",
      "Batch 170,  loss: 1.1074949026107788\n",
      "Batch 175,  loss: 1.7387599229812623\n",
      "Batch 180,  loss: 1.5020459175109864\n",
      "Batch 185,  loss: 1.807568335533142\n",
      "Batch 190,  loss: 1.00517281293869\n",
      "Batch 195,  loss: 1.7024751424789428\n",
      "Batch 200,  loss: 1.1505709052085877\n",
      "Batch 205,  loss: 1.8515739679336547\n",
      "Batch 210,  loss: 1.3999505996704102\n",
      "Batch 215,  loss: 1.317892575263977\n",
      "Batch 220,  loss: 1.3393907308578492\n",
      "Batch 225,  loss: 1.5193764209747314\n",
      "Batch 230,  loss: 1.6802247285842895\n",
      "Batch 235,  loss: 1.5126481294631957\n",
      "Batch 240,  loss: 1.5755693197250367\n",
      "Batch 245,  loss: 1.4077476263046265\n",
      "Batch 250,  loss: 1.5208962202072143\n",
      "Batch 255,  loss: 1.4696921825408935\n",
      "Batch 260,  loss: 1.4077857613563538\n",
      "Batch 265,  loss: 1.487004041671753\n",
      "Batch 270,  loss: 1.4852824449539184\n",
      "Batch 275,  loss: 1.6062894582748413\n",
      "Batch 280,  loss: 1.2819848299026488\n",
      "Batch 285,  loss: 1.651697826385498\n",
      "Batch 290,  loss: 1.7381511688232423\n",
      "Batch 295,  loss: 1.6453636169433594\n",
      "Batch 300,  loss: 1.4687221050262451\n",
      "Batch 305,  loss: 1.2458426117897035\n",
      "Batch 310,  loss: 1.7343730926513672\n",
      "Batch 315,  loss: 1.820890212059021\n",
      "Batch 320,  loss: 1.4671427965164185\n",
      "Batch 325,  loss: 1.392368245124817\n",
      "Batch 330,  loss: 1.5729718208312988\n",
      "Batch 335,  loss: 1.3005852699279785\n",
      "Batch 340,  loss: 1.4370175361633302\n",
      "Batch 345,  loss: 1.691883635520935\n",
      "Batch 350,  loss: 1.5999903440475465\n",
      "Batch 355,  loss: 1.6436445713043213\n",
      "Batch 360,  loss: 1.330304789543152\n",
      "Batch 365,  loss: 1.4054752349853517\n",
      "Batch 370,  loss: 1.7292134046554566\n",
      "Batch 375,  loss: 1.4935213804244996\n",
      "Batch 380,  loss: 1.773494005203247\n",
      "Batch 385,  loss: 1.4711761713027953\n",
      "Batch 390,  loss: 1.5730342626571656\n",
      "Batch 395,  loss: 1.4061633825302124\n",
      "Batch 400,  loss: 1.4075825929641723\n",
      "Batch 405,  loss: 1.3324147582054138\n",
      "Batch 410,  loss: 1.2734877467155457\n",
      "Batch 415,  loss: 1.2562221527099608\n",
      "Batch 420,  loss: 1.6346921920776367\n",
      "Batch 425,  loss: 2.0244293928146364\n",
      "Batch 430,  loss: 1.6936571836471557\n",
      "Batch 435,  loss: 1.462110424041748\n",
      "Batch 440,  loss: 1.4256506204605102\n",
      "Batch 445,  loss: 1.867705798149109\n",
      "Batch 450,  loss: 1.544545793533325\n",
      "Batch 455,  loss: 1.4459114909172057\n",
      "Batch 460,  loss: 1.53606173992157\n",
      "Batch 465,  loss: 2.061228084564209\n",
      "Batch 470,  loss: 1.307970643043518\n",
      "Batch 475,  loss: 1.2996638298034668\n",
      "Batch 480,  loss: 1.6616804599761963\n",
      "Batch 485,  loss: 1.5919908046722413\n",
      "Batch 490,  loss: 1.3899556517601013\n",
      "Batch 495,  loss: 1.5119004011154176\n",
      "Batch 500,  loss: 1.808559536933899\n",
      "Batch 505,  loss: 1.3989970445632935\n",
      "Batch 510,  loss: 1.5899347424507142\n",
      "Batch 515,  loss: 1.7090423583984375\n",
      "Batch 520,  loss: 1.5970499992370606\n",
      "Batch 525,  loss: 1.5721843481063842\n",
      "Batch 530,  loss: 1.4310442924499511\n",
      "Batch 535,  loss: 1.6669255375862122\n",
      "Batch 540,  loss: 1.486699914932251\n",
      "Batch 545,  loss: 1.394586968421936\n",
      "Batch 550,  loss: 1.4106634855270386\n",
      "Batch 555,  loss: 1.654702615737915\n",
      "Batch 560,  loss: 1.5627617120742798\n",
      "Batch 565,  loss: 1.4207495212554933\n",
      "Batch 570,  loss: 1.5469367027282714\n",
      "Batch 575,  loss: 1.6360854148864745\n",
      "Batch 580,  loss: 1.536466360092163\n",
      "Batch 585,  loss: 1.5652557373046876\n",
      "Batch 590,  loss: 1.8256099462509154\n",
      "Batch 595,  loss: 1.5187472701072693\n",
      "Batch 600,  loss: 1.8258953809738159\n",
      "Batch 605,  loss: 1.350644826889038\n",
      "Batch 610,  loss: 1.696070957183838\n",
      "Batch 615,  loss: 1.5777678489685059\n",
      "Batch 620,  loss: 1.5617430448532104\n",
      "Batch 625,  loss: 1.8418338537216186\n",
      "Batch 630,  loss: 1.5259007215499878\n",
      "Batch 635,  loss: 1.5433804750442506\n",
      "Batch 640,  loss: 1.4426912307739257\n",
      "Batch 645,  loss: 1.7983267068862916\n",
      "Batch 650,  loss: 1.2942904949188232\n",
      "Batch 655,  loss: 1.361449933052063\n",
      "Batch 660,  loss: 1.1631930232048036\n",
      "Batch 665,  loss: 1.2114542484283448\n",
      "Batch 670,  loss: 1.5570781469345092\n",
      "Batch 675,  loss: 1.9468974113464355\n",
      "Batch 680,  loss: 1.421712303161621\n",
      "Batch 685,  loss: 1.4880935311317445\n",
      "Batch 690,  loss: 1.6393811464309693\n",
      "Batch 695,  loss: 1.1343931913375855\n",
      "Batch 700,  loss: 1.6471812009811402\n",
      "Batch 705,  loss: 1.4946868181228639\n",
      "Batch 710,  loss: 1.3718569993972778\n",
      "Batch 715,  loss: 1.562256407737732\n",
      "Batch 720,  loss: 1.270738387107849\n",
      "Batch 725,  loss: 1.5750802040100098\n",
      "Batch 730,  loss: 1.6178357362747193\n",
      "Batch 735,  loss: 1.4366536617279053\n",
      "Batch 740,  loss: 1.4626644134521485\n",
      "Batch 745,  loss: 1.3861655950546266\n",
      "Batch 750,  loss: 1.6919495105743407\n",
      "Batch 755,  loss: 1.3263230323791504\n",
      "Batch 760,  loss: 1.5551178455352783\n",
      "Batch 765,  loss: 1.3987893223762513\n",
      "Batch 770,  loss: 1.5650212287902832\n",
      "Batch 775,  loss: 1.5077135801315307\n",
      "Batch 780,  loss: 1.3194505214691161\n",
      "Batch 785,  loss: 1.6783400535583497\n",
      "Batch 790,  loss: 1.669695830345154\n",
      "Batch 795,  loss: 1.282235562801361\n",
      "Batch 800,  loss: 1.6581522941589355\n",
      "Batch 805,  loss: 1.525911569595337\n",
      "Batch 810,  loss: 1.4864084482192994\n",
      "Batch 815,  loss: 1.2018867015838623\n",
      "Batch 820,  loss: 1.612198007106781\n",
      "Batch 825,  loss: 1.5525274753570557\n",
      "Batch 830,  loss: 1.5170578002929687\n",
      "Batch 835,  loss: 1.248583722114563\n",
      "Batch 840,  loss: 1.5635333776473999\n",
      "Batch 845,  loss: 1.3182994842529296\n",
      "Batch 850,  loss: 1.4251489043235779\n",
      "Batch 855,  loss: 1.4775587797164917\n",
      "Batch 860,  loss: 1.263506019115448\n",
      "Batch 865,  loss: 1.6323665142059327\n",
      "Batch 870,  loss: 1.4741684436798095\n",
      "Batch 875,  loss: 1.6721436977386475\n",
      "Batch 880,  loss: 1.561526608467102\n",
      "Batch 885,  loss: 1.3237386703491212\n",
      "Batch 890,  loss: 1.5646156072616577\n",
      "Batch 895,  loss: 1.2171871185302734\n",
      "Batch 900,  loss: 1.395104432106018\n",
      "Batch 905,  loss: 1.584351122379303\n",
      "Batch 910,  loss: 1.566456174850464\n",
      "Batch 915,  loss: 1.6460448026657104\n",
      "Batch 920,  loss: 1.3819410800933838\n",
      "Batch 925,  loss: 1.59337637424469\n",
      "Batch 930,  loss: 1.679230523109436\n",
      "Batch 935,  loss: 1.123049545288086\n",
      "Batch 940,  loss: 1.2992116928100585\n",
      "Batch 945,  loss: 1.921653437614441\n",
      "Batch 950,  loss: 1.3599026918411254\n",
      "Batch 955,  loss: 1.8995080947875977\n",
      "Batch 960,  loss: 1.6329153776168823\n",
      "Batch 965,  loss: 1.3378448128700255\n",
      "Batch 970,  loss: 1.6229834318161012\n",
      "Batch 975,  loss: 1.587723469734192\n",
      "Batch 980,  loss: 1.760942029953003\n",
      "Batch 985,  loss: 1.4607051372528077\n",
      "Batch 990,  loss: 1.462456214427948\n",
      "Batch 995,  loss: 1.7247555017471314\n",
      "Batch 1000,  loss: 1.750870680809021\n",
      "Batch 1005,  loss: 1.87514271736145\n",
      "Batch 1010,  loss: 1.5122851371765136\n",
      "Batch 1015,  loss: 1.2859446167945863\n",
      "Batch 1020,  loss: 1.4823360204696656\n",
      "Batch 1025,  loss: 1.475911593437195\n",
      "Batch 1030,  loss: 1.268813681602478\n",
      "Batch 1035,  loss: 1.259925651550293\n",
      "Batch 1040,  loss: 1.6300172328948974\n",
      "Batch 1045,  loss: 1.3238240242004395\n",
      "Batch 1050,  loss: 1.2957217216491699\n",
      "Batch 1055,  loss: 1.4281033754348755\n",
      "Batch 1060,  loss: 1.5310316562652588\n",
      "Batch 1065,  loss: 1.878081464767456\n",
      "Batch 1070,  loss: 1.728541111946106\n",
      "Batch 1075,  loss: 1.6077073574066163\n",
      "Batch 1080,  loss: 1.3772387504577637\n",
      "Batch 1085,  loss: 1.680552101135254\n",
      "Batch 1090,  loss: 1.4464473009109498\n",
      "Batch 1095,  loss: 1.4399962186813355\n",
      "Batch 1100,  loss: 1.3452954292297363\n",
      "Batch 1105,  loss: 1.4970406532287597\n",
      "Batch 1110,  loss: 1.3440127611160277\n",
      "Batch 1115,  loss: 1.321499276161194\n",
      "Batch 1120,  loss: 1.4718611240386963\n",
      "Batch 1125,  loss: 1.8714444637298584\n",
      "Batch 1130,  loss: 1.5947035074234008\n",
      "Batch 1135,  loss: 1.5538686037063598\n",
      "Batch 1140,  loss: 1.5412383317947387\n",
      "Batch 1145,  loss: 1.5054564476013184\n",
      "Batch 1150,  loss: 1.5250160694122314\n",
      "Batch 1155,  loss: 1.3237782716751099\n",
      "Batch 1160,  loss: 1.372206687927246\n",
      "Batch 1165,  loss: 1.5679703235626221\n",
      "Batch 1170,  loss: 1.398888397216797\n",
      "Batch 1175,  loss: 1.44673593044281\n",
      "Batch 1180,  loss: 1.2975875854492187\n",
      "Batch 1185,  loss: 1.4336053252220153\n",
      "Batch 1190,  loss: 1.7936845541000366\n",
      "Batch 1195,  loss: 1.5508525133132935\n",
      "Batch 1200,  loss: 1.643176555633545\n",
      "Batch 1205,  loss: 1.624119472503662\n",
      "Batch 1210,  loss: 1.3882572174072265\n",
      "Batch 1215,  loss: 1.456572699546814\n",
      "Batch 1220,  loss: 1.7507965803146361\n",
      "Batch 1225,  loss: 1.545642352104187\n",
      "Batch 1230,  loss: 1.489065933227539\n",
      "Batch 1235,  loss: 1.622793698310852\n",
      "Batch 1240,  loss: 1.4845490217208863\n",
      "Batch 1245,  loss: 1.3156310796737671\n",
      "Batch 1250,  loss: 1.4340354681015015\n",
      "Batch 1255,  loss: 1.522987723350525\n",
      "Batch 1260,  loss: 1.236175537109375\n",
      "Batch 1265,  loss: 1.3781466722488402\n",
      "Batch 1270,  loss: 1.3657219171524049\n",
      "Batch 1275,  loss: 1.3998189926147462\n",
      "Batch 1280,  loss: 1.5963889837265015\n",
      "Batch 1285,  loss: 1.5230605840682983\n",
      "Batch 1290,  loss: 1.122600519657135\n",
      "Batch 1295,  loss: 1.784488320350647\n",
      "LOSS train 1.784488320350647. Validation loss: 2.1013841867998795 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 41:\n",
      "Batch 5,  loss: 1.7273846387863159\n",
      "Batch 10,  loss: 1.4605550527572633\n",
      "Batch 15,  loss: 1.6163773775100707\n",
      "Batch 20,  loss: 1.5419407606124877\n",
      "Batch 25,  loss: 1.6351840257644654\n",
      "Batch 30,  loss: 1.8440337657928467\n",
      "Batch 35,  loss: 1.441289448738098\n",
      "Batch 40,  loss: 1.5545737743377686\n",
      "Batch 45,  loss: 1.8665967226028441\n",
      "Batch 50,  loss: 1.179709804058075\n",
      "Batch 55,  loss: 1.544468927383423\n",
      "Batch 60,  loss: 1.651932716369629\n",
      "Batch 65,  loss: 1.4438788294792175\n",
      "Batch 70,  loss: 1.5083904147148133\n",
      "Batch 75,  loss: 1.854319190979004\n",
      "Batch 80,  loss: 1.501286005973816\n",
      "Batch 85,  loss: 1.5836716413497924\n",
      "Batch 90,  loss: 1.3474488496780395\n",
      "Batch 95,  loss: 1.4958790421485901\n",
      "Batch 100,  loss: 1.3717178106307983\n",
      "Batch 105,  loss: 1.4731281280517579\n",
      "Batch 110,  loss: 1.2854240894317628\n",
      "Batch 115,  loss: 1.6424968004226685\n",
      "Batch 120,  loss: 1.5122992992401123\n",
      "Batch 125,  loss: 1.3855472803115845\n",
      "Batch 130,  loss: 1.5079288005828857\n",
      "Batch 135,  loss: 1.3424570322036744\n",
      "Batch 140,  loss: 1.4761486053466797\n",
      "Batch 145,  loss: 1.3861189126968383\n",
      "Batch 150,  loss: 1.6289583921432496\n",
      "Batch 155,  loss: 1.4968969106674195\n",
      "Batch 160,  loss: 1.2399617910385132\n",
      "Batch 165,  loss: 1.4407627940177918\n",
      "Batch 170,  loss: 1.7062028169631958\n",
      "Batch 175,  loss: 1.6262355327606202\n",
      "Batch 180,  loss: 1.0765907764434814\n",
      "Batch 185,  loss: 1.6704124450683593\n",
      "Batch 190,  loss: 1.281966471672058\n",
      "Batch 195,  loss: 1.3224377751350402\n",
      "Batch 200,  loss: 1.4632450819015503\n",
      "Batch 205,  loss: 1.5441174983978272\n",
      "Batch 210,  loss: 1.2806643843650818\n",
      "Batch 215,  loss: 1.4285329580307007\n",
      "Batch 220,  loss: 1.4351938724517823\n",
      "Batch 225,  loss: 1.262338674068451\n",
      "Batch 230,  loss: 1.2662182331085206\n",
      "Batch 235,  loss: 1.5878735065460206\n",
      "Batch 240,  loss: 1.5160823106765746\n",
      "Batch 245,  loss: 1.346262240409851\n",
      "Batch 250,  loss: 1.4234970569610597\n",
      "Batch 255,  loss: 1.3544721603393555\n",
      "Batch 260,  loss: 1.5246874332427978\n",
      "Batch 265,  loss: 1.6157294988632203\n",
      "Batch 270,  loss: 1.652929949760437\n",
      "Batch 275,  loss: 1.7797736406326294\n",
      "Batch 280,  loss: 1.575482439994812\n",
      "Batch 285,  loss: 1.324135136604309\n",
      "Batch 290,  loss: 1.3832252264022826\n",
      "Batch 295,  loss: 1.485768938064575\n",
      "Batch 300,  loss: 1.429350781440735\n",
      "Batch 305,  loss: 1.5883132934570312\n",
      "Batch 310,  loss: 1.4348426580429077\n",
      "Batch 315,  loss: 1.4762676239013672\n",
      "Batch 320,  loss: 1.202002191543579\n",
      "Batch 325,  loss: 1.423926091194153\n",
      "Batch 330,  loss: 1.6480401039123536\n",
      "Batch 335,  loss: 1.4785786867141724\n",
      "Batch 340,  loss: 1.4439645767211915\n",
      "Batch 345,  loss: 1.7342587947845458\n",
      "Batch 350,  loss: 1.6464434623718263\n",
      "Batch 355,  loss: 1.293023157119751\n",
      "Batch 360,  loss: 1.4328080892562867\n",
      "Batch 365,  loss: 1.6250389337539672\n",
      "Batch 370,  loss: 1.549102282524109\n",
      "Batch 375,  loss: 1.4207382202148438\n",
      "Batch 380,  loss: 1.5689631223678588\n",
      "Batch 385,  loss: 1.4249721050262452\n",
      "Batch 390,  loss: 1.383770227432251\n",
      "Batch 395,  loss: 1.5184321880340577\n",
      "Batch 400,  loss: 1.4794151425361632\n",
      "Batch 405,  loss: 1.7842003107070923\n",
      "Batch 410,  loss: 1.3546267986297607\n",
      "Batch 415,  loss: 1.5284334421157837\n",
      "Batch 420,  loss: 1.4508181095123291\n",
      "Batch 425,  loss: 1.3753633737564086\n",
      "Batch 430,  loss: 1.425571870803833\n",
      "Batch 435,  loss: 1.6328959941864014\n",
      "Batch 440,  loss: 1.5684358596801757\n",
      "Batch 445,  loss: 1.6414869546890258\n",
      "Batch 450,  loss: 1.265637421607971\n",
      "Batch 455,  loss: 1.3654595136642456\n",
      "Batch 460,  loss: 1.437324833869934\n",
      "Batch 465,  loss: 1.484677243232727\n",
      "Batch 470,  loss: 1.515756356716156\n",
      "Batch 475,  loss: 1.485763168334961\n",
      "Batch 480,  loss: 1.2815829873085023\n",
      "Batch 485,  loss: 1.5451016426086426\n",
      "Batch 490,  loss: 1.5407452583312988\n",
      "Batch 495,  loss: 1.4517554759979248\n",
      "Batch 500,  loss: 1.4080167770385743\n",
      "Batch 505,  loss: 1.497943902015686\n",
      "Batch 510,  loss: 1.5444100856781007\n",
      "Batch 515,  loss: 1.8056383371353149\n",
      "Batch 520,  loss: 1.0714639902114869\n",
      "Batch 525,  loss: 1.6572471618652345\n",
      "Batch 530,  loss: 1.5035294771194458\n",
      "Batch 535,  loss: 1.4016454458236693\n",
      "Batch 540,  loss: 1.5677377223968505\n",
      "Batch 545,  loss: 1.6807096004486084\n",
      "Batch 550,  loss: 1.6572321653366089\n",
      "Batch 555,  loss: 1.385819411277771\n",
      "Batch 560,  loss: 1.32642662525177\n",
      "Batch 565,  loss: 1.366058921813965\n",
      "Batch 570,  loss: 1.6653281211853028\n",
      "Batch 575,  loss: 1.4897552013397217\n",
      "Batch 580,  loss: 1.4855272889137268\n",
      "Batch 585,  loss: 1.6512113809585571\n",
      "Batch 590,  loss: 1.7594087600708008\n",
      "Batch 595,  loss: 1.6129877805709838\n",
      "Batch 600,  loss: 1.6032519817352295\n",
      "Batch 605,  loss: 1.4468469142913818\n",
      "Batch 610,  loss: 1.2976240396499634\n",
      "Batch 615,  loss: 1.4634067058563232\n",
      "Batch 620,  loss: 1.7648158311843871\n",
      "Batch 625,  loss: 1.5970119833946228\n",
      "Batch 630,  loss: 1.3426156997680665\n",
      "Batch 635,  loss: 1.54393470287323\n",
      "Batch 640,  loss: 1.4986016511917115\n",
      "Batch 645,  loss: 1.2301173448562621\n",
      "Batch 650,  loss: 1.7298145532608031\n",
      "Batch 655,  loss: 1.549875020980835\n",
      "Batch 660,  loss: 1.4043319463729858\n",
      "Batch 665,  loss: 1.3950017929077148\n",
      "Batch 670,  loss: 1.6486052513122558\n",
      "Batch 675,  loss: 1.6058233261108399\n",
      "Batch 680,  loss: 1.4388270139694215\n",
      "Batch 685,  loss: 1.4270807266235352\n",
      "Batch 690,  loss: 1.6918556451797486\n",
      "Batch 695,  loss: 1.7127460837364197\n",
      "Batch 700,  loss: 1.3441016674041748\n",
      "Batch 705,  loss: 2.0872205972671507\n",
      "Batch 710,  loss: 1.6458961725234986\n",
      "Batch 715,  loss: 1.5802252054214478\n",
      "Batch 720,  loss: 1.439068567752838\n",
      "Batch 725,  loss: 1.56539146900177\n",
      "Batch 730,  loss: 1.8246572017669678\n",
      "Batch 735,  loss: 1.5749635219573974\n",
      "Batch 740,  loss: 1.3210235953330993\n",
      "Batch 745,  loss: 1.447544026374817\n",
      "Batch 750,  loss: 1.4686274766921996\n",
      "Batch 755,  loss: 1.4696914076805114\n",
      "Batch 760,  loss: 1.4107067108154296\n",
      "Batch 765,  loss: 1.333274495601654\n",
      "Batch 770,  loss: 1.7523075103759767\n",
      "Batch 775,  loss: 1.429866647720337\n",
      "Batch 780,  loss: 1.500850009918213\n",
      "Batch 785,  loss: 1.5962597131729126\n",
      "Batch 790,  loss: 1.159307861328125\n",
      "Batch 795,  loss: 1.6590684294700622\n",
      "Batch 800,  loss: 1.229388129711151\n",
      "Batch 805,  loss: 1.4221742868423461\n",
      "Batch 810,  loss: 1.3925522089004516\n",
      "Batch 815,  loss: 1.571750235557556\n",
      "Batch 820,  loss: 1.5623911142349243\n",
      "Batch 825,  loss: 1.5007829666137695\n",
      "Batch 830,  loss: 1.540006947517395\n",
      "Batch 835,  loss: 1.3479164600372315\n",
      "Batch 840,  loss: 1.327195906639099\n",
      "Batch 845,  loss: 1.5135305166244506\n",
      "Batch 850,  loss: 1.51034734249115\n",
      "Batch 855,  loss: 1.435517954826355\n",
      "Batch 860,  loss: 1.431448221206665\n",
      "Batch 865,  loss: 1.2450300693511962\n",
      "Batch 870,  loss: 1.1925344467163086\n",
      "Batch 875,  loss: 1.251257586479187\n",
      "Batch 880,  loss: 1.2512935400009155\n",
      "Batch 885,  loss: 1.4880143761634828\n",
      "Batch 890,  loss: 1.3656120538711547\n",
      "Batch 895,  loss: 1.793508768081665\n",
      "Batch 900,  loss: 1.2301702737808227\n",
      "Batch 905,  loss: 1.2826277017593384\n",
      "Batch 910,  loss: 1.6375024795532227\n",
      "Batch 915,  loss: 1.3218828201293946\n",
      "Batch 920,  loss: 1.4338994026184082\n",
      "Batch 925,  loss: 1.564300274848938\n",
      "Batch 930,  loss: 1.2753517270088195\n",
      "Batch 935,  loss: 1.3518340110778808\n",
      "Batch 940,  loss: 1.6672562122344972\n",
      "Batch 945,  loss: 1.433747160434723\n",
      "Batch 950,  loss: 1.185593068599701\n",
      "Batch 955,  loss: 1.390921449661255\n",
      "Batch 960,  loss: 1.7519744873046874\n",
      "Batch 965,  loss: 1.2952159881591796\n",
      "Batch 970,  loss: 1.59961998462677\n",
      "Batch 975,  loss: 1.40781512260437\n",
      "Batch 980,  loss: 1.4451291799545287\n",
      "Batch 985,  loss: 1.511829662322998\n",
      "Batch 990,  loss: 1.3705238819122314\n",
      "Batch 995,  loss: 1.4764362812042235\n",
      "Batch 1000,  loss: 1.1637480020523072\n",
      "Batch 1005,  loss: 1.543929922580719\n",
      "Batch 1010,  loss: 1.4719098567962647\n",
      "Batch 1015,  loss: 1.2903955340385438\n",
      "Batch 1020,  loss: 1.4561861276626586\n",
      "Batch 1025,  loss: 1.4353062152862548\n",
      "Batch 1030,  loss: 1.5572751045227051\n",
      "Batch 1035,  loss: 1.4222986459732057\n",
      "Batch 1040,  loss: 1.8035677909851073\n",
      "Batch 1045,  loss: 1.5451589107513428\n",
      "Batch 1050,  loss: 1.4605313301086427\n",
      "Batch 1055,  loss: 1.5556963205337524\n",
      "Batch 1060,  loss: 1.627385640144348\n",
      "Batch 1065,  loss: 1.5775179386138916\n",
      "Batch 1070,  loss: 1.3099107265472412\n",
      "Batch 1075,  loss: 1.5333776950836182\n",
      "Batch 1080,  loss: 1.6622414112091064\n",
      "Batch 1085,  loss: 1.204684352874756\n",
      "Batch 1090,  loss: 1.224232017993927\n",
      "Batch 1095,  loss: 1.5263035774230957\n",
      "Batch 1100,  loss: 1.553753709793091\n",
      "Batch 1105,  loss: 1.5237358808517456\n",
      "Batch 1110,  loss: 1.5912246465682984\n",
      "Batch 1115,  loss: 1.4755711078643798\n",
      "Batch 1120,  loss: 1.4091147422790526\n",
      "Batch 1125,  loss: 1.0555448055267334\n",
      "Batch 1130,  loss: 1.4997740507125854\n",
      "Batch 1135,  loss: 1.4365499496459961\n",
      "Batch 1140,  loss: 1.4923561334609985\n",
      "Batch 1145,  loss: 1.5100869178771972\n",
      "Batch 1150,  loss: 1.9205726623535155\n",
      "Batch 1155,  loss: 1.6266422271728516\n",
      "Batch 1160,  loss: 1.4067880153656005\n",
      "Batch 1165,  loss: 1.4241214752197267\n",
      "Batch 1170,  loss: 1.3766643524169921\n",
      "Batch 1175,  loss: 1.7320884466171265\n",
      "Batch 1180,  loss: 1.269190526008606\n",
      "Batch 1185,  loss: 1.6994073390960693\n",
      "Batch 1190,  loss: 1.483392858505249\n",
      "Batch 1195,  loss: 1.3928841590881347\n",
      "Batch 1200,  loss: 1.6181630849838258\n",
      "Batch 1205,  loss: 1.7131699323654175\n",
      "Batch 1210,  loss: 1.3753934621810913\n",
      "Batch 1215,  loss: 1.6104679346084594\n",
      "Batch 1220,  loss: 1.4605492115020753\n",
      "Batch 1225,  loss: 1.507785964012146\n",
      "Batch 1230,  loss: 1.6997863531112671\n",
      "Batch 1235,  loss: 1.4879596948623657\n",
      "Batch 1240,  loss: 1.4268484115600586\n",
      "Batch 1245,  loss: 1.3528982043266295\n",
      "Batch 1250,  loss: 1.3646193385124206\n",
      "Batch 1255,  loss: 1.551918363571167\n",
      "Batch 1260,  loss: 1.2067049384117126\n",
      "Batch 1265,  loss: 1.2749128818511963\n",
      "Batch 1270,  loss: 1.2697994589805603\n",
      "Batch 1275,  loss: 1.8728969931602477\n",
      "Batch 1280,  loss: 1.291568350791931\n",
      "Batch 1285,  loss: 1.3615344285964965\n",
      "Batch 1290,  loss: 1.6869853019714356\n",
      "Batch 1295,  loss: 1.4326627612113954\n",
      "LOSS train 1.4326627612113954. Validation loss: 2.029447874923547 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 42:\n",
      "Batch 5,  loss: 2.0473303318023683\n",
      "Batch 10,  loss: 1.511113452911377\n",
      "Batch 15,  loss: 1.4261900901794433\n",
      "Batch 20,  loss: 1.5860394477844237\n",
      "Batch 25,  loss: 1.5345948696136475\n",
      "Batch 30,  loss: 1.4492613077163696\n",
      "Batch 35,  loss: 1.4254739999771118\n",
      "Batch 40,  loss: 1.5573609590530395\n",
      "Batch 45,  loss: 1.3867728233337402\n",
      "Batch 50,  loss: 1.4981422901153565\n",
      "Batch 55,  loss: 1.690460729598999\n",
      "Batch 60,  loss: 1.2460612773895263\n",
      "Batch 65,  loss: 1.1672431468963622\n",
      "Batch 70,  loss: 1.0282925128936768\n",
      "Batch 75,  loss: 1.3872396349906921\n",
      "Batch 80,  loss: 1.4503936767578125\n",
      "Batch 85,  loss: 1.3708197116851806\n",
      "Batch 90,  loss: 1.6910515308380127\n",
      "Batch 95,  loss: 1.766940999031067\n",
      "Batch 100,  loss: 1.56001935005188\n",
      "Batch 105,  loss: 1.5090888023376465\n",
      "Batch 110,  loss: 1.3219601154327392\n",
      "Batch 115,  loss: 1.4255879163742065\n",
      "Batch 120,  loss: 1.438945746421814\n",
      "Batch 125,  loss: 1.550885558128357\n",
      "Batch 130,  loss: 1.4763584852218627\n",
      "Batch 135,  loss: 1.52984938621521\n",
      "Batch 140,  loss: 1.2609893321990966\n",
      "Batch 145,  loss: 1.4071135997772217\n",
      "Batch 150,  loss: 1.4260821342468262\n",
      "Batch 155,  loss: 1.3687548637390137\n",
      "Batch 160,  loss: 1.4464401483535767\n",
      "Batch 165,  loss: 1.6018784523010254\n",
      "Batch 170,  loss: 1.904115653038025\n",
      "Batch 175,  loss: 1.5009118795394898\n",
      "Batch 180,  loss: 1.4944320440292358\n",
      "Batch 185,  loss: 1.6453619956970216\n",
      "Batch 190,  loss: 1.7565705537796021\n",
      "Batch 195,  loss: 1.3741977214813232\n",
      "Batch 200,  loss: 1.2925495386123658\n",
      "Batch 205,  loss: 1.3485405445098877\n",
      "Batch 210,  loss: 1.5626588344573975\n",
      "Batch 215,  loss: 1.2711341857910157\n",
      "Batch 220,  loss: 1.7686944961547852\n",
      "Batch 225,  loss: 1.4208238124847412\n",
      "Batch 230,  loss: 1.6314790725708008\n",
      "Batch 235,  loss: 1.3543014049530029\n",
      "Batch 240,  loss: 1.251531982421875\n",
      "Batch 245,  loss: 1.4710399866104127\n",
      "Batch 250,  loss: 1.3545912623405456\n",
      "Batch 255,  loss: 1.286536490917206\n",
      "Batch 260,  loss: 1.5303327083587646\n",
      "Batch 265,  loss: 1.212406039237976\n",
      "Batch 270,  loss: 1.7379647254943849\n",
      "Batch 275,  loss: 1.5858503580093384\n",
      "Batch 280,  loss: 1.7397922039031983\n",
      "Batch 285,  loss: 1.482166337966919\n",
      "Batch 290,  loss: 1.311597180366516\n",
      "Batch 295,  loss: 1.1000652313232422\n",
      "Batch 300,  loss: 1.6749938011169434\n",
      "Batch 305,  loss: 1.4141856193542481\n",
      "Batch 310,  loss: 1.6267033815383911\n",
      "Batch 315,  loss: 1.4045261859893798\n",
      "Batch 320,  loss: 1.3755133390426635\n",
      "Batch 325,  loss: 1.6239638328552246\n",
      "Batch 330,  loss: 1.5573143005371093\n",
      "Batch 335,  loss: 1.3394476652145386\n",
      "Batch 340,  loss: 1.4610197067260742\n",
      "Batch 345,  loss: 1.3680211782455445\n",
      "Batch 350,  loss: 1.882734513282776\n",
      "Batch 355,  loss: 1.6382314443588257\n",
      "Batch 360,  loss: 1.6324291467666625\n",
      "Batch 365,  loss: 1.4027922391891479\n",
      "Batch 370,  loss: 1.2651927947998047\n",
      "Batch 375,  loss: 1.4413889169692993\n",
      "Batch 380,  loss: 1.3483599185943604\n",
      "Batch 385,  loss: 1.672763752937317\n",
      "Batch 390,  loss: 1.556056308746338\n",
      "Batch 395,  loss: 1.371075940132141\n",
      "Batch 400,  loss: 1.594169521331787\n",
      "Batch 405,  loss: 1.3661876797676087\n",
      "Batch 410,  loss: 1.483097767829895\n",
      "Batch 415,  loss: 2.212661957740784\n",
      "Batch 420,  loss: 1.4566682577133179\n",
      "Batch 425,  loss: 1.3411598920822143\n",
      "Batch 430,  loss: 1.5450995206832885\n",
      "Batch 435,  loss: 1.2160922288894653\n",
      "Batch 440,  loss: 1.4188364028930665\n",
      "Batch 445,  loss: 1.1659987807273864\n",
      "Batch 450,  loss: 1.5522598147392273\n",
      "Batch 455,  loss: 1.9081831216812133\n",
      "Batch 460,  loss: 1.4443655490875245\n",
      "Batch 465,  loss: 1.2729312658309937\n",
      "Batch 470,  loss: 1.637757921218872\n",
      "Batch 475,  loss: 1.4193034172058105\n",
      "Batch 480,  loss: 1.3011120796203612\n",
      "Batch 485,  loss: 1.6873077630996705\n",
      "Batch 490,  loss: 1.5045379161834718\n",
      "Batch 495,  loss: 1.64851815700531\n",
      "Batch 500,  loss: 1.0802363514900208\n",
      "Batch 505,  loss: 1.699784779548645\n",
      "Batch 510,  loss: 1.5198805689811707\n",
      "Batch 515,  loss: 1.880864953994751\n",
      "Batch 520,  loss: 1.3453433036804199\n",
      "Batch 525,  loss: 1.7669278383255005\n",
      "Batch 530,  loss: 1.5417778015136718\n",
      "Batch 535,  loss: 1.448895239830017\n",
      "Batch 540,  loss: 1.644377303123474\n",
      "Batch 545,  loss: 2.116189980506897\n",
      "Batch 550,  loss: 1.436905527114868\n",
      "Batch 555,  loss: 1.3430313110351562\n",
      "Batch 560,  loss: 1.601851725578308\n",
      "Batch 565,  loss: 1.7306964635848998\n",
      "Batch 570,  loss: 1.3054075717926026\n",
      "Batch 575,  loss: 1.83057644367218\n",
      "Batch 580,  loss: 1.4478538751602172\n",
      "Batch 585,  loss: 1.2886015057563782\n",
      "Batch 590,  loss: 1.3849519968032837\n",
      "Batch 595,  loss: 1.7158464431762694\n",
      "Batch 600,  loss: 1.8047972679138184\n",
      "Batch 605,  loss: 1.5416016578674316\n",
      "Batch 610,  loss: 1.5260354995727539\n",
      "Batch 615,  loss: 1.3481245517730713\n",
      "Batch 620,  loss: 1.7752953052520752\n",
      "Batch 625,  loss: 1.8876718521118163\n",
      "Batch 630,  loss: 1.704000210762024\n",
      "Batch 635,  loss: 1.8556845903396606\n",
      "Batch 640,  loss: 1.6043230295181274\n",
      "Batch 645,  loss: 1.5606666564941407\n",
      "Batch 650,  loss: 1.3124798774719237\n",
      "Batch 655,  loss: 1.3284645795822143\n",
      "Batch 660,  loss: 1.637169361114502\n",
      "Batch 665,  loss: 1.388218879699707\n",
      "Batch 670,  loss: 1.5591763973236084\n",
      "Batch 675,  loss: 1.3743947982788085\n",
      "Batch 680,  loss: 1.3961177110671996\n",
      "Batch 685,  loss: 1.2155816912651063\n",
      "Batch 690,  loss: 1.7810881376266479\n",
      "Batch 695,  loss: 1.280058002471924\n",
      "Batch 700,  loss: 1.5377156257629394\n",
      "Batch 705,  loss: 1.324454164505005\n",
      "Batch 710,  loss: 1.5819950580596924\n",
      "Batch 715,  loss: 1.3365595579147338\n",
      "Batch 720,  loss: 1.492163848876953\n",
      "Batch 725,  loss: 1.1320611715316773\n",
      "Batch 730,  loss: 1.546424674987793\n",
      "Batch 735,  loss: 1.9370384216308594\n",
      "Batch 740,  loss: 1.504084062576294\n",
      "Batch 745,  loss: 1.6925984859466552\n",
      "Batch 750,  loss: 1.2756500482559203\n",
      "Batch 755,  loss: 1.2486828088760376\n",
      "Batch 760,  loss: 1.3871376276016236\n",
      "Batch 765,  loss: 1.328710389137268\n",
      "Batch 770,  loss: 1.3505911827087402\n",
      "Batch 775,  loss: 1.4022369742393495\n",
      "Batch 780,  loss: 1.5174092769622802\n",
      "Batch 785,  loss: 1.2288861274719238\n",
      "Batch 790,  loss: 1.2549406051635743\n",
      "Batch 795,  loss: 1.779434370994568\n",
      "Batch 800,  loss: 1.1832602620124817\n",
      "Batch 805,  loss: 1.4299689531326294\n",
      "Batch 810,  loss: 1.6176981210708619\n",
      "Batch 815,  loss: 1.612249982357025\n",
      "Batch 820,  loss: 1.4498023748397828\n",
      "Batch 825,  loss: 1.3808321952819824\n",
      "Batch 830,  loss: 1.1763702034950256\n",
      "Batch 835,  loss: 1.4705533266067505\n",
      "Batch 840,  loss: 1.6522415399551391\n",
      "Batch 845,  loss: 1.3750150799751282\n",
      "Batch 850,  loss: 1.4447919368743896\n",
      "Batch 855,  loss: 1.484098482131958\n",
      "Batch 860,  loss: 1.2538507223129272\n",
      "Batch 865,  loss: 2.0048367500305178\n",
      "Batch 870,  loss: 1.321845829486847\n",
      "Batch 875,  loss: 1.3277353763580322\n",
      "Batch 880,  loss: 1.7187831401824951\n",
      "Batch 885,  loss: 1.5382034063339234\n",
      "Batch 890,  loss: 1.5752112865447998\n",
      "Batch 895,  loss: 1.3015893936157226\n",
      "Batch 900,  loss: 1.4884337782859802\n",
      "Batch 905,  loss: 1.3892759799957275\n",
      "Batch 910,  loss: 1.50075044631958\n",
      "Batch 915,  loss: 1.2028943061828614\n",
      "Batch 920,  loss: 1.466157817840576\n",
      "Batch 925,  loss: 1.6698114633560182\n",
      "Batch 930,  loss: 1.4398257970809936\n",
      "Batch 935,  loss: 1.5552046060562135\n",
      "Batch 940,  loss: 1.4927491664886474\n",
      "Batch 945,  loss: 1.9003418207168579\n",
      "Batch 950,  loss: 1.4626528024673462\n",
      "Batch 955,  loss: 1.4299440860748291\n",
      "Batch 960,  loss: 1.5372037887573242\n",
      "Batch 965,  loss: 1.8349984407424926\n",
      "Batch 970,  loss: 1.6515781879425049\n",
      "Batch 975,  loss: 1.3271602511405944\n",
      "Batch 980,  loss: 1.358645498752594\n",
      "Batch 985,  loss: 1.4856693506240846\n",
      "Batch 990,  loss: 1.7546360492706299\n",
      "Batch 995,  loss: 1.4003645658493042\n",
      "Batch 1000,  loss: 1.4076492071151734\n",
      "Batch 1005,  loss: 1.5509961605072022\n",
      "Batch 1010,  loss: 1.547177505493164\n",
      "Batch 1015,  loss: 1.683740210533142\n",
      "Batch 1020,  loss: 1.357712495326996\n",
      "Batch 1025,  loss: 1.901688551902771\n",
      "Batch 1030,  loss: 1.2542396306991577\n",
      "Batch 1035,  loss: 1.4351647615432739\n",
      "Batch 1040,  loss: 1.5045668601989746\n",
      "Batch 1045,  loss: 1.489415693283081\n",
      "Batch 1050,  loss: 1.5468471050262451\n",
      "Batch 1055,  loss: 1.7607776880264283\n",
      "Batch 1060,  loss: 1.1577764868736267\n",
      "Batch 1065,  loss: 1.3450912237167358\n",
      "Batch 1070,  loss: 1.4549206733703612\n",
      "Batch 1075,  loss: 1.3025707960128785\n",
      "Batch 1080,  loss: 1.271820604801178\n",
      "Batch 1085,  loss: 1.2823842763900757\n",
      "Batch 1090,  loss: 1.3402311325073242\n",
      "Batch 1095,  loss: 1.2999017238616943\n",
      "Batch 1100,  loss: 1.5168497085571289\n",
      "Batch 1105,  loss: 1.5137010097503663\n",
      "Batch 1110,  loss: 1.3916536808013915\n",
      "Batch 1115,  loss: 1.2647740602493287\n",
      "Batch 1120,  loss: 1.5342818737030028\n",
      "Batch 1125,  loss: 1.2229835987091064\n",
      "Batch 1130,  loss: 1.5252082824707032\n",
      "Batch 1135,  loss: 1.5412240028381348\n",
      "Batch 1140,  loss: 1.4825096368789672\n",
      "Batch 1145,  loss: 1.2407974600791931\n",
      "Batch 1150,  loss: 1.4343727350234985\n",
      "Batch 1155,  loss: 1.9951204776763916\n",
      "Batch 1160,  loss: 1.8163855075836182\n",
      "Batch 1165,  loss: 1.8084427118301392\n",
      "Batch 1170,  loss: 1.373195767402649\n",
      "Batch 1175,  loss: 1.3938536405563355\n",
      "Batch 1180,  loss: 1.4691258430480958\n",
      "Batch 1185,  loss: 1.1629844307899475\n",
      "Batch 1190,  loss: 1.2745729804039\n",
      "Batch 1195,  loss: 1.6246541500091554\n",
      "Batch 1200,  loss: 1.5253217458724975\n",
      "Batch 1205,  loss: 1.3081223964691162\n",
      "Batch 1210,  loss: 1.933670973777771\n",
      "Batch 1215,  loss: 1.6222858667373656\n",
      "Batch 1220,  loss: 1.840534234046936\n",
      "Batch 1225,  loss: 1.3340622425079345\n",
      "Batch 1230,  loss: 1.497760009765625\n",
      "Batch 1235,  loss: 1.5278234004974365\n",
      "Batch 1240,  loss: 1.3731297731399537\n",
      "Batch 1245,  loss: 1.4708143472671509\n",
      "Batch 1250,  loss: 1.5619634866714478\n",
      "Batch 1255,  loss: 1.5855964183807374\n",
      "Batch 1260,  loss: 1.5184021711349487\n",
      "Batch 1265,  loss: 1.3621025800704956\n",
      "Batch 1270,  loss: 1.613180112838745\n",
      "Batch 1275,  loss: 1.277337634563446\n",
      "Batch 1280,  loss: 1.353816843032837\n",
      "Batch 1285,  loss: 1.503339958190918\n",
      "Batch 1290,  loss: 2.028071701526642\n",
      "Batch 1295,  loss: 1.4978994846343994\n",
      "LOSS train 1.4978994846343994. Validation loss: 2.0917527327383008 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 43:\n",
      "Batch 5,  loss: 1.5179378032684325\n",
      "Batch 10,  loss: 2.018010640144348\n",
      "Batch 15,  loss: 1.8514644622802734\n",
      "Batch 20,  loss: 1.616592788696289\n",
      "Batch 25,  loss: 1.607997512817383\n",
      "Batch 30,  loss: 1.3288724899291993\n",
      "Batch 35,  loss: 1.758650803565979\n",
      "Batch 40,  loss: 1.4420896768569946\n",
      "Batch 45,  loss: 1.4533678531646728\n",
      "Batch 50,  loss: 1.3620855569839478\n",
      "Batch 55,  loss: 1.438172173500061\n",
      "Batch 60,  loss: 1.5100579023361207\n",
      "Batch 65,  loss: 1.314993691444397\n",
      "Batch 70,  loss: 1.3311665177345275\n",
      "Batch 75,  loss: 1.6717549085617065\n",
      "Batch 80,  loss: 1.1380818843841554\n",
      "Batch 85,  loss: 1.4104586601257325\n",
      "Batch 90,  loss: 1.2080142498016357\n",
      "Batch 95,  loss: 1.738684892654419\n",
      "Batch 100,  loss: 1.4262582302093505\n",
      "Batch 105,  loss: 1.3338831424713136\n",
      "Batch 110,  loss: 1.7324443101882934\n",
      "Batch 115,  loss: 1.417834210395813\n",
      "Batch 120,  loss: 1.3111985325813293\n",
      "Batch 125,  loss: 1.6385342597961425\n",
      "Batch 130,  loss: 1.3566259860992431\n",
      "Batch 135,  loss: 1.1480693578720094\n",
      "Batch 140,  loss: 1.4397005319595337\n",
      "Batch 145,  loss: 1.3246949195861817\n",
      "Batch 150,  loss: 1.390571904182434\n",
      "Batch 155,  loss: 1.8598601579666139\n",
      "Batch 160,  loss: 1.2955646276474\n",
      "Batch 165,  loss: 1.3947265625\n",
      "Batch 170,  loss: 1.3534220695495605\n",
      "Batch 175,  loss: 1.9175189971923827\n",
      "Batch 180,  loss: 1.6197335481643678\n",
      "Batch 185,  loss: 1.4484093189239502\n",
      "Batch 190,  loss: 1.415302586555481\n",
      "Batch 195,  loss: 1.485419797897339\n",
      "Batch 200,  loss: 1.4717389822006226\n",
      "Batch 205,  loss: 1.8646308422088622\n",
      "Batch 210,  loss: 1.6472461700439454\n",
      "Batch 215,  loss: 1.3828537464141846\n",
      "Batch 220,  loss: 1.3360724925994873\n",
      "Batch 225,  loss: 1.1211750268936158\n",
      "Batch 230,  loss: 1.629835033416748\n",
      "Batch 235,  loss: 1.6844960927963257\n",
      "Batch 240,  loss: 1.39839928150177\n",
      "Batch 245,  loss: 1.238340389728546\n",
      "Batch 250,  loss: 1.399394679069519\n",
      "Batch 255,  loss: 1.546949529647827\n",
      "Batch 260,  loss: 1.542073130607605\n",
      "Batch 265,  loss: 1.6543203830718993\n",
      "Batch 270,  loss: 1.5632063150405884\n",
      "Batch 275,  loss: 1.695826232433319\n",
      "Batch 280,  loss: 1.5056355953216554\n",
      "Batch 285,  loss: 1.6344462156295776\n",
      "Batch 290,  loss: 1.7301403522491454\n",
      "Batch 295,  loss: 1.3821141958236693\n",
      "Batch 300,  loss: 1.6442887783050537\n",
      "Batch 305,  loss: 1.731929111480713\n",
      "Batch 310,  loss: 1.5238399028778076\n",
      "Batch 315,  loss: 1.573433756828308\n",
      "Batch 320,  loss: 1.6657721996307373\n",
      "Batch 325,  loss: 1.438968253135681\n",
      "Batch 330,  loss: 1.5342909574508667\n",
      "Batch 335,  loss: 1.8175278663635255\n",
      "Batch 340,  loss: 1.488473129272461\n",
      "Batch 345,  loss: 1.3766069054603576\n",
      "Batch 350,  loss: 1.8675071001052856\n",
      "Batch 355,  loss: 1.6755236387252808\n",
      "Batch 360,  loss: 1.3900651693344117\n",
      "Batch 365,  loss: 1.2023519158363343\n",
      "Batch 370,  loss: 1.4866755485534668\n",
      "Batch 375,  loss: 1.708730363845825\n",
      "Batch 380,  loss: 1.5574337482452392\n",
      "Batch 385,  loss: 1.447723627090454\n",
      "Batch 390,  loss: 1.4813517332077026\n",
      "Batch 395,  loss: 1.654576873779297\n",
      "Batch 400,  loss: 1.2574488401412964\n",
      "Batch 405,  loss: 1.4467060804367065\n",
      "Batch 410,  loss: 1.4891108274459839\n",
      "Batch 415,  loss: 1.5885817766189576\n",
      "Batch 420,  loss: 1.4119385719299316\n",
      "Batch 425,  loss: 1.6090964555740357\n",
      "Batch 430,  loss: 1.648907971382141\n",
      "Batch 435,  loss: 2.0927085161209105\n",
      "Batch 440,  loss: 1.3500598311424254\n",
      "Batch 445,  loss: 1.2742219924926759\n",
      "Batch 450,  loss: 1.3020520210266113\n",
      "Batch 455,  loss: 1.5639228343963623\n",
      "Batch 460,  loss: 1.310878849029541\n",
      "Batch 465,  loss: 1.7461057901382446\n",
      "Batch 470,  loss: 1.4575175046920776\n",
      "Batch 475,  loss: 1.4531298398971557\n",
      "Batch 480,  loss: 1.833484673500061\n",
      "Batch 485,  loss: 1.3421541690826415\n",
      "Batch 490,  loss: 1.4544577121734619\n",
      "Batch 495,  loss: 1.7798105239868165\n",
      "Batch 500,  loss: 1.573993992805481\n",
      "Batch 505,  loss: 1.5977932929992675\n",
      "Batch 510,  loss: 1.8107276201248168\n",
      "Batch 515,  loss: 1.3892432928085328\n",
      "Batch 520,  loss: 1.349567675590515\n",
      "Batch 525,  loss: 1.21557936668396\n",
      "Batch 530,  loss: 1.4045279145240783\n",
      "Batch 535,  loss: 1.6125521659851074\n",
      "Batch 540,  loss: 1.778111982345581\n",
      "Batch 545,  loss: 1.394227147102356\n",
      "Batch 550,  loss: 1.1117575645446778\n",
      "Batch 555,  loss: 1.3739084959030152\n",
      "Batch 560,  loss: 1.7563187599182128\n",
      "Batch 565,  loss: 1.5172678351402282\n",
      "Batch 570,  loss: 1.3187619686126708\n",
      "Batch 575,  loss: 1.0729761004447937\n",
      "Batch 580,  loss: 1.2891571044921875\n",
      "Batch 585,  loss: 1.5892924070358276\n",
      "Batch 590,  loss: 1.4683868646621705\n",
      "Batch 595,  loss: 1.6488517999649048\n",
      "Batch 600,  loss: 1.6774706602096559\n",
      "Batch 605,  loss: 1.310210371017456\n",
      "Batch 610,  loss: 1.4190455675125122\n",
      "Batch 615,  loss: 1.3216023921966553\n",
      "Batch 620,  loss: 1.3043776512145997\n",
      "Batch 625,  loss: 1.5353935956954956\n",
      "Batch 630,  loss: 1.3047410488128661\n",
      "Batch 635,  loss: 1.2052016019821168\n",
      "Batch 640,  loss: 1.3784715414047242\n",
      "Batch 645,  loss: 1.1474833011627197\n",
      "Batch 650,  loss: 1.401611614227295\n",
      "Batch 655,  loss: 1.6747841596603394\n",
      "Batch 660,  loss: 1.3009847164154054\n",
      "Batch 665,  loss: 1.2995073795318604\n",
      "Batch 670,  loss: 1.4563544034957885\n",
      "Batch 675,  loss: 1.3629194736480712\n",
      "Batch 680,  loss: 1.182589864730835\n",
      "Batch 685,  loss: 1.4190802097320556\n",
      "Batch 690,  loss: 1.3543991565704345\n",
      "Batch 695,  loss: 1.5552510023117065\n",
      "Batch 700,  loss: 1.3747220277786254\n",
      "Batch 705,  loss: 1.357207179069519\n",
      "Batch 710,  loss: 1.544299602508545\n",
      "Batch 715,  loss: 1.4763374328613281\n",
      "Batch 720,  loss: 1.2547696590423585\n",
      "Batch 725,  loss: 1.1960787296295166\n",
      "Batch 730,  loss: 1.6805796861648559\n",
      "Batch 735,  loss: 1.6993212461471559\n",
      "Batch 740,  loss: 1.52928569316864\n",
      "Batch 745,  loss: 1.3072321534156799\n",
      "Batch 750,  loss: 1.5873676538467407\n",
      "Batch 755,  loss: 1.3809083700180054\n",
      "Batch 760,  loss: 1.267845582962036\n",
      "Batch 765,  loss: 1.7119174003601074\n",
      "Batch 770,  loss: 1.478933572769165\n",
      "Batch 775,  loss: 1.3042056083679199\n",
      "Batch 780,  loss: 1.0908252596855164\n",
      "Batch 785,  loss: 1.6217037439346313\n",
      "Batch 790,  loss: 1.4786628246307374\n",
      "Batch 795,  loss: 1.3133693933486938\n",
      "Batch 800,  loss: 1.377476406097412\n",
      "Batch 805,  loss: 1.3892197966575623\n",
      "Batch 810,  loss: 1.2299431562423706\n",
      "Batch 815,  loss: 1.5296928644180299\n",
      "Batch 820,  loss: 1.6037539958953857\n",
      "Batch 825,  loss: 1.4764460563659667\n",
      "Batch 830,  loss: 1.6028711318969726\n",
      "Batch 835,  loss: 1.4742712259292603\n",
      "Batch 840,  loss: 1.5985737323760987\n",
      "Batch 845,  loss: 1.4025712966918946\n",
      "Batch 850,  loss: 1.68850839138031\n",
      "Batch 855,  loss: 1.3478260040283203\n",
      "Batch 860,  loss: 1.6128024101257323\n",
      "Batch 865,  loss: 1.2176239609718322\n",
      "Batch 870,  loss: 1.4451317548751832\n",
      "Batch 875,  loss: 1.4858198642730713\n",
      "Batch 880,  loss: 1.6401127099990844\n",
      "Batch 885,  loss: 1.3305564641952514\n",
      "Batch 890,  loss: 1.3294126987457275\n",
      "Batch 895,  loss: 1.3081173419952392\n",
      "Batch 900,  loss: 1.49892258644104\n",
      "Batch 905,  loss: 1.786459255218506\n",
      "Batch 910,  loss: 1.259695541858673\n",
      "Batch 915,  loss: 1.5296429634094237\n",
      "Batch 920,  loss: 1.6872747898101808\n",
      "Batch 925,  loss: 1.1147165894508362\n",
      "Batch 930,  loss: 1.6703389167785645\n",
      "Batch 935,  loss: 1.194610857963562\n",
      "Batch 940,  loss: 1.2097172141075134\n",
      "Batch 945,  loss: 1.6159710168838501\n",
      "Batch 950,  loss: 1.583820676803589\n",
      "Batch 955,  loss: 1.3035852670669557\n",
      "Batch 960,  loss: 1.6011396646499634\n",
      "Batch 965,  loss: 1.2275301933288574\n",
      "Batch 970,  loss: 1.4045543432235719\n",
      "Batch 975,  loss: 1.6788711071014404\n",
      "Batch 980,  loss: 1.582138192653656\n",
      "Batch 985,  loss: 1.7282175064086913\n",
      "Batch 990,  loss: 1.4364805459976195\n",
      "Batch 995,  loss: 1.2936569333076477\n",
      "Batch 1000,  loss: 1.4089438676834107\n",
      "Batch 1005,  loss: 1.7269920349121093\n",
      "Batch 1010,  loss: 1.5000509738922119\n",
      "Batch 1015,  loss: 1.9026878118515014\n",
      "Batch 1020,  loss: 1.706622064113617\n",
      "Batch 1025,  loss: 1.8993377804756164\n",
      "Batch 1030,  loss: 1.5739294767379761\n",
      "Batch 1035,  loss: 1.4127223491668701\n",
      "Batch 1040,  loss: 1.3508430242538452\n",
      "Batch 1045,  loss: 1.5581122398376466\n",
      "Batch 1050,  loss: 1.6021856307983398\n",
      "Batch 1055,  loss: 1.578476619720459\n",
      "Batch 1060,  loss: 1.4684725999832153\n",
      "Batch 1065,  loss: 1.424853539466858\n",
      "Batch 1070,  loss: 1.2603034973144531\n",
      "Batch 1075,  loss: 1.9057791233062744\n",
      "Batch 1080,  loss: 1.7330302953720094\n",
      "Batch 1085,  loss: 1.5062430858612061\n",
      "Batch 1090,  loss: 1.2222479701042175\n",
      "Batch 1095,  loss: 1.4107131242752076\n",
      "Batch 1100,  loss: 1.5278523206710815\n",
      "Batch 1105,  loss: 1.260740089416504\n",
      "Batch 1110,  loss: 1.4966858148574829\n",
      "Batch 1115,  loss: 1.4221004962921142\n",
      "Batch 1120,  loss: 1.5233150482177735\n",
      "Batch 1125,  loss: 1.368140697479248\n",
      "Batch 1130,  loss: 1.8077713966369628\n",
      "Batch 1135,  loss: 1.3501975774765014\n",
      "Batch 1140,  loss: 1.4658536672592164\n",
      "Batch 1145,  loss: 1.3940300941467285\n",
      "Batch 1150,  loss: 1.671666955947876\n",
      "Batch 1155,  loss: 1.3168794155120849\n",
      "Batch 1160,  loss: 1.4606789827346802\n",
      "Batch 1165,  loss: 1.4520025253295898\n",
      "Batch 1170,  loss: 1.3941898822784424\n",
      "Batch 1175,  loss: 1.3486096620559693\n",
      "Batch 1180,  loss: 1.360845422744751\n",
      "Batch 1185,  loss: 1.6096043109893798\n",
      "Batch 1190,  loss: 1.379657506942749\n",
      "Batch 1195,  loss: 1.44728684425354\n",
      "Batch 1200,  loss: 1.1955992698669433\n",
      "Batch 1205,  loss: 1.7702768325805665\n",
      "Batch 1210,  loss: 1.8756673097610475\n",
      "Batch 1215,  loss: 1.4882256269454956\n",
      "Batch 1220,  loss: 1.4616825819015502\n",
      "Batch 1225,  loss: 1.4947205305099487\n",
      "Batch 1230,  loss: 1.3834283590316772\n",
      "Batch 1235,  loss: 1.523470902442932\n",
      "Batch 1240,  loss: 1.0554551124572753\n",
      "Batch 1245,  loss: 1.7459977626800538\n",
      "Batch 1250,  loss: 1.462499451637268\n",
      "Batch 1255,  loss: 1.4669806003570556\n",
      "Batch 1260,  loss: 1.5682238340377808\n",
      "Batch 1265,  loss: 1.4833124876022339\n",
      "Batch 1270,  loss: 1.5132467985153197\n",
      "Batch 1275,  loss: 1.559609842300415\n",
      "Batch 1280,  loss: 1.422958791255951\n",
      "Batch 1285,  loss: 1.1816016912460328\n",
      "Batch 1290,  loss: 1.2795284271240235\n",
      "Batch 1295,  loss: 1.4658582210540771\n",
      "LOSS train 1.4658582210540771. Validation loss: 2.123093180661952 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 44:\n",
      "Batch 5,  loss: 1.4602690100669862\n",
      "Batch 10,  loss: 1.682560420036316\n",
      "Batch 15,  loss: 1.2963386297225952\n",
      "Batch 20,  loss: 1.4096420526504516\n",
      "Batch 25,  loss: 1.2597806692123412\n",
      "Batch 30,  loss: 1.3646131992340087\n",
      "Batch 35,  loss: 1.2541041731834413\n",
      "Batch 40,  loss: 1.59951753616333\n",
      "Batch 45,  loss: 1.3388676524162293\n",
      "Batch 50,  loss: 1.371395969390869\n",
      "Batch 55,  loss: 1.601185965538025\n",
      "Batch 60,  loss: 1.222705316543579\n",
      "Batch 65,  loss: 1.5087363958358764\n",
      "Batch 70,  loss: 1.6071146249771118\n",
      "Batch 75,  loss: 1.3952542304992677\n",
      "Batch 80,  loss: 1.5297665238380431\n",
      "Batch 85,  loss: 1.2987550020217895\n",
      "Batch 90,  loss: 1.3465879440307618\n",
      "Batch 95,  loss: 1.6276710271835326\n",
      "Batch 100,  loss: 1.43902268409729\n",
      "Batch 105,  loss: 1.3284167766571044\n",
      "Batch 110,  loss: 1.6592945098876952\n",
      "Batch 115,  loss: 1.7454951286315918\n",
      "Batch 120,  loss: 1.528140950202942\n",
      "Batch 125,  loss: 1.7724432229995728\n",
      "Batch 130,  loss: 1.9031627893447876\n",
      "Batch 135,  loss: 1.4814489841461183\n",
      "Batch 140,  loss: 1.4523842334747314\n",
      "Batch 145,  loss: 1.3923309087753295\n",
      "Batch 150,  loss: 1.6109565258026124\n",
      "Batch 155,  loss: 1.410637664794922\n",
      "Batch 160,  loss: 1.7194622039794922\n",
      "Batch 165,  loss: 1.5900333881378175\n",
      "Batch 170,  loss: 1.5793472528457642\n",
      "Batch 175,  loss: 1.3926363825798034\n",
      "Batch 180,  loss: 1.4353463411331178\n",
      "Batch 185,  loss: 1.2442284107208252\n",
      "Batch 190,  loss: 1.5087368488311768\n",
      "Batch 195,  loss: 1.6786583065986633\n",
      "Batch 200,  loss: 1.4105091094970703\n",
      "Batch 205,  loss: 1.4934638261795044\n",
      "Batch 210,  loss: 1.6002174019813538\n",
      "Batch 215,  loss: 1.3757597923278808\n",
      "Batch 220,  loss: 1.041687262058258\n",
      "Batch 225,  loss: 1.233373999595642\n",
      "Batch 230,  loss: 1.1843319058418273\n",
      "Batch 235,  loss: 1.4671257734298706\n",
      "Batch 240,  loss: 1.5239053249359131\n",
      "Batch 245,  loss: 1.4949467658996582\n",
      "Batch 250,  loss: 1.7513941287994386\n",
      "Batch 255,  loss: 1.5622114658355712\n",
      "Batch 260,  loss: 1.7902178525924684\n",
      "Batch 265,  loss: 1.51795334815979\n",
      "Batch 270,  loss: 1.1763741016387939\n",
      "Batch 275,  loss: 1.488062596321106\n",
      "Batch 280,  loss: 1.239200806617737\n",
      "Batch 285,  loss: 1.3756157398223876\n",
      "Batch 290,  loss: 1.188210916519165\n",
      "Batch 295,  loss: 1.346950650215149\n",
      "Batch 300,  loss: 1.673833155632019\n",
      "Batch 305,  loss: 1.2585339069366455\n",
      "Batch 310,  loss: 1.1968563556671143\n",
      "Batch 315,  loss: 1.7304816484451293\n",
      "Batch 320,  loss: 1.6159636735916139\n",
      "Batch 325,  loss: 1.473363995552063\n",
      "Batch 330,  loss: 1.5100403547286987\n",
      "Batch 335,  loss: 1.5138746976852417\n",
      "Batch 340,  loss: 1.885628080368042\n",
      "Batch 345,  loss: 1.8865466117858887\n",
      "Batch 350,  loss: 1.6224694728851319\n",
      "Batch 355,  loss: 1.7087549924850465\n",
      "Batch 360,  loss: 1.5884873390197753\n",
      "Batch 365,  loss: 1.7839123010635376\n",
      "Batch 370,  loss: 1.5291609048843384\n",
      "Batch 375,  loss: 1.516843032836914\n",
      "Batch 380,  loss: 1.3905022382736205\n",
      "Batch 385,  loss: 1.4908858776092528\n",
      "Batch 390,  loss: 1.5493127584457398\n",
      "Batch 395,  loss: 1.7155804634094238\n",
      "Batch 400,  loss: 1.3955075979232787\n",
      "Batch 405,  loss: 1.2873677730560302\n",
      "Batch 410,  loss: 1.4210457801818848\n",
      "Batch 415,  loss: 1.4318467617034911\n",
      "Batch 420,  loss: 1.5958305478096009\n",
      "Batch 425,  loss: 1.270108437538147\n",
      "Batch 430,  loss: 1.7108586311340332\n",
      "Batch 435,  loss: 1.3632392644882203\n",
      "Batch 440,  loss: 1.5141802787780763\n",
      "Batch 445,  loss: 1.526135516166687\n",
      "Batch 450,  loss: 1.5000545740127564\n",
      "Batch 455,  loss: 1.8238137722015382\n",
      "Batch 460,  loss: 1.563725769519806\n",
      "Batch 465,  loss: 1.4828376293182373\n",
      "Batch 470,  loss: 1.3095539569854737\n",
      "Batch 475,  loss: 1.2490089774131774\n",
      "Batch 480,  loss: 1.5515846610069275\n",
      "Batch 485,  loss: 1.7010197639465332\n",
      "Batch 490,  loss: 1.6927856922149658\n",
      "Batch 495,  loss: 1.505132794380188\n",
      "Batch 500,  loss: 1.4775068759918213\n",
      "Batch 505,  loss: 1.5499658107757568\n",
      "Batch 510,  loss: 1.3139008283615112\n",
      "Batch 515,  loss: 1.5017561435699462\n",
      "Batch 520,  loss: 1.548928689956665\n",
      "Batch 525,  loss: 1.6561699151992797\n",
      "Batch 530,  loss: 1.5524587631225586\n",
      "Batch 535,  loss: 1.3277859210968017\n",
      "Batch 540,  loss: 1.2311741828918457\n",
      "Batch 545,  loss: 1.6341967105865478\n",
      "Batch 550,  loss: 1.5066115617752076\n",
      "Batch 555,  loss: 1.832938027381897\n",
      "Batch 560,  loss: 1.4490519285202026\n",
      "Batch 565,  loss: 1.4259429216384887\n",
      "Batch 570,  loss: 1.5140953540802002\n",
      "Batch 575,  loss: 1.3502581357955932\n",
      "Batch 580,  loss: 1.484656000137329\n",
      "Batch 585,  loss: 1.1184585452079774\n",
      "Batch 590,  loss: 1.3991612553596497\n",
      "Batch 595,  loss: 1.6364442586898804\n",
      "Batch 600,  loss: 1.4490172624588014\n",
      "Batch 605,  loss: 1.3444724559783936\n",
      "Batch 610,  loss: 1.0331918120384216\n",
      "Batch 615,  loss: 1.5758927583694458\n",
      "Batch 620,  loss: 1.6025603055953979\n",
      "Batch 625,  loss: 1.7864292860031128\n",
      "Batch 630,  loss: 1.5771183729171754\n",
      "Batch 635,  loss: 1.5152860403060913\n",
      "Batch 640,  loss: 1.39360511302948\n",
      "Batch 645,  loss: 1.694054079055786\n",
      "Batch 650,  loss: 1.1611872673034669\n",
      "Batch 655,  loss: 1.234969437122345\n",
      "Batch 660,  loss: 1.540023684501648\n",
      "Batch 665,  loss: 1.6261438131332397\n",
      "Batch 670,  loss: 1.2014652490615845\n",
      "Batch 675,  loss: 1.218410277366638\n",
      "Batch 680,  loss: 1.2935280561447144\n",
      "Batch 685,  loss: 1.560378360748291\n",
      "Batch 690,  loss: 1.4359971761703492\n",
      "Batch 695,  loss: 1.8024796962738037\n",
      "Batch 700,  loss: 1.5042208433151245\n",
      "Batch 705,  loss: 1.5514341115951538\n",
      "Batch 710,  loss: 1.2555200099945067\n",
      "Batch 715,  loss: 1.4569737434387207\n",
      "Batch 720,  loss: 1.3725820302963256\n",
      "Batch 725,  loss: 1.1759156227111816\n",
      "Batch 730,  loss: 1.4577017307281495\n",
      "Batch 735,  loss: 1.4084412336349488\n",
      "Batch 740,  loss: 1.3481941938400268\n",
      "Batch 745,  loss: 1.3226470232009888\n",
      "Batch 750,  loss: 1.8712864875793458\n",
      "Batch 755,  loss: 1.5153489112854004\n",
      "Batch 760,  loss: 1.4796065092086792\n",
      "Batch 765,  loss: 1.5908809185028077\n",
      "Batch 770,  loss: 1.4931304931640625\n",
      "Batch 775,  loss: 1.4818116188049317\n",
      "Batch 780,  loss: 1.324937677383423\n",
      "Batch 785,  loss: 1.446831715106964\n",
      "Batch 790,  loss: 1.5366706132888794\n",
      "Batch 795,  loss: 1.4458272933959961\n",
      "Batch 800,  loss: 1.3621736884117126\n",
      "Batch 805,  loss: 1.2491821765899658\n",
      "Batch 810,  loss: 1.5273315191268921\n",
      "Batch 815,  loss: 1.5103668928146363\n",
      "Batch 820,  loss: 1.3439716815948486\n",
      "Batch 825,  loss: 1.6264376401901246\n",
      "Batch 830,  loss: 1.4919211387634277\n",
      "Batch 835,  loss: 1.520603609085083\n",
      "Batch 840,  loss: 1.5874658107757569\n",
      "Batch 845,  loss: 1.594591498374939\n",
      "Batch 850,  loss: 1.146701693534851\n",
      "Batch 855,  loss: 1.35141761302948\n",
      "Batch 860,  loss: 1.3621596574783326\n",
      "Batch 865,  loss: 1.5735172748565673\n",
      "Batch 870,  loss: 1.2225881338119506\n",
      "Batch 875,  loss: 1.6647222280502318\n",
      "Batch 880,  loss: 1.6691048622131348\n",
      "Batch 885,  loss: 1.486376976966858\n",
      "Batch 890,  loss: 1.5228819012641908\n",
      "Batch 895,  loss: 1.5212179183959962\n",
      "Batch 900,  loss: 1.750779676437378\n",
      "Batch 905,  loss: 1.4275903344154357\n",
      "Batch 910,  loss: 1.4607691287994384\n",
      "Batch 915,  loss: 1.5368191719055175\n",
      "Batch 920,  loss: 1.4792760133743286\n",
      "Batch 925,  loss: 1.3678390502929687\n",
      "Batch 930,  loss: 1.2778719902038573\n",
      "Batch 935,  loss: 1.4024351596832276\n",
      "Batch 940,  loss: 1.342029881477356\n",
      "Batch 945,  loss: 1.7101209998130797\n",
      "Batch 950,  loss: 1.5002143144607545\n",
      "Batch 955,  loss: 1.619667387008667\n",
      "Batch 960,  loss: 1.4392354965209961\n",
      "Batch 965,  loss: 1.4918791055679321\n",
      "Batch 970,  loss: 1.3805131912231445\n",
      "Batch 975,  loss: 1.6908024549484253\n",
      "Batch 980,  loss: 1.5156917095184326\n",
      "Batch 985,  loss: 1.6056681871414185\n",
      "Batch 990,  loss: 1.426283311843872\n",
      "Batch 995,  loss: 1.539862370491028\n",
      "Batch 1000,  loss: 1.8212390899658204\n",
      "Batch 1005,  loss: 1.22572603225708\n",
      "Batch 1010,  loss: 1.1876009702682495\n",
      "Batch 1015,  loss: 1.3177242517471313\n",
      "Batch 1020,  loss: 1.528266978263855\n",
      "Batch 1025,  loss: 1.3176386117935182\n",
      "Batch 1030,  loss: 1.5415781617164612\n",
      "Batch 1035,  loss: 1.2752023696899415\n",
      "Batch 1040,  loss: 2.138139271736145\n",
      "Batch 1045,  loss: 1.2528707027435302\n",
      "Batch 1050,  loss: 1.1399134635925292\n",
      "Batch 1055,  loss: 1.5553325891494751\n",
      "Batch 1060,  loss: 1.4269923210144042\n",
      "Batch 1065,  loss: 1.2500132083892823\n",
      "Batch 1070,  loss: 1.5388315439224243\n",
      "Batch 1075,  loss: 1.5678819179534913\n",
      "Batch 1080,  loss: 1.6221276760101317\n",
      "Batch 1085,  loss: 1.7248202085494995\n",
      "Batch 1090,  loss: 1.1536751747131349\n",
      "Batch 1095,  loss: 1.1384989857673644\n",
      "Batch 1100,  loss: 1.3825381636619567\n",
      "Batch 1105,  loss: 1.6795328378677368\n",
      "Batch 1110,  loss: 1.5054050922393798\n",
      "Batch 1115,  loss: 1.4388540744781495\n",
      "Batch 1120,  loss: 1.5731032371520997\n",
      "Batch 1125,  loss: 1.5918930053710938\n",
      "Batch 1130,  loss: 1.3869643926620483\n",
      "Batch 1135,  loss: 1.5192203760147094\n",
      "Batch 1140,  loss: 1.5140080451965332\n",
      "Batch 1145,  loss: 1.5100191354751586\n",
      "Batch 1150,  loss: 1.6832945823669434\n",
      "Batch 1155,  loss: 1.1738130569458007\n",
      "Batch 1160,  loss: 1.4192513465881347\n",
      "Batch 1165,  loss: 1.3015167474746705\n",
      "Batch 1170,  loss: 1.7968550443649292\n",
      "Batch 1175,  loss: 1.3555164098739625\n",
      "Batch 1180,  loss: 1.4929306030273437\n",
      "Batch 1185,  loss: 1.736514163017273\n",
      "Batch 1190,  loss: 1.3588449954986572\n",
      "Batch 1195,  loss: 1.2634069204330445\n",
      "Batch 1200,  loss: 1.7117860317230225\n",
      "Batch 1205,  loss: 1.5184867620468139\n",
      "Batch 1210,  loss: 1.3398473501205443\n",
      "Batch 1215,  loss: 1.6460174441337585\n",
      "Batch 1220,  loss: 1.382092261314392\n",
      "Batch 1225,  loss: 1.4141474723815919\n",
      "Batch 1230,  loss: 1.5295321822166443\n",
      "Batch 1235,  loss: 1.1619979619979859\n",
      "Batch 1240,  loss: 1.262113618850708\n",
      "Batch 1245,  loss: 1.4985429048538208\n",
      "Batch 1250,  loss: 1.6442807197570801\n",
      "Batch 1255,  loss: 1.3659440398216247\n",
      "Batch 1260,  loss: 1.6298431873321533\n",
      "Batch 1265,  loss: 1.5530848741531371\n",
      "Batch 1270,  loss: 1.4003422737121582\n",
      "Batch 1275,  loss: 1.425771999359131\n",
      "Batch 1280,  loss: 1.1774935007095337\n",
      "Batch 1285,  loss: 1.3027023077011108\n",
      "Batch 1290,  loss: 1.3673810720443726\n",
      "Batch 1295,  loss: 1.260841464996338\n",
      "LOSS train 1.260841464996338. Validation loss: 1.9848246722999545 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 45:\n",
      "Batch 5,  loss: 1.520406460762024\n",
      "Batch 10,  loss: 1.412287211418152\n",
      "Batch 15,  loss: 1.358342218399048\n",
      "Batch 20,  loss: 1.7472294330596925\n",
      "Batch 25,  loss: 1.4254706144332885\n",
      "Batch 30,  loss: 1.4905489683151245\n",
      "Batch 35,  loss: 1.6246777057647706\n",
      "Batch 40,  loss: 1.4730983257293702\n",
      "Batch 45,  loss: 1.5562678098678588\n",
      "Batch 50,  loss: 1.4657650947570802\n",
      "Batch 55,  loss: 1.3300021171569825\n",
      "Batch 60,  loss: 1.3842559218406678\n",
      "Batch 65,  loss: 1.3303627967834473\n",
      "Batch 70,  loss: 1.3930281400680542\n",
      "Batch 75,  loss: 1.3783200263977051\n",
      "Batch 80,  loss: 1.5541073560714722\n",
      "Batch 85,  loss: 1.3949747562408448\n",
      "Batch 90,  loss: 1.2396822690963745\n",
      "Batch 95,  loss: 1.2756201505661011\n",
      "Batch 100,  loss: 1.5978834867477416\n",
      "Batch 105,  loss: 1.5025159120559692\n",
      "Batch 110,  loss: 1.680378246307373\n",
      "Batch 115,  loss: 1.3409913301467895\n",
      "Batch 120,  loss: 1.3487384080886842\n",
      "Batch 125,  loss: 1.4319513082504272\n",
      "Batch 130,  loss: 1.341496193408966\n",
      "Batch 135,  loss: 1.272979187965393\n",
      "Batch 140,  loss: 1.2083760261535645\n",
      "Batch 145,  loss: 1.4961042642593383\n",
      "Batch 150,  loss: 1.2189966797828675\n",
      "Batch 155,  loss: 2.0553390264511107\n",
      "Batch 160,  loss: 1.2465569496154785\n",
      "Batch 165,  loss: 1.355482316017151\n",
      "Batch 170,  loss: 1.290108323097229\n",
      "Batch 175,  loss: 1.320884609222412\n",
      "Batch 180,  loss: 1.8727047443389893\n",
      "Batch 185,  loss: 1.5426145792007446\n",
      "Batch 190,  loss: 1.4416284561157227\n",
      "Batch 195,  loss: 1.3286153554916382\n",
      "Batch 200,  loss: 1.6685734748840333\n",
      "Batch 205,  loss: 1.380475890636444\n",
      "Batch 210,  loss: 1.58184974193573\n",
      "Batch 215,  loss: 1.3878069877624513\n",
      "Batch 220,  loss: 1.3551584005355835\n",
      "Batch 225,  loss: 1.3506555557250977\n",
      "Batch 230,  loss: 1.3556264638900757\n",
      "Batch 235,  loss: 1.530508005619049\n",
      "Batch 240,  loss: 1.3962237119674683\n",
      "Batch 245,  loss: 1.5923760414123536\n",
      "Batch 250,  loss: 1.5563960671424866\n",
      "Batch 255,  loss: 1.2965429782867433\n",
      "Batch 260,  loss: 1.4862856149673462\n",
      "Batch 265,  loss: 1.6060891628265381\n",
      "Batch 270,  loss: 1.7922966718673705\n",
      "Batch 275,  loss: 1.6159943103790284\n",
      "Batch 280,  loss: 1.266682744026184\n",
      "Batch 285,  loss: 1.3161600828170776\n",
      "Batch 290,  loss: 1.6023452281951904\n",
      "Batch 295,  loss: 1.2999014616012574\n",
      "Batch 300,  loss: 1.873196268081665\n",
      "Batch 305,  loss: 1.3292634963989258\n",
      "Batch 310,  loss: 1.829885721206665\n",
      "Batch 315,  loss: 1.2989914178848267\n",
      "Batch 320,  loss: 1.2856303811073304\n",
      "Batch 325,  loss: 1.7730345487594605\n",
      "Batch 330,  loss: 1.670944881439209\n",
      "Batch 335,  loss: 1.5329531908035279\n",
      "Batch 340,  loss: 1.1337385177612305\n",
      "Batch 345,  loss: 1.289794659614563\n",
      "Batch 350,  loss: 1.3350432991981507\n",
      "Batch 355,  loss: 1.3407545804977417\n",
      "Batch 360,  loss: 1.4692068099975586\n",
      "Batch 365,  loss: 1.4877635002136231\n",
      "Batch 370,  loss: 1.3273506879806518\n",
      "Batch 375,  loss: 1.136378037929535\n",
      "Batch 380,  loss: 1.4721177339553833\n",
      "Batch 385,  loss: 1.3941955804824828\n",
      "Batch 390,  loss: 1.5627484560012816\n",
      "Batch 395,  loss: 1.6398019313812255\n",
      "Batch 400,  loss: 1.4367787837982178\n",
      "Batch 405,  loss: 1.5500911712646483\n",
      "Batch 410,  loss: 1.4861450910568237\n",
      "Batch 415,  loss: 1.583754014968872\n",
      "Batch 420,  loss: 1.5194048404693603\n",
      "Batch 425,  loss: 1.4940610885620118\n",
      "Batch 430,  loss: 1.4266819715499879\n",
      "Batch 435,  loss: 1.461867618560791\n",
      "Batch 440,  loss: 1.4286388158798218\n",
      "Batch 445,  loss: 1.5088163018226624\n",
      "Batch 450,  loss: 1.2772860765457152\n",
      "Batch 455,  loss: 1.2917601108551025\n",
      "Batch 460,  loss: 2.0078815698623655\n",
      "Batch 465,  loss: 1.3993629932403564\n",
      "Batch 470,  loss: 1.686823582649231\n",
      "Batch 475,  loss: 1.258987510204315\n",
      "Batch 480,  loss: 1.2696977853775024\n",
      "Batch 485,  loss: 1.62096049785614\n",
      "Batch 490,  loss: 1.7938079118728638\n",
      "Batch 495,  loss: 1.5120119333267212\n",
      "Batch 500,  loss: 1.344959306716919\n",
      "Batch 505,  loss: 1.8439437866210937\n",
      "Batch 510,  loss: 1.5529284477233887\n",
      "Batch 515,  loss: 1.3827229022979737\n",
      "Batch 520,  loss: 1.417737066745758\n",
      "Batch 525,  loss: 1.2412997722625732\n",
      "Batch 530,  loss: 1.7683955907821656\n",
      "Batch 535,  loss: 1.1967596530914306\n",
      "Batch 540,  loss: 1.540294885635376\n",
      "Batch 545,  loss: 1.5855341911315919\n",
      "Batch 550,  loss: 1.4633077383041382\n",
      "Batch 555,  loss: 1.9266640901565553\n",
      "Batch 560,  loss: 1.511242890357971\n",
      "Batch 565,  loss: 1.4436047315597533\n",
      "Batch 570,  loss: 1.4608774423599242\n",
      "Batch 575,  loss: 1.3110371112823487\n",
      "Batch 580,  loss: 1.3311472296714784\n",
      "Batch 585,  loss: 1.6062233209609986\n",
      "Batch 590,  loss: 1.2397802352905274\n",
      "Batch 595,  loss: 1.4480895519256591\n",
      "Batch 600,  loss: 1.3061947107315064\n",
      "Batch 605,  loss: 1.3458967208862305\n",
      "Batch 610,  loss: 1.5212332487106324\n",
      "Batch 615,  loss: 1.5635830760002136\n",
      "Batch 620,  loss: 1.5649662733078002\n",
      "Batch 625,  loss: 1.5815260648727416\n",
      "Batch 630,  loss: 1.6501648664474486\n",
      "Batch 635,  loss: 1.1357182621955872\n",
      "Batch 640,  loss: 1.414901900291443\n",
      "Batch 645,  loss: 1.602468478679657\n",
      "Batch 650,  loss: 1.341464924812317\n",
      "Batch 655,  loss: 1.4293187618255616\n",
      "Batch 660,  loss: 1.6473816633224487\n",
      "Batch 665,  loss: 1.320681917667389\n",
      "Batch 670,  loss: 1.4016113519668578\n",
      "Batch 675,  loss: 1.6616851329803466\n",
      "Batch 680,  loss: 1.4959394693374635\n",
      "Batch 685,  loss: 1.6734728574752809\n",
      "Batch 690,  loss: 1.31802898645401\n",
      "Batch 695,  loss: 1.461949348449707\n",
      "Batch 700,  loss: 1.4922065019607544\n",
      "Batch 705,  loss: 1.7048298120498657\n",
      "Batch 710,  loss: 1.5868160128593445\n",
      "Batch 715,  loss: 1.5654773235321044\n",
      "Batch 720,  loss: 1.4883558988571166\n",
      "Batch 725,  loss: 1.2643840074539185\n",
      "Batch 730,  loss: 1.4551657676696776\n",
      "Batch 735,  loss: 1.458461594581604\n",
      "Batch 740,  loss: 1.764374589920044\n",
      "Batch 745,  loss: 1.4116848230361938\n",
      "Batch 750,  loss: 1.504647421836853\n",
      "Batch 755,  loss: 1.574720311164856\n",
      "Batch 760,  loss: 1.28489031791687\n",
      "Batch 765,  loss: 1.519768238067627\n",
      "Batch 770,  loss: 1.3407838225364686\n",
      "Batch 775,  loss: 1.6721884489059449\n",
      "Batch 780,  loss: 1.2702492713928222\n",
      "Batch 785,  loss: 1.5214983940124511\n",
      "Batch 790,  loss: 1.4512571096420288\n",
      "Batch 795,  loss: 1.3682157516479492\n",
      "Batch 800,  loss: 1.3806605100631715\n",
      "Batch 805,  loss: 1.7237984895706178\n",
      "Batch 810,  loss: 1.3944282293319703\n",
      "Batch 815,  loss: 1.460359311103821\n",
      "Batch 820,  loss: 1.341303038597107\n",
      "Batch 825,  loss: 1.517553734779358\n",
      "Batch 830,  loss: 1.5542455434799194\n",
      "Batch 835,  loss: 1.3909490585327149\n",
      "Batch 840,  loss: 1.5073237419128418\n",
      "Batch 845,  loss: 1.4609111070632934\n",
      "Batch 850,  loss: 1.395829701423645\n",
      "Batch 855,  loss: 1.326365089416504\n",
      "Batch 860,  loss: 1.5764485359191895\n",
      "Batch 865,  loss: 1.598054337501526\n",
      "Batch 870,  loss: 1.5331271648406983\n",
      "Batch 875,  loss: 1.7779754400253296\n",
      "Batch 880,  loss: 1.4974190235137939\n",
      "Batch 885,  loss: 1.5837601184844972\n",
      "Batch 890,  loss: 1.3473358631134034\n",
      "Batch 895,  loss: 1.6487680673599243\n",
      "Batch 900,  loss: 1.4517048358917237\n",
      "Batch 905,  loss: 1.3749279975891113\n",
      "Batch 910,  loss: 1.548631453514099\n",
      "Batch 915,  loss: 1.1818370580673219\n",
      "Batch 920,  loss: 1.7317100048065186\n",
      "Batch 925,  loss: 1.0662207126617431\n",
      "Batch 930,  loss: 1.730518388748169\n",
      "Batch 935,  loss: 1.1554803013801576\n",
      "Batch 940,  loss: 1.3312666058540343\n",
      "Batch 945,  loss: 1.8888273000717164\n",
      "Batch 950,  loss: 1.443633770942688\n",
      "Batch 955,  loss: 1.4731207966804505\n",
      "Batch 960,  loss: 1.428254497051239\n",
      "Batch 965,  loss: 1.395884108543396\n",
      "Batch 970,  loss: 1.444952940940857\n",
      "Batch 975,  loss: 1.4347079157829286\n",
      "Batch 980,  loss: 1.420183539390564\n",
      "Batch 985,  loss: 1.6335806369781494\n",
      "Batch 990,  loss: 1.1960376620292663\n",
      "Batch 995,  loss: 1.3830994844436646\n",
      "Batch 1000,  loss: 1.8192539453506469\n",
      "Batch 1005,  loss: 1.3583603143692016\n",
      "Batch 1010,  loss: 1.3170011281967162\n",
      "Batch 1015,  loss: 1.5444000959396362\n",
      "Batch 1020,  loss: 1.1810589909553528\n",
      "Batch 1025,  loss: 1.5418514251708983\n",
      "Batch 1030,  loss: 1.471836233139038\n",
      "Batch 1035,  loss: 1.3616776704788207\n",
      "Batch 1040,  loss: 1.2969839811325072\n",
      "Batch 1045,  loss: 1.6191041946411133\n",
      "Batch 1050,  loss: 1.5798614263534545\n",
      "Batch 1055,  loss: 1.6718320369720459\n",
      "Batch 1060,  loss: 1.6805509328842163\n",
      "Batch 1065,  loss: 1.4131046652793884\n",
      "Batch 1070,  loss: 1.3728191256523132\n",
      "Batch 1075,  loss: 1.5381068468093873\n",
      "Batch 1080,  loss: 1.60069842338562\n",
      "Batch 1085,  loss: 1.8691521644592286\n",
      "Batch 1090,  loss: 1.7291394472122192\n",
      "Batch 1095,  loss: 1.1382416248321534\n",
      "Batch 1100,  loss: 1.562290906906128\n",
      "Batch 1105,  loss: 1.2904477834701538\n",
      "Batch 1110,  loss: 1.6094301700592042\n",
      "Batch 1115,  loss: 1.2537454605102538\n",
      "Batch 1120,  loss: 1.500030255317688\n",
      "Batch 1125,  loss: 1.2415813684463501\n",
      "Batch 1130,  loss: 1.6643948554992676\n",
      "Batch 1135,  loss: 1.1673705101013183\n",
      "Batch 1140,  loss: 1.8136300802230836\n",
      "Batch 1145,  loss: 1.3599051237106323\n",
      "Batch 1150,  loss: 1.6294360160827637\n",
      "Batch 1155,  loss: 1.468466591835022\n",
      "Batch 1160,  loss: 1.3181873083114624\n",
      "Batch 1165,  loss: 1.3451665401458741\n",
      "Batch 1170,  loss: 1.8066462993621826\n",
      "Batch 1175,  loss: 1.2795963168144227\n",
      "Batch 1180,  loss: 1.7846378326416015\n",
      "Batch 1185,  loss: 1.2912218809127807\n",
      "Batch 1190,  loss: 1.771933126449585\n",
      "Batch 1195,  loss: 1.3107259035110475\n",
      "Batch 1200,  loss: 1.4497754096984863\n",
      "Batch 1205,  loss: 1.6357903957366944\n",
      "Batch 1210,  loss: 1.5322898149490356\n",
      "Batch 1215,  loss: 1.1927100777626038\n",
      "Batch 1220,  loss: 1.2772537112236022\n",
      "Batch 1225,  loss: 1.118095302581787\n",
      "Batch 1230,  loss: 1.3069321393966675\n",
      "Batch 1235,  loss: 1.4022880792617798\n",
      "Batch 1240,  loss: 1.5277775883674622\n",
      "Batch 1245,  loss: 1.6549804210662842\n",
      "Batch 1250,  loss: 1.3790661573410035\n",
      "Batch 1255,  loss: 1.347115969657898\n",
      "Batch 1260,  loss: 1.2134628295898438\n",
      "Batch 1265,  loss: 1.31413414478302\n",
      "Batch 1270,  loss: 1.4881401538848877\n",
      "Batch 1275,  loss: 1.7313009023666381\n",
      "Batch 1280,  loss: 1.4629582166671753\n",
      "Batch 1285,  loss: 1.0801981329917907\n",
      "Batch 1290,  loss: 1.2163581490516662\n",
      "Batch 1295,  loss: 1.7244148969650268\n",
      "LOSS train 1.7244148969650268. Validation loss: 2.028449441023447 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 46:\n",
      "Batch 5,  loss: 1.454955768585205\n",
      "Batch 10,  loss: 1.7128953695297242\n",
      "Batch 15,  loss: 1.4843881845474243\n",
      "Batch 20,  loss: 1.4072136163711548\n",
      "Batch 25,  loss: 1.4166130781173707\n",
      "Batch 30,  loss: 1.4270493268966675\n",
      "Batch 35,  loss: 1.464054036140442\n",
      "Batch 40,  loss: 1.3214277982711793\n",
      "Batch 45,  loss: 1.6697910189628602\n",
      "Batch 50,  loss: 1.2575153946876525\n",
      "Batch 55,  loss: 1.3616176128387452\n",
      "Batch 60,  loss: 1.5975025177001954\n",
      "Batch 65,  loss: 1.424054002761841\n",
      "Batch 70,  loss: 1.3654541015625\n",
      "Batch 75,  loss: 1.5645459175109864\n",
      "Batch 80,  loss: 1.6244378805160522\n",
      "Batch 85,  loss: 1.4622389078140259\n",
      "Batch 90,  loss: 1.376957082748413\n",
      "Batch 95,  loss: 1.285802960395813\n",
      "Batch 100,  loss: 1.2623497605323792\n",
      "Batch 105,  loss: 1.3060921430587769\n",
      "Batch 110,  loss: 1.5087685108184814\n",
      "Batch 115,  loss: 1.6928914070129395\n",
      "Batch 120,  loss: 1.372717595100403\n",
      "Batch 125,  loss: 1.3096360206604003\n",
      "Batch 130,  loss: 1.4644193172454834\n",
      "Batch 135,  loss: 1.2986182689666748\n",
      "Batch 140,  loss: 1.362876033782959\n",
      "Batch 145,  loss: 1.659396266937256\n",
      "Batch 150,  loss: 1.5187670469284058\n",
      "Batch 155,  loss: 1.4988942623138428\n",
      "Batch 160,  loss: 1.4085378527641297\n",
      "Batch 165,  loss: 1.6783315896987916\n",
      "Batch 170,  loss: 1.16476628780365\n",
      "Batch 175,  loss: 1.2234360218048095\n",
      "Batch 180,  loss: 1.4166742324829102\n",
      "Batch 185,  loss: 1.4813455581665038\n",
      "Batch 190,  loss: 1.2415683031082154\n",
      "Batch 195,  loss: 1.2900469422340393\n",
      "Batch 200,  loss: 1.5491718292236327\n",
      "Batch 205,  loss: 1.4895248651504516\n",
      "Batch 210,  loss: 1.4775814771652223\n",
      "Batch 215,  loss: 1.7800134897232056\n",
      "Batch 220,  loss: 1.6804951667785644\n",
      "Batch 225,  loss: 1.3776235818862914\n",
      "Batch 230,  loss: 1.594131088256836\n",
      "Batch 235,  loss: 1.283309531211853\n",
      "Batch 240,  loss: 1.2589261531829834\n",
      "Batch 245,  loss: 1.1771877646446227\n",
      "Batch 250,  loss: 1.6077643632888794\n",
      "Batch 255,  loss: 1.4760064363479615\n",
      "Batch 260,  loss: 1.3128761529922486\n",
      "Batch 265,  loss: 1.416723871231079\n",
      "Batch 270,  loss: 1.5052220821380615\n",
      "Batch 275,  loss: 1.6189105272293092\n",
      "Batch 280,  loss: 1.5155312299728394\n",
      "Batch 285,  loss: 1.6453072547912597\n",
      "Batch 290,  loss: 1.3795651912689209\n",
      "Batch 295,  loss: 1.4580695271492004\n",
      "Batch 300,  loss: 1.237111210823059\n",
      "Batch 305,  loss: 1.4706733703613282\n",
      "Batch 310,  loss: 1.522070574760437\n",
      "Batch 315,  loss: 1.44018235206604\n",
      "Batch 320,  loss: 1.8617486476898193\n",
      "Batch 325,  loss: 1.4782574415206908\n",
      "Batch 330,  loss: 1.1103570938110352\n",
      "Batch 335,  loss: 2.0820696353912354\n",
      "Batch 340,  loss: 1.7014031410217285\n",
      "Batch 345,  loss: 1.5308830261230468\n",
      "Batch 350,  loss: 1.4046308517456054\n",
      "Batch 355,  loss: 1.623592460155487\n",
      "Batch 360,  loss: 1.6510258674621583\n",
      "Batch 365,  loss: 1.3759716272354126\n",
      "Batch 370,  loss: 1.5936276197433472\n",
      "Batch 375,  loss: 1.4388320684432983\n",
      "Batch 380,  loss: 1.4073562383651734\n",
      "Batch 385,  loss: 1.2707090616226195\n",
      "Batch 390,  loss: 1.268919336795807\n",
      "Batch 395,  loss: 1.8133132696151733\n",
      "Batch 400,  loss: 1.4796213269233705\n",
      "Batch 405,  loss: 1.3853664875030518\n",
      "Batch 410,  loss: 1.2620101928710938\n",
      "Batch 415,  loss: 1.2773908019065856\n",
      "Batch 420,  loss: 1.397315549850464\n",
      "Batch 425,  loss: 1.5334118843078612\n",
      "Batch 430,  loss: 1.6502002954483033\n",
      "Batch 435,  loss: 1.568754506111145\n",
      "Batch 440,  loss: 1.550913119316101\n",
      "Batch 445,  loss: 1.2181230783462524\n",
      "Batch 450,  loss: 1.5549405097961426\n",
      "Batch 455,  loss: 1.223816704750061\n",
      "Batch 460,  loss: 1.4737499237060547\n",
      "Batch 465,  loss: 1.262150228023529\n",
      "Batch 470,  loss: 1.6606114864349366\n",
      "Batch 475,  loss: 1.3007039070129394\n",
      "Batch 480,  loss: 1.6147215127944947\n",
      "Batch 485,  loss: 1.8083679676055908\n",
      "Batch 490,  loss: 1.792130184173584\n",
      "Batch 495,  loss: 1.3015970230102538\n",
      "Batch 500,  loss: 1.4163981676101685\n",
      "Batch 505,  loss: 1.5088219404220582\n",
      "Batch 510,  loss: 1.3079482436180114\n",
      "Batch 515,  loss: 1.3502021789550782\n",
      "Batch 520,  loss: 1.5220545291900636\n",
      "Batch 525,  loss: 1.9056578159332276\n",
      "Batch 530,  loss: 1.0423370003700256\n",
      "Batch 535,  loss: 1.573126769065857\n",
      "Batch 540,  loss: 1.4935675382614135\n",
      "Batch 545,  loss: 1.367221426963806\n",
      "Batch 550,  loss: 1.3098349094390869\n",
      "Batch 555,  loss: 1.5324714422225951\n",
      "Batch 560,  loss: 1.3331731319427491\n",
      "Batch 565,  loss: 1.4371786832809448\n",
      "Batch 570,  loss: 2.187889242172241\n",
      "Batch 575,  loss: 1.6681074380874634\n",
      "Batch 580,  loss: 1.5576300144195556\n",
      "Batch 585,  loss: 1.3691535472869873\n",
      "Batch 590,  loss: 1.2477747678756714\n",
      "Batch 595,  loss: 1.5516164541244506\n",
      "Batch 600,  loss: 1.2680493593215942\n",
      "Batch 605,  loss: 1.6817687034606934\n",
      "Batch 610,  loss: 1.7642383575439453\n",
      "Batch 615,  loss: 1.5097601532936096\n",
      "Batch 620,  loss: 1.6322418451309204\n",
      "Batch 625,  loss: 1.773760151863098\n",
      "Batch 630,  loss: 1.5134252071380616\n",
      "Batch 635,  loss: 1.370486283302307\n",
      "Batch 640,  loss: 1.3667109251022338\n",
      "Batch 645,  loss: 1.3885205507278442\n",
      "Batch 650,  loss: 1.2865922927856446\n",
      "Batch 655,  loss: 1.9285178422927856\n",
      "Batch 660,  loss: 1.4462298631668091\n",
      "Batch 665,  loss: 1.5683278799057008\n",
      "Batch 670,  loss: 1.6440642118453979\n",
      "Batch 675,  loss: 1.243191170692444\n",
      "Batch 680,  loss: 1.7524280786514281\n",
      "Batch 685,  loss: 1.209417724609375\n",
      "Batch 690,  loss: 1.3411893367767334\n",
      "Batch 695,  loss: 1.3233529329299927\n",
      "Batch 700,  loss: 1.047538447380066\n",
      "Batch 705,  loss: 1.6404544353485107\n",
      "Batch 710,  loss: 1.3497994184494018\n",
      "Batch 715,  loss: 1.307194185256958\n",
      "Batch 720,  loss: 1.563890242576599\n",
      "Batch 725,  loss: 1.312786328792572\n",
      "Batch 730,  loss: 1.717273736000061\n",
      "Batch 735,  loss: 1.646359348297119\n",
      "Batch 740,  loss: 1.5034764766693116\n",
      "Batch 745,  loss: 1.4689724683761596\n",
      "Batch 750,  loss: 1.453070843219757\n",
      "Batch 755,  loss: 1.6520489931106568\n",
      "Batch 760,  loss: 1.3900259256362915\n",
      "Batch 765,  loss: 1.5671296834945678\n",
      "Batch 770,  loss: 1.409714937210083\n",
      "Batch 775,  loss: 1.3864003419876099\n",
      "Batch 780,  loss: 1.2924025774002075\n",
      "Batch 785,  loss: 1.6603701829910278\n",
      "Batch 790,  loss: 1.2919310569763183\n",
      "Batch 795,  loss: 1.2893684148788451\n",
      "Batch 800,  loss: 1.3715098977088929\n",
      "Batch 805,  loss: 1.2727392435073852\n",
      "Batch 810,  loss: 1.267139744758606\n",
      "Batch 815,  loss: 1.316735827922821\n",
      "Batch 820,  loss: 1.8143561601638794\n",
      "Batch 825,  loss: 1.3512236595153808\n",
      "Batch 830,  loss: 1.5783460855484008\n",
      "Batch 835,  loss: 1.3920626044273376\n",
      "Batch 840,  loss: 1.347772240638733\n",
      "Batch 845,  loss: 1.2136977910995483\n",
      "Batch 850,  loss: 1.030494511127472\n",
      "Batch 855,  loss: 1.3657665967941284\n",
      "Batch 860,  loss: 1.511117696762085\n",
      "Batch 865,  loss: 1.24847514629364\n",
      "Batch 870,  loss: 1.4284707069396974\n",
      "Batch 875,  loss: 1.6717400550842285\n",
      "Batch 880,  loss: 1.672846245765686\n",
      "Batch 885,  loss: 1.3224286556243896\n",
      "Batch 890,  loss: 1.3807873010635376\n",
      "Batch 895,  loss: 1.3978826761245728\n",
      "Batch 900,  loss: 1.5120418310165404\n",
      "Batch 905,  loss: 1.4506624698638917\n",
      "Batch 910,  loss: 1.8798921585083008\n",
      "Batch 915,  loss: 1.5656446456909179\n",
      "Batch 920,  loss: 1.4400826215744018\n",
      "Batch 925,  loss: 1.4561384797096253\n",
      "Batch 930,  loss: 1.8019373655319213\n",
      "Batch 935,  loss: 1.4182402610778808\n",
      "Batch 940,  loss: 1.4980597734451293\n",
      "Batch 945,  loss: 1.5053208112716674\n",
      "Batch 950,  loss: 1.469593071937561\n",
      "Batch 955,  loss: 1.4522457361221313\n",
      "Batch 960,  loss: 1.557698130607605\n",
      "Batch 965,  loss: 1.2395928263664246\n",
      "Batch 970,  loss: 1.726179027557373\n",
      "Batch 975,  loss: 1.5797366142272948\n",
      "Batch 980,  loss: 1.3471834301948546\n",
      "Batch 985,  loss: 1.171964430809021\n",
      "Batch 990,  loss: 1.5227613687515258\n",
      "Batch 995,  loss: 1.2381179332733154\n",
      "Batch 1000,  loss: 1.3500635862350463\n",
      "Batch 1005,  loss: 1.4043187379837037\n",
      "Batch 1010,  loss: 1.4829283952713013\n",
      "Batch 1015,  loss: 1.6695681810379028\n",
      "Batch 1020,  loss: 1.3985113859176637\n",
      "Batch 1025,  loss: 1.8283196687698364\n",
      "Batch 1030,  loss: 1.322267746925354\n",
      "Batch 1035,  loss: 1.4605587720870972\n",
      "Batch 1040,  loss: 1.3772639274597167\n",
      "Batch 1045,  loss: 1.1843734741210938\n",
      "Batch 1050,  loss: 1.3772016763687134\n",
      "Batch 1055,  loss: 1.3201636791229248\n",
      "Batch 1060,  loss: 1.3829277276992797\n",
      "Batch 1065,  loss: 1.244411301612854\n",
      "Batch 1070,  loss: 1.56083025932312\n",
      "Batch 1075,  loss: 1.7149375438690186\n",
      "Batch 1080,  loss: 1.2480547070503234\n",
      "Batch 1085,  loss: 1.4040223360061646\n",
      "Batch 1090,  loss: 1.2147100925445558\n",
      "Batch 1095,  loss: 1.4302426099777221\n",
      "Batch 1100,  loss: 1.4140167236328125\n",
      "Batch 1105,  loss: 1.4355730295181275\n",
      "Batch 1110,  loss: 1.2933868527412415\n",
      "Batch 1115,  loss: 1.8633461475372315\n",
      "Batch 1120,  loss: 1.3640260338783263\n",
      "Batch 1125,  loss: 1.3704179763793944\n",
      "Batch 1130,  loss: 1.2841097116470337\n",
      "Batch 1135,  loss: 1.6164833307266235\n",
      "Batch 1140,  loss: 1.1844172954559327\n",
      "Batch 1145,  loss: 1.525788426399231\n",
      "Batch 1150,  loss: 1.3775773882865905\n",
      "Batch 1155,  loss: 1.3730953693389893\n",
      "Batch 1160,  loss: 1.5716073989868165\n",
      "Batch 1165,  loss: 1.5538896322250366\n",
      "Batch 1170,  loss: 1.536525845527649\n",
      "Batch 1175,  loss: 1.4879725933074952\n",
      "Batch 1180,  loss: 1.543158197402954\n",
      "Batch 1185,  loss: 1.4024861931800843\n",
      "Batch 1190,  loss: 1.5726929426193237\n",
      "Batch 1195,  loss: 1.540364718437195\n",
      "Batch 1200,  loss: 1.6338257551193238\n",
      "Batch 1205,  loss: 1.3501389503479004\n",
      "Batch 1210,  loss: 1.5180506348609923\n",
      "Batch 1215,  loss: 1.3189096450805664\n",
      "Batch 1220,  loss: 1.4248961687088013\n",
      "Batch 1225,  loss: 1.4921692132949829\n",
      "Batch 1230,  loss: 1.6247445583343505\n",
      "Batch 1235,  loss: 1.4741278648376466\n",
      "Batch 1240,  loss: 1.6079565048217774\n",
      "Batch 1245,  loss: 1.221541738510132\n",
      "Batch 1250,  loss: 1.4891041755676269\n",
      "Batch 1255,  loss: 1.6603755116462708\n",
      "Batch 1260,  loss: 1.8750523805618287\n",
      "Batch 1265,  loss: 1.495072078704834\n",
      "Batch 1270,  loss: 1.2895169496536254\n",
      "Batch 1275,  loss: 1.5830731391906738\n",
      "Batch 1280,  loss: 1.5895286083221436\n",
      "Batch 1285,  loss: 1.403749203681946\n",
      "Batch 1290,  loss: 1.391975736618042\n",
      "Batch 1295,  loss: 1.829910898208618\n",
      "LOSS train 1.829910898208618. Validation loss: 2.3754248526361255 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 47:\n",
      "Batch 5,  loss: 1.2572542190551759\n",
      "Batch 10,  loss: 1.7903024911880494\n",
      "Batch 15,  loss: 1.326358938217163\n",
      "Batch 20,  loss: 1.1534643769264221\n",
      "Batch 25,  loss: 1.352796983718872\n",
      "Batch 30,  loss: 1.643291449546814\n",
      "Batch 35,  loss: 1.414227557182312\n",
      "Batch 40,  loss: 1.4281554222106934\n",
      "Batch 45,  loss: 1.1355837345123292\n",
      "Batch 50,  loss: 1.4113418102264403\n",
      "Batch 55,  loss: 1.4865411043167114\n",
      "Batch 60,  loss: 1.458763885498047\n",
      "Batch 65,  loss: 1.4707451343536377\n",
      "Batch 70,  loss: 1.402243423461914\n",
      "Batch 75,  loss: 1.4791094779968261\n",
      "Batch 80,  loss: 1.4899545192718506\n",
      "Batch 85,  loss: 1.4263335466384888\n",
      "Batch 90,  loss: 1.4415230989456176\n",
      "Batch 95,  loss: 1.2910357236862182\n",
      "Batch 100,  loss: 1.2537497282028198\n",
      "Batch 105,  loss: 1.4530307531356812\n",
      "Batch 110,  loss: 1.293791151046753\n",
      "Batch 115,  loss: 1.2847380995750428\n",
      "Batch 120,  loss: 1.5576229095458984\n",
      "Batch 125,  loss: 1.4979626059532165\n",
      "Batch 130,  loss: 1.2727028369903564\n",
      "Batch 135,  loss: 1.5215382814407348\n",
      "Batch 140,  loss: 1.3752207994461059\n",
      "Batch 145,  loss: 1.4164786100387574\n",
      "Batch 150,  loss: 1.609588623046875\n",
      "Batch 155,  loss: 1.119068682193756\n",
      "Batch 160,  loss: 1.4420926570892334\n",
      "Batch 165,  loss: 1.4304892539978027\n",
      "Batch 170,  loss: 1.8182918310165406\n",
      "Batch 175,  loss: 1.689765167236328\n",
      "Batch 180,  loss: 1.238206422328949\n",
      "Batch 185,  loss: 1.2602448225021363\n",
      "Batch 190,  loss: 1.3541539907455444\n",
      "Batch 195,  loss: 1.4459761381149292\n",
      "Batch 200,  loss: 1.2519604086875915\n",
      "Batch 205,  loss: 1.489722204208374\n",
      "Batch 210,  loss: 1.2846783995628357\n",
      "Batch 215,  loss: 1.2805135846138\n",
      "Batch 220,  loss: 1.3358343601226808\n",
      "Batch 225,  loss: 1.5979340076446533\n",
      "Batch 230,  loss: 1.3685145378112793\n",
      "Batch 235,  loss: 1.5261146783828736\n",
      "Batch 240,  loss: 1.715911364555359\n",
      "Batch 245,  loss: 1.2790706038475037\n",
      "Batch 250,  loss: 1.5073483467102051\n",
      "Batch 255,  loss: 1.5103173732757569\n",
      "Batch 260,  loss: 1.4809749841690063\n",
      "Batch 265,  loss: 1.3732699871063232\n",
      "Batch 270,  loss: 1.3409594297409058\n",
      "Batch 275,  loss: 1.767621111869812\n",
      "Batch 280,  loss: 1.5710012674331666\n",
      "Batch 285,  loss: 1.1427353620529175\n",
      "Batch 290,  loss: 1.8094327449798584\n",
      "Batch 295,  loss: 1.5831279754638672\n",
      "Batch 300,  loss: 1.353143608570099\n",
      "Batch 305,  loss: 1.3726717352867126\n",
      "Batch 310,  loss: 1.600820577144623\n",
      "Batch 315,  loss: 1.3458951711654663\n",
      "Batch 320,  loss: 1.4458869218826294\n",
      "Batch 325,  loss: 1.346052384376526\n",
      "Batch 330,  loss: 1.315353798866272\n",
      "Batch 335,  loss: 1.5877252101898194\n",
      "Batch 340,  loss: 1.4849807500839234\n",
      "Batch 345,  loss: 1.5823121309280395\n",
      "Batch 350,  loss: 1.4492104768753051\n",
      "Batch 355,  loss: 1.2628436803817749\n",
      "Batch 360,  loss: 1.1780899047851563\n",
      "Batch 365,  loss: 1.2221681833267213\n",
      "Batch 370,  loss: 1.3776318073272704\n",
      "Batch 375,  loss: 1.5037675619125366\n",
      "Batch 380,  loss: 1.4454078435897828\n",
      "Batch 385,  loss: 1.3941468000411987\n",
      "Batch 390,  loss: 1.1261152505874634\n",
      "Batch 395,  loss: 1.811531138420105\n",
      "Batch 400,  loss: 1.472430658340454\n",
      "Batch 405,  loss: 1.2889618158340455\n",
      "Batch 410,  loss: 1.5738531351089478\n",
      "Batch 415,  loss: 1.3983161449432373\n",
      "Batch 420,  loss: 1.439212727546692\n",
      "Batch 425,  loss: 1.5706633090972901\n",
      "Batch 430,  loss: 1.458360743522644\n",
      "Batch 435,  loss: 1.5361185312271117\n",
      "Batch 440,  loss: 1.616083550453186\n",
      "Batch 445,  loss: 1.3676610946655274\n",
      "Batch 450,  loss: 1.5869160652160645\n",
      "Batch 455,  loss: 1.4151381015777589\n",
      "Batch 460,  loss: 1.721812653541565\n",
      "Batch 465,  loss: 1.5817758321762085\n",
      "Batch 470,  loss: 1.1038324594497682\n",
      "Batch 475,  loss: 1.3248990058898926\n",
      "Batch 480,  loss: 1.6591974020004272\n",
      "Batch 485,  loss: 1.783319854736328\n",
      "Batch 490,  loss: 1.294543242454529\n",
      "Batch 495,  loss: 1.4294012308120727\n",
      "Batch 500,  loss: 1.4864533424377442\n",
      "Batch 505,  loss: 1.5995067596435546\n",
      "Batch 510,  loss: 1.2188129663467406\n",
      "Batch 515,  loss: 1.3222169160842896\n",
      "Batch 520,  loss: 1.7391414165496826\n",
      "Batch 525,  loss: 1.0787704467773438\n",
      "Batch 530,  loss: 1.4603781700134277\n",
      "Batch 535,  loss: 1.5871171951293945\n",
      "Batch 540,  loss: 1.4221030950546265\n",
      "Batch 545,  loss: 1.498716688156128\n",
      "Batch 550,  loss: 1.5948861360549926\n",
      "Batch 555,  loss: 1.094240629673004\n",
      "Batch 560,  loss: 1.5006913900375367\n",
      "Batch 565,  loss: 1.570441484451294\n",
      "Batch 570,  loss: 1.09863623380661\n",
      "Batch 575,  loss: 1.5184444189071655\n",
      "Batch 580,  loss: 1.6646300196647643\n",
      "Batch 585,  loss: 1.4143942594528198\n",
      "Batch 590,  loss: 1.410013747215271\n",
      "Batch 595,  loss: 1.3450839042663574\n",
      "Batch 600,  loss: 1.4369740247726441\n",
      "Batch 605,  loss: 1.7987369656562806\n",
      "Batch 610,  loss: 1.3038480043411256\n",
      "Batch 615,  loss: 1.642622995376587\n",
      "Batch 620,  loss: 1.4686461210250854\n",
      "Batch 625,  loss: 1.33272944688797\n",
      "Batch 630,  loss: 1.326405906677246\n",
      "Batch 635,  loss: 1.6082828521728516\n",
      "Batch 640,  loss: 1.4447177648544312\n",
      "Batch 645,  loss: 1.347988533973694\n",
      "Batch 650,  loss: 1.638451838493347\n",
      "Batch 655,  loss: 1.7896550655364991\n",
      "Batch 660,  loss: 1.4720090866088866\n",
      "Batch 665,  loss: 1.7176633834838868\n",
      "Batch 670,  loss: 1.5456294775009156\n",
      "Batch 675,  loss: 1.5331892251968384\n",
      "Batch 680,  loss: 1.3968299627304077\n",
      "Batch 685,  loss: 1.2051712632179261\n",
      "Batch 690,  loss: 1.2774111032485962\n",
      "Batch 695,  loss: 1.6665208697319032\n",
      "Batch 700,  loss: 1.4520979404449463\n",
      "Batch 705,  loss: 1.3507827520370483\n",
      "Batch 710,  loss: 1.3226924896240235\n",
      "Batch 715,  loss: 1.6983271598815919\n",
      "Batch 720,  loss: 1.6651426076889038\n",
      "Batch 725,  loss: 1.728325867652893\n",
      "Batch 730,  loss: 1.9066666603088378\n",
      "Batch 735,  loss: 1.4670713424682618\n",
      "Batch 740,  loss: 1.7212314367294312\n",
      "Batch 745,  loss: 1.5456975460052491\n",
      "Batch 750,  loss: 1.4359780073165893\n",
      "Batch 755,  loss: 1.5023492336273194\n",
      "Batch 760,  loss: 1.35130832195282\n",
      "Batch 765,  loss: 1.2210175752639771\n",
      "Batch 770,  loss: 1.635198998451233\n",
      "Batch 775,  loss: 1.3889349341392516\n",
      "Batch 780,  loss: 1.4422317504882813\n",
      "Batch 785,  loss: 1.4429984092712402\n",
      "Batch 790,  loss: 1.5303604125976562\n",
      "Batch 795,  loss: 1.6197125911712646\n",
      "Batch 800,  loss: 1.3083916425704956\n",
      "Batch 805,  loss: 1.5354539394378661\n",
      "Batch 810,  loss: 1.6376136302948\n",
      "Batch 815,  loss: 1.3362343549728393\n",
      "Batch 820,  loss: 1.443805956840515\n",
      "Batch 825,  loss: 1.3679197549819946\n",
      "Batch 830,  loss: 1.5207366228103638\n",
      "Batch 835,  loss: 1.2763100862503052\n",
      "Batch 840,  loss: 1.70267653465271\n",
      "Batch 845,  loss: 1.4093087911605835\n",
      "Batch 850,  loss: 1.3791877031326294\n",
      "Batch 855,  loss: 1.60770263671875\n",
      "Batch 860,  loss: 1.621910524368286\n",
      "Batch 865,  loss: 1.7069905757904054\n",
      "Batch 870,  loss: 1.5354085445404053\n",
      "Batch 875,  loss: 1.6304651498794556\n",
      "Batch 880,  loss: 1.222127652168274\n",
      "Batch 885,  loss: 1.1569341659545898\n",
      "Batch 890,  loss: 1.4479009509086609\n",
      "Batch 895,  loss: 1.7100972652435302\n",
      "Batch 900,  loss: 1.6759213924407959\n",
      "Batch 905,  loss: 1.5499500274658202\n",
      "Batch 910,  loss: 1.3732299327850341\n",
      "Batch 915,  loss: 1.3557574510574342\n",
      "Batch 920,  loss: 1.3785952091217042\n",
      "Batch 925,  loss: 1.3969724655151368\n",
      "Batch 930,  loss: 1.4434321165084838\n",
      "Batch 935,  loss: 0.9750191450119019\n",
      "Batch 940,  loss: 1.2632152318954468\n",
      "Batch 945,  loss: 1.819922924041748\n",
      "Batch 950,  loss: 1.4032676577568055\n",
      "Batch 955,  loss: 1.5224363565444947\n",
      "Batch 960,  loss: 1.3079174637794495\n",
      "Batch 965,  loss: 1.2090649127960205\n",
      "Batch 970,  loss: 1.5990269660949707\n",
      "Batch 975,  loss: 1.2747462749481202\n",
      "Batch 980,  loss: 1.723612642288208\n",
      "Batch 985,  loss: 1.4532103180885314\n",
      "Batch 990,  loss: 1.4601391673088073\n",
      "Batch 995,  loss: 1.1304258346557616\n",
      "Batch 1000,  loss: 1.3443073987960816\n",
      "Batch 1005,  loss: 1.4854636907577514\n",
      "Batch 1010,  loss: 1.6806869506835938\n",
      "Batch 1015,  loss: 1.4672473192214965\n",
      "Batch 1020,  loss: 1.3937074422836304\n",
      "Batch 1025,  loss: 1.5219212770462036\n",
      "Batch 1030,  loss: 1.7063119888305665\n",
      "Batch 1035,  loss: 1.5166000843048095\n",
      "Batch 1040,  loss: 1.6165991306304932\n",
      "Batch 1045,  loss: 1.279293704032898\n",
      "Batch 1050,  loss: 1.6286315202713013\n",
      "Batch 1055,  loss: 1.5037544012069701\n",
      "Batch 1060,  loss: 1.2226400256156922\n",
      "Batch 1065,  loss: 1.4801949501037597\n",
      "Batch 1070,  loss: 1.4108831405639648\n",
      "Batch 1075,  loss: 1.2343639612197876\n",
      "Batch 1080,  loss: 1.3717774868011474\n",
      "Batch 1085,  loss: 1.5595006227493287\n",
      "Batch 1090,  loss: 1.5238690614700316\n",
      "Batch 1095,  loss: 1.4421831607818603\n",
      "Batch 1100,  loss: 1.6142467975616455\n",
      "Batch 1105,  loss: 1.5687429904937744\n",
      "Batch 1110,  loss: 1.5760572671890258\n",
      "Batch 1115,  loss: 1.5830538988113403\n",
      "Batch 1120,  loss: 1.3447292804718018\n",
      "Batch 1125,  loss: 1.555941653251648\n",
      "Batch 1130,  loss: 1.507266092300415\n",
      "Batch 1135,  loss: 1.7351226687431336\n",
      "Batch 1140,  loss: 1.5628883361816406\n",
      "Batch 1145,  loss: 1.3974413633346559\n",
      "Batch 1150,  loss: 1.4022859334945679\n",
      "Batch 1155,  loss: 1.3826443195343017\n",
      "Batch 1160,  loss: 1.3509574055671691\n",
      "Batch 1165,  loss: 1.4759026288986206\n",
      "Batch 1170,  loss: 1.449788463115692\n",
      "Batch 1175,  loss: 1.256076717376709\n",
      "Batch 1180,  loss: 1.4503987312316895\n",
      "Batch 1185,  loss: 1.5617957353591918\n",
      "Batch 1190,  loss: 1.7427233934402466\n",
      "Batch 1195,  loss: 1.5370608210563659\n",
      "Batch 1200,  loss: 1.3361599683761596\n",
      "Batch 1205,  loss: 1.5543503284454345\n",
      "Batch 1210,  loss: 1.5413830995559692\n",
      "Batch 1215,  loss: 1.695845365524292\n",
      "Batch 1220,  loss: 1.441006588935852\n",
      "Batch 1225,  loss: 1.4059698343276978\n",
      "Batch 1230,  loss: 2.206871271133423\n",
      "Batch 1235,  loss: 1.364429521560669\n",
      "Batch 1240,  loss: 1.3945446252822875\n",
      "Batch 1245,  loss: 1.352781069278717\n",
      "Batch 1250,  loss: 1.4439713954925537\n",
      "Batch 1255,  loss: 1.3801414489746093\n",
      "Batch 1260,  loss: 1.2772258520126343\n",
      "Batch 1265,  loss: 1.4606419563293458\n",
      "Batch 1270,  loss: 1.7607873916625976\n",
      "Batch 1275,  loss: 1.4121594905853272\n",
      "Batch 1280,  loss: 1.676984977722168\n",
      "Batch 1285,  loss: 1.3608144283294679\n",
      "Batch 1290,  loss: 1.5111520767211915\n",
      "Batch 1295,  loss: 1.3896613836288452\n",
      "LOSS train 1.3896613836288452. Validation loss: 2.1397064613385335 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 48:\n",
      "Batch 5,  loss: 1.7213872432708741\n",
      "Batch 10,  loss: 1.3331381797790527\n",
      "Batch 15,  loss: 1.5195276498794557\n",
      "Batch 20,  loss: 1.1937701940536498\n",
      "Batch 25,  loss: 1.315537691116333\n",
      "Batch 30,  loss: 1.658615517616272\n",
      "Batch 35,  loss: 1.5140007376670837\n",
      "Batch 40,  loss: 1.4946455001831054\n",
      "Batch 45,  loss: 1.3474129915237427\n",
      "Batch 50,  loss: 1.5371876239776612\n",
      "Batch 55,  loss: 1.4309438228607179\n",
      "Batch 60,  loss: 1.3592697620391845\n",
      "Batch 65,  loss: 1.4531184911727906\n",
      "Batch 70,  loss: 1.4186457633972167\n",
      "Batch 75,  loss: 1.1153151512145996\n",
      "Batch 80,  loss: 1.4682350277900695\n",
      "Batch 85,  loss: 1.5282960891723634\n",
      "Batch 90,  loss: 1.5585054874420166\n",
      "Batch 95,  loss: 1.273184061050415\n",
      "Batch 100,  loss: 1.4059982180595398\n",
      "Batch 105,  loss: 1.5804720640182495\n",
      "Batch 110,  loss: 1.536855411529541\n",
      "Batch 115,  loss: 1.4949634313583373\n",
      "Batch 120,  loss: 1.40267333984375\n",
      "Batch 125,  loss: 1.4822542190551757\n",
      "Batch 130,  loss: 1.471123719215393\n",
      "Batch 135,  loss: 1.5795875430107116\n",
      "Batch 140,  loss: 1.562007212638855\n",
      "Batch 145,  loss: 1.4826513528823853\n",
      "Batch 150,  loss: 1.4807809114456176\n",
      "Batch 155,  loss: 1.5041391611099244\n",
      "Batch 160,  loss: 1.3054503440856933\n",
      "Batch 165,  loss: 1.2651714086532593\n",
      "Batch 170,  loss: 1.2360912322998048\n",
      "Batch 175,  loss: 1.3495917081832887\n",
      "Batch 180,  loss: 1.6183059692382813\n",
      "Batch 185,  loss: 1.459284818172455\n",
      "Batch 190,  loss: 1.436797034740448\n",
      "Batch 195,  loss: 1.6462836384773254\n",
      "Batch 200,  loss: 1.5546052217483521\n",
      "Batch 205,  loss: 1.7437604665756226\n",
      "Batch 210,  loss: 1.3080167770385742\n",
      "Batch 215,  loss: 1.5964313268661499\n",
      "Batch 220,  loss: 1.2205098628997804\n",
      "Batch 225,  loss: 1.3682350873947144\n",
      "Batch 230,  loss: 1.400488805770874\n",
      "Batch 235,  loss: 1.7538387775421143\n",
      "Batch 240,  loss: 1.2135069608688354\n",
      "Batch 245,  loss: 1.2087308168411255\n",
      "Batch 250,  loss: 1.4429929733276368\n",
      "Batch 255,  loss: 1.6226396799087524\n",
      "Batch 260,  loss: 1.2595703721046447\n",
      "Batch 265,  loss: 1.2662776350975036\n",
      "Batch 270,  loss: 1.2478907108306885\n",
      "Batch 275,  loss: 1.2516744017601014\n",
      "Batch 280,  loss: 1.383630132675171\n",
      "Batch 285,  loss: 1.3505524158477784\n",
      "Batch 290,  loss: 1.4255295515060424\n",
      "Batch 295,  loss: 1.5826996564865112\n",
      "Batch 300,  loss: 1.1976234674453736\n",
      "Batch 305,  loss: 1.375457727909088\n",
      "Batch 310,  loss: 1.5872496366500854\n",
      "Batch 315,  loss: 1.1775878310203551\n",
      "Batch 320,  loss: 1.5193605303764344\n",
      "Batch 325,  loss: 1.5859561681747436\n",
      "Batch 330,  loss: 1.3303621530532836\n",
      "Batch 335,  loss: 1.2214987874031067\n",
      "Batch 340,  loss: 1.3134079933166505\n",
      "Batch 345,  loss: 1.399762761592865\n",
      "Batch 350,  loss: 1.617933464050293\n",
      "Batch 355,  loss: 1.2374936580657958\n",
      "Batch 360,  loss: 1.3342801451683044\n",
      "Batch 365,  loss: 1.3115261316299438\n",
      "Batch 370,  loss: 1.2389130234718322\n",
      "Batch 375,  loss: 1.3902913570404052\n",
      "Batch 380,  loss: 1.5267587184906006\n",
      "Batch 385,  loss: 1.4094877004623414\n",
      "Batch 390,  loss: 1.7142564058303833\n",
      "Batch 395,  loss: 1.5062513589859008\n",
      "Batch 400,  loss: 1.4723942279815674\n",
      "Batch 405,  loss: 1.5586843490600586\n",
      "Batch 410,  loss: 1.2146315336227418\n",
      "Batch 415,  loss: 1.3497305750846862\n",
      "Batch 420,  loss: 1.4715692043304442\n",
      "Batch 425,  loss: 1.7574243068695068\n",
      "Batch 430,  loss: 1.5435067415237427\n",
      "Batch 435,  loss: 1.251663899421692\n",
      "Batch 440,  loss: 1.5948221206665039\n",
      "Batch 445,  loss: 1.7629853725433349\n",
      "Batch 450,  loss: 1.237837839126587\n",
      "Batch 455,  loss: 1.2519793272018434\n",
      "Batch 460,  loss: 1.338500714302063\n",
      "Batch 465,  loss: 1.3287163972854614\n",
      "Batch 470,  loss: 1.403535509109497\n",
      "Batch 475,  loss: 1.280992293357849\n",
      "Batch 480,  loss: 1.435315155982971\n",
      "Batch 485,  loss: 1.5528483390808105\n",
      "Batch 490,  loss: 1.5790222883224487\n",
      "Batch 495,  loss: 1.3763459205627442\n",
      "Batch 500,  loss: 1.2784736514091493\n",
      "Batch 505,  loss: 1.3858351707458496\n",
      "Batch 510,  loss: 1.5308476209640502\n",
      "Batch 515,  loss: 1.4813288569450378\n",
      "Batch 520,  loss: 1.7019614934921266\n",
      "Batch 525,  loss: 1.2202808260917664\n",
      "Batch 530,  loss: 1.344901943206787\n",
      "Batch 535,  loss: 1.4233994007110595\n",
      "Batch 540,  loss: 1.4421043753623963\n",
      "Batch 545,  loss: 1.6997066497802735\n",
      "Batch 550,  loss: 1.7157646894454956\n",
      "Batch 555,  loss: 1.6008618235588075\n",
      "Batch 560,  loss: 1.6044113874435424\n",
      "Batch 565,  loss: 1.6553720235824585\n",
      "Batch 570,  loss: 1.5155832290649414\n",
      "Batch 575,  loss: 1.7597854614257813\n",
      "Batch 580,  loss: 1.3079527378082276\n",
      "Batch 585,  loss: 1.3864412307739258\n",
      "Batch 590,  loss: 1.4473751783370972\n",
      "Batch 595,  loss: 1.1779261827468872\n",
      "Batch 600,  loss: 1.2196602821350098\n",
      "Batch 605,  loss: 1.6580994606018067\n",
      "Batch 610,  loss: 1.7214617013931275\n",
      "Batch 615,  loss: 1.3426998376846313\n",
      "Batch 620,  loss: 1.469284760951996\n",
      "Batch 625,  loss: 1.5737510204315186\n",
      "Batch 630,  loss: 1.4899885177612304\n",
      "Batch 635,  loss: 1.2654297471046447\n",
      "Batch 640,  loss: 1.4204771161079406\n",
      "Batch 645,  loss: 1.6990670442581177\n",
      "Batch 650,  loss: 1.5155765533447265\n",
      "Batch 655,  loss: 1.4222737908363343\n",
      "Batch 660,  loss: 1.486486792564392\n",
      "Batch 665,  loss: 1.6243685245513917\n",
      "Batch 670,  loss: 1.3376221895217895\n",
      "Batch 675,  loss: 1.2937358736991882\n",
      "Batch 680,  loss: 1.7555421113967895\n",
      "Batch 685,  loss: 1.5224661827087402\n",
      "Batch 690,  loss: 1.5900765180587768\n",
      "Batch 695,  loss: 1.702133870124817\n",
      "Batch 700,  loss: 1.7440374374389649\n",
      "Batch 705,  loss: 1.301581358909607\n",
      "Batch 710,  loss: 1.3634170532226562\n",
      "Batch 715,  loss: 1.6014256715774535\n",
      "Batch 720,  loss: 1.4725090503692626\n",
      "Batch 725,  loss: 1.473627996444702\n",
      "Batch 730,  loss: 1.4223360538482666\n",
      "Batch 735,  loss: 1.244485092163086\n",
      "Batch 740,  loss: 1.3259737730026244\n",
      "Batch 745,  loss: 1.474867868423462\n",
      "Batch 750,  loss: 1.3737659931182862\n",
      "Batch 755,  loss: 1.6490694761276246\n",
      "Batch 760,  loss: 1.4409316778182983\n",
      "Batch 765,  loss: 1.2774755716323853\n",
      "Batch 770,  loss: 1.1115907907485962\n",
      "Batch 775,  loss: 1.3488191604614257\n",
      "Batch 780,  loss: 1.3630820512771606\n",
      "Batch 785,  loss: 1.436148452758789\n",
      "Batch 790,  loss: 1.2748250484466552\n",
      "Batch 795,  loss: 1.539467692375183\n",
      "Batch 800,  loss: 1.3225883722305298\n",
      "Batch 805,  loss: 1.3232649326324464\n",
      "Batch 810,  loss: 1.190896475315094\n",
      "Batch 815,  loss: 1.673431396484375\n",
      "Batch 820,  loss: 1.5364593744277955\n",
      "Batch 825,  loss: 1.8549503564834595\n",
      "Batch 830,  loss: 1.454188048839569\n",
      "Batch 835,  loss: 1.4420530319213867\n",
      "Batch 840,  loss: 1.4222878456115722\n",
      "Batch 845,  loss: 1.2301198959350585\n",
      "Batch 850,  loss: 1.1672020316123963\n",
      "Batch 855,  loss: 1.266985034942627\n",
      "Batch 860,  loss: 1.8141165256500245\n",
      "Batch 865,  loss: 1.715321922302246\n",
      "Batch 870,  loss: 1.6959150075912475\n",
      "Batch 875,  loss: 1.3627435684204101\n",
      "Batch 880,  loss: 1.6660931825637817\n",
      "Batch 885,  loss: 1.447231698036194\n",
      "Batch 890,  loss: 1.5357714653015138\n",
      "Batch 895,  loss: 1.3836889743804932\n",
      "Batch 900,  loss: 1.283853530883789\n",
      "Batch 905,  loss: 1.2771116852760316\n",
      "Batch 910,  loss: 1.478602361679077\n",
      "Batch 915,  loss: 1.4614329576492309\n",
      "Batch 920,  loss: 1.3872246980667113\n",
      "Batch 925,  loss: 1.3128267049789428\n",
      "Batch 930,  loss: 1.2393216371536255\n",
      "Batch 935,  loss: 1.6217983961105347\n",
      "Batch 940,  loss: 1.4856253385543823\n",
      "Batch 945,  loss: 1.2586738348007203\n",
      "Batch 950,  loss: 1.4882002830505372\n",
      "Batch 955,  loss: 1.3015294551849366\n",
      "Batch 960,  loss: 1.4316523194313049\n",
      "Batch 965,  loss: 1.5442132472991943\n",
      "Batch 970,  loss: 1.3013299703598022\n",
      "Batch 975,  loss: 1.4539296746253967\n",
      "Batch 980,  loss: 1.391635775566101\n",
      "Batch 985,  loss: 1.4132944345474243\n",
      "Batch 990,  loss: 1.3910810708999635\n",
      "Batch 995,  loss: 1.5186230659484863\n",
      "Batch 1000,  loss: 1.3170751571655273\n",
      "Batch 1005,  loss: 1.2608580112457275\n",
      "Batch 1010,  loss: 1.4118161916732788\n",
      "Batch 1015,  loss: 1.244975197315216\n",
      "Batch 1020,  loss: 1.270953118801117\n",
      "Batch 1025,  loss: 1.250670349597931\n",
      "Batch 1030,  loss: 1.6939420461654664\n",
      "Batch 1035,  loss: 1.5283682942390442\n",
      "Batch 1040,  loss: 1.3847464323043823\n",
      "Batch 1045,  loss: 1.2906473278999329\n",
      "Batch 1050,  loss: 1.4045005798339845\n",
      "Batch 1055,  loss: 1.2485607385635376\n",
      "Batch 1060,  loss: 1.3821593046188354\n",
      "Batch 1065,  loss: 1.3828819036483764\n",
      "Batch 1070,  loss: 1.3885818004608155\n",
      "Batch 1075,  loss: 1.5067875385284424\n",
      "Batch 1080,  loss: 1.7404388904571533\n",
      "Batch 1085,  loss: 1.5287490844726563\n",
      "Batch 1090,  loss: 1.3372390270233154\n",
      "Batch 1095,  loss: 1.5719727277755737\n",
      "Batch 1100,  loss: 1.340620994567871\n",
      "Batch 1105,  loss: 1.4622892379760741\n",
      "Batch 1110,  loss: 1.4351835250854492\n",
      "Batch 1115,  loss: 1.6200897455215455\n",
      "Batch 1120,  loss: 1.445896530151367\n",
      "Batch 1125,  loss: 1.3691995859146118\n",
      "Batch 1130,  loss: 1.5093194961547851\n",
      "Batch 1135,  loss: 1.427140760421753\n",
      "Batch 1140,  loss: 1.3992301940917968\n",
      "Batch 1145,  loss: 1.3468196868896485\n",
      "Batch 1150,  loss: 1.8016065597534179\n",
      "Batch 1155,  loss: 1.46042857170105\n",
      "Batch 1160,  loss: 1.3395983457565308\n",
      "Batch 1165,  loss: 1.2811654210090637\n",
      "Batch 1170,  loss: 1.659090280532837\n",
      "Batch 1175,  loss: 1.8274258971214294\n",
      "Batch 1180,  loss: 1.3982589960098266\n",
      "Batch 1185,  loss: 1.5117044687271117\n",
      "Batch 1190,  loss: 1.3058194041252136\n",
      "Batch 1195,  loss: 1.3777214288711548\n",
      "Batch 1200,  loss: 1.3578240990638732\n",
      "Batch 1205,  loss: 1.5778668761253356\n",
      "Batch 1210,  loss: 1.3669264793395997\n",
      "Batch 1215,  loss: 1.4533055067062377\n",
      "Batch 1220,  loss: 1.5977867603302003\n",
      "Batch 1225,  loss: 1.4189550876617432\n",
      "Batch 1230,  loss: 1.6829394102096558\n",
      "Batch 1235,  loss: 1.4699578285217285\n",
      "Batch 1240,  loss: 1.3354772806167603\n",
      "Batch 1245,  loss: 1.6531513929367065\n",
      "Batch 1250,  loss: 1.5929120063781739\n",
      "Batch 1255,  loss: 1.4849664211273192\n",
      "Batch 1260,  loss: 1.4958961725234985\n",
      "Batch 1265,  loss: 1.2490567803382873\n",
      "Batch 1270,  loss: 1.4234413146972655\n",
      "Batch 1275,  loss: 1.4310171842575072\n",
      "Batch 1280,  loss: 1.5599843263626099\n",
      "Batch 1285,  loss: 1.3508136987686157\n",
      "Batch 1290,  loss: 1.755621862411499\n",
      "Batch 1295,  loss: 1.3693902969360352\n",
      "LOSS train 1.3693902969360352. Validation loss: 2.1381408370755337 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 49:\n",
      "Batch 5,  loss: 1.4570431232452392\n",
      "Batch 10,  loss: 1.4094130277633667\n",
      "Batch 15,  loss: 1.8050691604614257\n",
      "Batch 20,  loss: 1.3566797018051147\n",
      "Batch 25,  loss: 1.277633261680603\n",
      "Batch 30,  loss: 1.6140289545059203\n",
      "Batch 35,  loss: 1.3194490194320678\n",
      "Batch 40,  loss: 1.6223864793777465\n",
      "Batch 45,  loss: 1.2464678525924682\n",
      "Batch 50,  loss: 1.4129682064056397\n",
      "Batch 55,  loss: 1.1723614573478698\n",
      "Batch 60,  loss: 1.4079840660095215\n",
      "Batch 65,  loss: 1.435582685470581\n",
      "Batch 70,  loss: 1.4910739302635192\n",
      "Batch 75,  loss: 1.6588452339172364\n",
      "Batch 80,  loss: 1.423453140258789\n",
      "Batch 85,  loss: 1.540051031112671\n",
      "Batch 90,  loss: 1.3922967910766602\n",
      "Batch 95,  loss: 1.4025077104568482\n",
      "Batch 100,  loss: 1.2007509469985962\n",
      "Batch 105,  loss: 1.4882388591766358\n",
      "Batch 110,  loss: 1.5605264902114868\n",
      "Batch 115,  loss: 1.5033922910690307\n",
      "Batch 120,  loss: 1.1639806747436523\n",
      "Batch 125,  loss: 1.0704639911651612\n",
      "Batch 130,  loss: 1.24248206615448\n",
      "Batch 135,  loss: 1.6249847650527953\n",
      "Batch 140,  loss: 1.5029524564743042\n",
      "Batch 145,  loss: 1.6138200521469117\n",
      "Batch 150,  loss: 1.209505033493042\n",
      "Batch 155,  loss: 1.2468875646591187\n",
      "Batch 160,  loss: 1.6050063371658325\n",
      "Batch 165,  loss: 1.2312785744667054\n",
      "Batch 170,  loss: 1.4967637300491332\n",
      "Batch 175,  loss: 1.46658753156662\n",
      "Batch 180,  loss: 1.3314665794372558\n",
      "Batch 185,  loss: 1.5143184185028076\n",
      "Batch 190,  loss: 1.3099067449569701\n",
      "Batch 195,  loss: 1.502429234981537\n",
      "Batch 200,  loss: 1.1590508103370667\n",
      "Batch 205,  loss: 1.3108713388442994\n",
      "Batch 210,  loss: 1.582676601409912\n",
      "Batch 215,  loss: 1.0611709713935853\n",
      "Batch 220,  loss: 1.436877393722534\n",
      "Batch 225,  loss: 1.5419268608093262\n",
      "Batch 230,  loss: 1.3569831609725953\n",
      "Batch 235,  loss: 1.3532683372497558\n",
      "Batch 240,  loss: 1.3022752046585082\n",
      "Batch 245,  loss: 1.418443536758423\n",
      "Batch 250,  loss: 1.373624587059021\n",
      "Batch 255,  loss: 1.4784989833831788\n",
      "Batch 260,  loss: 1.3583606719970702\n",
      "Batch 265,  loss: 1.1730425596237182\n",
      "Batch 270,  loss: 1.307481622695923\n",
      "Batch 275,  loss: 1.5707027435302734\n",
      "Batch 280,  loss: 1.446725058555603\n",
      "Batch 285,  loss: 1.462362241744995\n",
      "Batch 290,  loss: 1.7264977931976317\n",
      "Batch 295,  loss: 1.334510326385498\n",
      "Batch 300,  loss: 1.2978002786636353\n",
      "Batch 305,  loss: 1.3829227685928345\n",
      "Batch 310,  loss: 1.4922779321670532\n",
      "Batch 315,  loss: 1.1854840755462646\n",
      "Batch 320,  loss: 2.1356048583984375\n",
      "Batch 325,  loss: 1.4023853540420532\n",
      "Batch 330,  loss: 1.1855880260467528\n",
      "Batch 335,  loss: 1.4840014457702637\n",
      "Batch 340,  loss: 1.3700671434402465\n",
      "Batch 345,  loss: 1.4290239572525025\n",
      "Batch 350,  loss: 1.5947582721710205\n",
      "Batch 355,  loss: 1.5461310505867005\n",
      "Batch 360,  loss: 1.3441006183624267\n",
      "Batch 365,  loss: 1.3311681985855102\n",
      "Batch 370,  loss: 1.4682274341583252\n",
      "Batch 375,  loss: 1.360022783279419\n",
      "Batch 380,  loss: 1.2331441402435304\n",
      "Batch 385,  loss: 1.4264944076538086\n",
      "Batch 390,  loss: 1.2001025438308717\n",
      "Batch 395,  loss: 1.1626387596130372\n",
      "Batch 400,  loss: 1.5874449014663696\n",
      "Batch 405,  loss: 1.4923846483230592\n",
      "Batch 410,  loss: 1.455814266204834\n",
      "Batch 415,  loss: 1.6858591079711913\n",
      "Batch 420,  loss: 1.3088283300399781\n",
      "Batch 425,  loss: 1.5253111600875855\n",
      "Batch 430,  loss: 1.3001495718955993\n",
      "Batch 435,  loss: 1.7035748481750488\n",
      "Batch 440,  loss: 1.6540887117385865\n",
      "Batch 445,  loss: 1.2850677490234375\n",
      "Batch 450,  loss: 1.79816814661026\n",
      "Batch 455,  loss: 1.4074077486991883\n",
      "Batch 460,  loss: 1.3240424633026122\n",
      "Batch 465,  loss: 1.4061553955078125\n",
      "Batch 470,  loss: 1.4436448574066163\n",
      "Batch 475,  loss: 1.3697608947753905\n",
      "Batch 480,  loss: 1.4670504093170167\n",
      "Batch 485,  loss: 1.4505907058715821\n",
      "Batch 490,  loss: 1.5842268228530885\n",
      "Batch 495,  loss: 1.2623031616210938\n",
      "Batch 500,  loss: 1.5947048664093018\n",
      "Batch 505,  loss: 1.1989575743675231\n",
      "Batch 510,  loss: 1.641341209411621\n",
      "Batch 515,  loss: 1.3879197835922241\n",
      "Batch 520,  loss: 1.363176441192627\n",
      "Batch 525,  loss: 1.724902081489563\n",
      "Batch 530,  loss: 1.3512621641159057\n",
      "Batch 535,  loss: 1.2778581500053405\n",
      "Batch 540,  loss: 1.18672456741333\n",
      "Batch 545,  loss: 1.368057870864868\n",
      "Batch 550,  loss: 1.2443803787231444\n",
      "Batch 555,  loss: 1.28786803483963\n",
      "Batch 560,  loss: 1.3931232929229735\n",
      "Batch 565,  loss: 1.285552167892456\n",
      "Batch 570,  loss: 1.1643432140350343\n",
      "Batch 575,  loss: 1.4253787517547607\n",
      "Batch 580,  loss: 1.2355485200881957\n",
      "Batch 585,  loss: 1.3209107398986817\n",
      "Batch 590,  loss: 1.4175297975540162\n",
      "Batch 595,  loss: 1.336885118484497\n",
      "Batch 600,  loss: 1.2377553939819337\n",
      "Batch 605,  loss: 1.3080693006515502\n",
      "Batch 610,  loss: 1.6886038303375244\n",
      "Batch 615,  loss: 1.5539029836654663\n",
      "Batch 620,  loss: 1.1636585235595702\n",
      "Batch 625,  loss: 1.4621381282806396\n",
      "Batch 630,  loss: 1.3406766176223754\n",
      "Batch 635,  loss: 1.7973313808441163\n",
      "Batch 640,  loss: 1.6845961093902588\n",
      "Batch 645,  loss: 1.6231723308563233\n",
      "Batch 650,  loss: 1.515054202079773\n",
      "Batch 655,  loss: 1.4025204181671143\n",
      "Batch 660,  loss: 1.363653302192688\n",
      "Batch 665,  loss: 1.5267420291900635\n",
      "Batch 670,  loss: 1.5697696089744568\n",
      "Batch 675,  loss: 1.3246522307395936\n",
      "Batch 680,  loss: 1.453460431098938\n",
      "Batch 685,  loss: 1.4690402507781983\n",
      "Batch 690,  loss: 1.4359772443771361\n",
      "Batch 695,  loss: 1.3451449632644654\n",
      "Batch 700,  loss: 1.5050705432891847\n",
      "Batch 705,  loss: 1.6786165475845336\n",
      "Batch 710,  loss: 1.5418797492980958\n",
      "Batch 715,  loss: 1.2743783831596374\n",
      "Batch 720,  loss: 1.1068148493766785\n",
      "Batch 725,  loss: 1.3857353925704956\n",
      "Batch 730,  loss: 1.66473491191864\n",
      "Batch 735,  loss: 1.3499748468399049\n",
      "Batch 740,  loss: 1.2517816305160523\n",
      "Batch 745,  loss: 1.59737548828125\n",
      "Batch 750,  loss: 1.3502925992012025\n",
      "Batch 755,  loss: 1.445299458503723\n",
      "Batch 760,  loss: 1.3780624628067017\n",
      "Batch 765,  loss: 1.544079852104187\n",
      "Batch 770,  loss: 1.6509798765182495\n",
      "Batch 775,  loss: 1.336875081062317\n",
      "Batch 780,  loss: 1.2885147333145142\n",
      "Batch 785,  loss: 1.3341730833053589\n",
      "Batch 790,  loss: 1.2843635201454162\n",
      "Batch 795,  loss: 1.1547436714172363\n",
      "Batch 800,  loss: 1.3778123617172242\n",
      "Batch 805,  loss: 1.4489089965820312\n",
      "Batch 810,  loss: 1.3672758102416993\n",
      "Batch 815,  loss: 1.4882801294326782\n",
      "Batch 820,  loss: 1.5451652765274049\n",
      "Batch 825,  loss: 1.7831418991088868\n",
      "Batch 830,  loss: 1.2572059273719787\n",
      "Batch 835,  loss: 1.4259361743927002\n",
      "Batch 840,  loss: 1.2482967257499695\n",
      "Batch 845,  loss: 1.4044331789016724\n",
      "Batch 850,  loss: 1.605866837501526\n",
      "Batch 855,  loss: 1.3176538825035096\n",
      "Batch 860,  loss: 1.449715757369995\n",
      "Batch 865,  loss: 1.5922099113464356\n",
      "Batch 870,  loss: 1.9253250360488892\n",
      "Batch 875,  loss: 1.4283822774887085\n",
      "Batch 880,  loss: 1.2354287266731263\n",
      "Batch 885,  loss: 1.5951658248901368\n",
      "Batch 890,  loss: 1.3769893169403076\n",
      "Batch 895,  loss: 1.4943197250366211\n",
      "Batch 900,  loss: 1.3729719877243043\n",
      "Batch 905,  loss: 1.7381788492202759\n",
      "Batch 910,  loss: 1.711540412902832\n",
      "Batch 915,  loss: 1.5550569295883179\n",
      "Batch 920,  loss: 1.347020673751831\n",
      "Batch 925,  loss: 1.2323042154312134\n",
      "Batch 930,  loss: 1.6707882404327392\n",
      "Batch 935,  loss: 1.5283002376556396\n",
      "Batch 940,  loss: 1.4356863498687744\n",
      "Batch 945,  loss: 1.4609599351882934\n",
      "Batch 950,  loss: 1.4544363021850586\n",
      "Batch 955,  loss: 1.362062406539917\n",
      "Batch 960,  loss: 1.3342992782592773\n",
      "Batch 965,  loss: 1.5455130100250245\n",
      "Batch 970,  loss: 1.43443021774292\n",
      "Batch 975,  loss: 1.7063410997390747\n",
      "Batch 980,  loss: 1.7012428522109986\n",
      "Batch 985,  loss: 1.5183254718780517\n",
      "Batch 990,  loss: 1.540704345703125\n",
      "Batch 995,  loss: 1.5333911418914794\n",
      "Batch 1000,  loss: 1.301162552833557\n",
      "Batch 1005,  loss: 1.4562849879264832\n",
      "Batch 1010,  loss: 1.4207244873046876\n",
      "Batch 1015,  loss: 1.3432223439216613\n",
      "Batch 1020,  loss: 1.476666235923767\n",
      "Batch 1025,  loss: 1.622287964820862\n",
      "Batch 1030,  loss: 1.597029173374176\n",
      "Batch 1035,  loss: 1.576069974899292\n",
      "Batch 1040,  loss: 1.5728598594665528\n",
      "Batch 1045,  loss: 1.2196643948554993\n",
      "Batch 1050,  loss: 1.2231659531593322\n",
      "Batch 1055,  loss: 1.4712458610534669\n",
      "Batch 1060,  loss: 1.4144979000091553\n",
      "Batch 1065,  loss: 1.2132806301116943\n",
      "Batch 1070,  loss: 1.4570611000061036\n",
      "Batch 1075,  loss: 1.3447555303573608\n",
      "Batch 1080,  loss: 1.528096318244934\n",
      "Batch 1085,  loss: 1.140898883342743\n",
      "Batch 1090,  loss: 1.5277916431427\n",
      "Batch 1095,  loss: 1.2633583307266236\n",
      "Batch 1100,  loss: 1.2122944593429565\n",
      "Batch 1105,  loss: 1.7767063140869142\n",
      "Batch 1110,  loss: 1.442904007434845\n",
      "Batch 1115,  loss: 1.3679333209991456\n",
      "Batch 1120,  loss: 1.3575740575790405\n",
      "Batch 1125,  loss: 1.898034429550171\n",
      "Batch 1130,  loss: 1.310860002040863\n",
      "Batch 1135,  loss: 1.3306894540786742\n",
      "Batch 1140,  loss: 1.3710841059684753\n",
      "Batch 1145,  loss: 1.50521080493927\n",
      "Batch 1150,  loss: 1.3600531339645385\n",
      "Batch 1155,  loss: 1.35811448097229\n",
      "Batch 1160,  loss: 1.477014970779419\n",
      "Batch 1165,  loss: 1.310499632358551\n",
      "Batch 1170,  loss: 1.530340552330017\n",
      "Batch 1175,  loss: 1.670123243331909\n",
      "Batch 1180,  loss: 1.3398366928100587\n",
      "Batch 1185,  loss: 1.2997391939163208\n",
      "Batch 1190,  loss: 1.4341207981109618\n",
      "Batch 1195,  loss: 1.2282411217689515\n",
      "Batch 1200,  loss: 1.2460222125053406\n",
      "Batch 1205,  loss: 1.5589885234832763\n",
      "Batch 1210,  loss: 1.7766571283340453\n",
      "Batch 1215,  loss: 1.6982905626296998\n",
      "Batch 1220,  loss: 1.5531244277954102\n",
      "Batch 1225,  loss: 1.4514497876167298\n",
      "Batch 1230,  loss: 1.3128184080123901\n",
      "Batch 1235,  loss: 1.3216716527938843\n",
      "Batch 1240,  loss: 1.4147319793701172\n",
      "Batch 1245,  loss: 1.7006402373313905\n",
      "Batch 1250,  loss: 1.2497278571128845\n",
      "Batch 1255,  loss: 1.6285696506500245\n",
      "Batch 1260,  loss: 1.489215350151062\n",
      "Batch 1265,  loss: 1.4286167860031127\n",
      "Batch 1270,  loss: 1.5845289707183838\n",
      "Batch 1275,  loss: 1.451414155960083\n",
      "Batch 1280,  loss: 1.5422655105590821\n",
      "Batch 1285,  loss: 1.6701427936553954\n",
      "Batch 1290,  loss: 1.374959897994995\n",
      "Batch 1295,  loss: 1.4301768779754638\n",
      "LOSS train 1.4301768779754638. Validation loss: 2.075445266716458 \n",
      "\n",
      "\n",
      "\n",
      "EPOCH 50:\n",
      "Batch 5,  loss: 1.6647056102752686\n",
      "Batch 10,  loss: 1.5598514318466186\n",
      "Batch 15,  loss: 1.312815523147583\n",
      "Batch 20,  loss: 1.484423041343689\n",
      "Batch 25,  loss: 1.549067735671997\n",
      "Batch 30,  loss: 1.4922598719596862\n",
      "Batch 35,  loss: 1.388749098777771\n",
      "Batch 40,  loss: 1.39147789478302\n",
      "Batch 45,  loss: 1.3073242902755737\n",
      "Batch 50,  loss: 1.506575846672058\n",
      "Batch 55,  loss: 1.4559301376342773\n",
      "Batch 60,  loss: 1.5401721715927124\n",
      "Batch 65,  loss: 1.3991178631782533\n",
      "Batch 70,  loss: 1.5763723373413085\n",
      "Batch 75,  loss: 1.4493854522705079\n",
      "Batch 80,  loss: 1.61408429145813\n",
      "Batch 85,  loss: 1.3699986696243287\n",
      "Batch 90,  loss: 1.480661392211914\n",
      "Batch 95,  loss: 1.4853118181228637\n",
      "Batch 100,  loss: 1.2484095096588135\n",
      "Batch 105,  loss: 1.5252529621124267\n",
      "Batch 110,  loss: 1.47622549533844\n",
      "Batch 115,  loss: 1.4952455759048462\n",
      "Batch 120,  loss: 1.4073044776916503\n",
      "Batch 125,  loss: 1.4681890249252318\n",
      "Batch 130,  loss: 1.2900803923606872\n",
      "Batch 135,  loss: 1.2383100986480713\n",
      "Batch 140,  loss: 1.5783058404922485\n",
      "Batch 145,  loss: 1.2237119793891906\n",
      "Batch 150,  loss: 1.4603402137756347\n",
      "Batch 155,  loss: 1.4486286401748658\n",
      "Batch 160,  loss: 1.3575567960739137\n",
      "Batch 165,  loss: 1.504733681678772\n",
      "Batch 170,  loss: 1.3093283891677856\n",
      "Batch 175,  loss: 1.538950300216675\n",
      "Batch 180,  loss: 1.4878860116004944\n",
      "Batch 185,  loss: 1.2016606688499452\n",
      "Batch 190,  loss: 1.4303648829460145\n",
      "Batch 195,  loss: 1.1602346658706666\n",
      "Batch 200,  loss: 1.2977444529533386\n",
      "Batch 205,  loss: 1.5170320272445679\n",
      "Batch 210,  loss: 1.1629906058311463\n",
      "Batch 215,  loss: 1.3656733274459838\n",
      "Batch 220,  loss: 1.3124043226242066\n",
      "Batch 225,  loss: 1.6118267059326172\n",
      "Batch 230,  loss: 1.2680391311645507\n",
      "Batch 235,  loss: 1.5830125093460083\n",
      "Batch 240,  loss: 1.2837052822113038\n",
      "Batch 245,  loss: 1.2816442251205444\n",
      "Batch 250,  loss: 2.1309234619140627\n",
      "Batch 255,  loss: 1.6190053820610046\n",
      "Batch 260,  loss: 1.3433328986167907\n",
      "Batch 265,  loss: 1.622630774974823\n",
      "Batch 270,  loss: 1.7108292818069457\n",
      "Batch 275,  loss: 1.5575134992599486\n",
      "Batch 280,  loss: 1.4619555950164795\n",
      "Batch 285,  loss: 1.3708115100860596\n",
      "Batch 290,  loss: 1.1759472608566284\n",
      "Batch 295,  loss: 1.4101249933242799\n",
      "Batch 300,  loss: 1.529250979423523\n",
      "Batch 305,  loss: 1.4652815580368042\n",
      "Batch 310,  loss: 1.5268342733383178\n",
      "Batch 315,  loss: 1.3843310594558715\n",
      "Batch 320,  loss: 1.3679527997970582\n",
      "Batch 325,  loss: 1.256580650806427\n",
      "Batch 330,  loss: 1.3982021689414978\n",
      "Batch 335,  loss: 1.3951632142066956\n",
      "Batch 340,  loss: 1.5893985748291015\n",
      "Batch 345,  loss: 1.3640464067459106\n",
      "Batch 350,  loss: 1.2991588950157165\n",
      "Batch 355,  loss: 1.4200191736221313\n",
      "Batch 360,  loss: 1.617640233039856\n",
      "Batch 365,  loss: 1.4117342472076415\n",
      "Batch 370,  loss: 1.2785630464553832\n",
      "Batch 375,  loss: 1.4219717979431152\n",
      "Batch 380,  loss: 1.6772158861160278\n",
      "Batch 385,  loss: 1.6449580669403077\n",
      "Batch 390,  loss: 1.291140580177307\n",
      "Batch 395,  loss: 1.6023341059684753\n",
      "Batch 400,  loss: 1.7378875732421875\n",
      "Batch 405,  loss: 1.3468943238258362\n",
      "Batch 410,  loss: 1.2782291889190673\n",
      "Batch 415,  loss: 1.439899492263794\n",
      "Batch 420,  loss: 0.9378481149673462\n",
      "Batch 425,  loss: 1.3127285480499267\n",
      "Batch 430,  loss: 1.2723064064979552\n",
      "Batch 435,  loss: 1.4233248233795166\n",
      "Batch 440,  loss: 1.5119858980178833\n",
      "Batch 445,  loss: 1.4264513015747071\n",
      "Batch 450,  loss: 1.4350585460662841\n",
      "Batch 455,  loss: 1.3560293793678284\n",
      "Batch 460,  loss: 1.2253170013427734\n",
      "Batch 465,  loss: 1.707873272895813\n",
      "Batch 470,  loss: 1.7747036933898925\n",
      "Batch 475,  loss: 1.4526402711868287\n",
      "Batch 480,  loss: 1.574860167503357\n",
      "Batch 485,  loss: 1.1898638010025024\n",
      "Batch 490,  loss: 1.6608670711517335\n",
      "Batch 495,  loss: 1.5180677413940429\n",
      "Batch 500,  loss: 1.4097814083099365\n",
      "Batch 505,  loss: 1.4540364742279053\n",
      "Batch 510,  loss: 1.5101692914962768\n",
      "Batch 515,  loss: 1.3230689406394958\n",
      "Batch 520,  loss: 1.2712831020355224\n",
      "Batch 525,  loss: 1.600860834121704\n",
      "Batch 530,  loss: 1.5341938257217407\n",
      "Batch 535,  loss: 1.3197631239891052\n",
      "Batch 540,  loss: 1.2356078624725342\n",
      "Batch 545,  loss: 1.2946619510650634\n",
      "Batch 550,  loss: 1.6608160018920899\n",
      "Batch 555,  loss: 1.4222321271896363\n",
      "Batch 560,  loss: 1.3913554191589355\n",
      "Batch 565,  loss: 1.2676572322845459\n",
      "Batch 570,  loss: 1.0651378989219666\n",
      "Batch 575,  loss: 1.6582079887390138\n",
      "Batch 580,  loss: 1.367852807044983\n",
      "Batch 585,  loss: 1.579161810874939\n",
      "Batch 590,  loss: 1.3658890008926392\n",
      "Batch 595,  loss: 1.188534927368164\n",
      "Batch 600,  loss: 1.3704969882965088\n",
      "Batch 605,  loss: 1.2068273544311523\n",
      "Batch 610,  loss: 1.2341451168060302\n",
      "Batch 615,  loss: 1.3675065994262696\n",
      "Batch 620,  loss: 1.3373888731002808\n",
      "Batch 625,  loss: 1.2975748300552368\n",
      "Batch 630,  loss: 1.5700568199157714\n",
      "Batch 635,  loss: 1.1876390099525451\n",
      "Batch 640,  loss: 1.3953941345214844\n",
      "Batch 645,  loss: 1.520050549507141\n",
      "Batch 650,  loss: 1.3081576824188232\n",
      "Batch 655,  loss: 1.6084568977355957\n",
      "Batch 660,  loss: 1.3311842918395995\n",
      "Batch 665,  loss: 1.280669355392456\n",
      "Batch 670,  loss: 1.4342055797576905\n",
      "Batch 675,  loss: 1.2685511827468872\n",
      "Batch 680,  loss: 1.503185510635376\n",
      "Batch 685,  loss: 1.322313904762268\n",
      "Batch 690,  loss: 1.2413020610809327\n",
      "Batch 695,  loss: 1.532279372215271\n",
      "Batch 700,  loss: 1.2875919699668885\n",
      "Batch 705,  loss: 1.112894558906555\n",
      "Batch 710,  loss: 1.4106249332427978\n",
      "Batch 715,  loss: 1.4638010740280152\n",
      "Batch 720,  loss: 1.300475037097931\n",
      "Batch 725,  loss: 1.1696229696273803\n",
      "Batch 730,  loss: 1.581291878223419\n",
      "Batch 735,  loss: 1.5983030319213867\n",
      "Batch 740,  loss: 1.1687312841415405\n",
      "Batch 745,  loss: 1.2330276846885682\n",
      "Batch 750,  loss: 1.4467347145080567\n",
      "Batch 755,  loss: 1.500982975959778\n",
      "Batch 760,  loss: 1.5582292795181274\n",
      "Batch 765,  loss: 1.5533918142318726\n",
      "Batch 770,  loss: 1.4132145166397094\n",
      "Batch 775,  loss: 1.2625839710235596\n",
      "Batch 780,  loss: 1.6321651697158814\n",
      "Batch 785,  loss: 1.3451251268386841\n",
      "Batch 790,  loss: 1.3926850318908692\n",
      "Batch 795,  loss: 1.5284899711608886\n",
      "Batch 800,  loss: 1.3952592849731444\n",
      "Batch 805,  loss: 1.5966678142547608\n",
      "Batch 810,  loss: 1.3131098747253418\n",
      "Batch 815,  loss: 1.190473771095276\n",
      "Batch 820,  loss: 1.326086449623108\n",
      "Batch 825,  loss: 1.3956542372703553\n",
      "Batch 830,  loss: 1.3778262376785277\n",
      "Batch 835,  loss: 1.7244614601135253\n",
      "Batch 840,  loss: 1.3316619634628295\n",
      "Batch 845,  loss: 1.6702150225639343\n",
      "Batch 850,  loss: 1.4313079118728638\n",
      "Batch 855,  loss: 1.770771050453186\n",
      "Batch 860,  loss: 1.3259545683860778\n",
      "Batch 865,  loss: 1.6449680089950562\n",
      "Batch 870,  loss: 1.7347529649734497\n",
      "Batch 875,  loss: 1.4997723817825317\n",
      "Batch 880,  loss: 1.5295408368110657\n",
      "Batch 885,  loss: 1.6096590757369995\n",
      "Batch 890,  loss: 1.46842520236969\n",
      "Batch 895,  loss: 1.5712923288345337\n",
      "Batch 900,  loss: 1.4651476383209228\n",
      "Batch 905,  loss: 1.3280702352523803\n",
      "Batch 910,  loss: 1.4104877233505249\n",
      "Batch 915,  loss: 1.1926761031150819\n",
      "Batch 920,  loss: 1.2753524780273438\n",
      "Batch 925,  loss: 1.3954992532730102\n",
      "Batch 930,  loss: 1.5375120639801025\n",
      "Batch 935,  loss: 1.3850314617156982\n",
      "Batch 940,  loss: 1.6540136098861695\n",
      "Batch 945,  loss: 1.5260412693023682\n",
      "Batch 950,  loss: 1.5712080001831055\n",
      "Batch 955,  loss: 1.5968086481094361\n",
      "Batch 960,  loss: 1.4693088054656982\n",
      "Batch 965,  loss: 1.4628618717193604\n",
      "Batch 970,  loss: 1.3815829396247863\n",
      "Batch 975,  loss: 1.2920372247695924\n",
      "Batch 980,  loss: 1.4541532278060914\n",
      "Batch 985,  loss: 1.3185471296310425\n",
      "Batch 990,  loss: 1.3728134751319885\n",
      "Batch 995,  loss: 1.227846097946167\n",
      "Batch 1000,  loss: 1.3734426498413086\n",
      "Batch 1005,  loss: 1.5113754987716674\n",
      "Batch 1010,  loss: 1.4184540510177612\n",
      "Batch 1015,  loss: 1.3944472074508667\n",
      "Batch 1020,  loss: 1.4082410335540771\n",
      "Batch 1025,  loss: 1.6351871728897094\n",
      "Batch 1030,  loss: 2.1478537797927855\n",
      "Batch 1035,  loss: 1.5393957853317262\n",
      "Batch 1040,  loss: 1.3732053518295289\n",
      "Batch 1045,  loss: 1.2129035353660584\n",
      "Batch 1050,  loss: 1.4469060063362122\n",
      "Batch 1055,  loss: 1.4563048839569093\n",
      "Batch 1060,  loss: 1.258097779750824\n",
      "Batch 1065,  loss: 1.1932478308677674\n",
      "Batch 1070,  loss: 1.4781511306762696\n",
      "Batch 1075,  loss: 1.4253254175186156\n",
      "Batch 1080,  loss: 1.2367948174476624\n",
      "Batch 1085,  loss: 1.4050376892089844\n",
      "Batch 1090,  loss: 1.3387182235717774\n",
      "Batch 1095,  loss: 1.348054337501526\n",
      "Batch 1100,  loss: 1.4895043611526488\n",
      "Batch 1105,  loss: 1.339020848274231\n",
      "Batch 1110,  loss: 1.7748952627182006\n",
      "Batch 1115,  loss: 1.2440378189086914\n",
      "Batch 1120,  loss: 1.4657376527786254\n",
      "Batch 1125,  loss: 1.7327814102172852\n",
      "Batch 1130,  loss: 1.2194926023483277\n",
      "Batch 1135,  loss: 1.389296329021454\n",
      "Batch 1140,  loss: 1.5545639514923095\n",
      "Batch 1145,  loss: 1.2031547009944916\n",
      "Batch 1150,  loss: 1.0866982579231261\n",
      "Batch 1155,  loss: 1.488227367401123\n",
      "Batch 1160,  loss: 1.4008017301559448\n",
      "Batch 1165,  loss: 1.3533653259277343\n",
      "Batch 1170,  loss: 1.6092231035232545\n",
      "Batch 1175,  loss: 1.5592153787612915\n",
      "Batch 1180,  loss: 1.3990468978881836\n",
      "Batch 1185,  loss: 1.3193503499031067\n",
      "Batch 1190,  loss: 1.2145994544029235\n",
      "Batch 1195,  loss: 1.3700620889663697\n",
      "Batch 1200,  loss: 1.6063083171844483\n",
      "Batch 1205,  loss: 1.2823808908462524\n",
      "Batch 1210,  loss: 1.4417407751083373\n",
      "Batch 1215,  loss: 1.2214018106460571\n",
      "Batch 1220,  loss: 1.5051302194595337\n",
      "Batch 1225,  loss: 1.414221727848053\n",
      "Batch 1230,  loss: 1.4383834600448608\n",
      "Batch 1235,  loss: 1.326101338863373\n",
      "Batch 1240,  loss: 1.4491190195083619\n",
      "Batch 1245,  loss: 1.382951283454895\n",
      "Batch 1250,  loss: 1.1835855960845947\n",
      "Batch 1255,  loss: 1.327945303916931\n",
      "Batch 1260,  loss: 1.5687899112701416\n",
      "Batch 1265,  loss: 1.2997886419296265\n",
      "Batch 1270,  loss: 1.3253298282623291\n",
      "Batch 1275,  loss: 1.45275936126709\n",
      "Batch 1280,  loss: 1.9166741847991944\n",
      "Batch 1285,  loss: 1.6614373207092286\n",
      "Batch 1290,  loss: 1.410708975791931\n",
      "Batch 1295,  loss: 1.342042875289917\n",
      "LOSS train 1.342042875289917. Validation loss: 2.100764373627802 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, loss_fn, optimizer, device, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_PATH+'voicemodel.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
