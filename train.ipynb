{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "This is the training section of the Spectrogram U-Net for Music Source Separation. Before running this, make sure the Spectrograms directory (along with the data points for testing and training, of course) is generated by running the preprocessor.py file once.\n",
    "\n",
    "The training process utilizes the architecture of U-Net implemented in the UNet.py file and the loss functions implemented in the loss_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from architectures.UNet.UNet import SpectrogramUNet\n",
    "from architectures.UNet.loss_functions import VocalLoss, InstrumentLoss\n",
    "from dataset import DSDDataset\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializations and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECTROGRAMS_PATH = './Spectrograms'\n",
    "VOCAL_ONLY = False\n",
    "MODEL_PATH = \"./models/vocal-accompaniment-separation/\" if VOCAL_ONLY else \"./models/all-separation/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_CHANNELS = 1\n",
    "OUT_CHANNELS = 2 if VOCAL_ONLY else 4\n",
    "FEATURES = [32, 64, 128, 256, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAL_ALPHA = 0.651 #as per my calculations that hinge on borderline delusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_VOCAL = 0.233\n",
    "ALPHA_DRUM = 0.263\n",
    "ALPHA_GUITAR = 0.286\n",
    "ALPHA4_OTHER = 0.218"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpectrogramUNet(in_channel=IN_CHANNELS, out_channel=OUT_CHANNELS, features=FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─ModuleList: 1-1                             --\n",
      "|    └─DoubleConv: 2-1                        --\n",
      "|    |    └─Sequential: 3-1                   9,696\n",
      "|    └─DoubleConv: 2-2                        --\n",
      "|    |    └─Sequential: 3-2                   55,680\n",
      "|    └─DoubleConv: 2-3                        --\n",
      "|    |    └─Sequential: 3-3                   221,952\n",
      "|    └─DoubleConv: 2-4                        --\n",
      "|    |    └─Sequential: 3-4                   886,272\n",
      "|    └─DoubleConv: 2-5                        --\n",
      "|    |    └─Sequential: 3-5                   3,542,016\n",
      "├─ModuleList: 1-2                             --\n",
      "|    └─UpSampling: 2-6                        --\n",
      "|    |    └─Sequential: 3-6                   3,277,568\n",
      "|    └─DoubleDeConv: 2-7                      --\n",
      "|    |    └─Sequential: 3-7                   1,771,008\n",
      "|    └─UpSampling: 2-8                        --\n",
      "|    |    └─Sequential: 3-8                   819,584\n",
      "|    └─DoubleDeConv: 2-9                      --\n",
      "|    |    └─Sequential: 3-9                   443,136\n",
      "|    └─UpSampling: 2-10                       --\n",
      "|    |    └─Sequential: 3-10                  204,992\n",
      "|    └─DoubleDeConv: 2-11                     --\n",
      "|    |    └─Sequential: 3-11                  110,976\n",
      "|    └─UpSampling: 2-12                       --\n",
      "|    |    └─Sequential: 3-12                  51,296\n",
      "|    └─DoubleDeConv: 2-13                     --\n",
      "|    |    └─Sequential: 3-13                  27,840\n",
      "├─MaxPool2d: 1-3                              --\n",
      "├─Conv2d: 1-4                                 132\n",
      "======================================================================\n",
      "Total params: 11,422,148\n",
      "Trainable params: 11,422,148\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─ModuleList: 1-1                             --\n",
       "|    └─DoubleConv: 2-1                        --\n",
       "|    |    └─Sequential: 3-1                   9,696\n",
       "|    └─DoubleConv: 2-2                        --\n",
       "|    |    └─Sequential: 3-2                   55,680\n",
       "|    └─DoubleConv: 2-3                        --\n",
       "|    |    └─Sequential: 3-3                   221,952\n",
       "|    └─DoubleConv: 2-4                        --\n",
       "|    |    └─Sequential: 3-4                   886,272\n",
       "|    └─DoubleConv: 2-5                        --\n",
       "|    |    └─Sequential: 3-5                   3,542,016\n",
       "├─ModuleList: 1-2                             --\n",
       "|    └─UpSampling: 2-6                        --\n",
       "|    |    └─Sequential: 3-6                   3,277,568\n",
       "|    └─DoubleDeConv: 2-7                      --\n",
       "|    |    └─Sequential: 3-7                   1,771,008\n",
       "|    └─UpSampling: 2-8                        --\n",
       "|    |    └─Sequential: 3-8                   819,584\n",
       "|    └─DoubleDeConv: 2-9                      --\n",
       "|    |    └─Sequential: 3-9                   443,136\n",
       "|    └─UpSampling: 2-10                       --\n",
       "|    |    └─Sequential: 3-10                  204,992\n",
       "|    └─DoubleDeConv: 2-11                     --\n",
       "|    |    └─Sequential: 3-11                  110,976\n",
       "|    └─UpSampling: 2-12                       --\n",
       "|    |    └─Sequential: 3-12                  51,296\n",
       "|    └─DoubleDeConv: 2-13                     --\n",
       "|    |    └─Sequential: 3-13                  27,840\n",
       "├─MaxPool2d: 1-3                              --\n",
       "├─Conv2d: 1-4                                 132\n",
       "======================================================================\n",
       "Total params: 11,422,148\n",
       "Trainable params: 11,422,148\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = DSDDataset(spectrograms_path=SPECTROGRAMS_PATH, vocal_only=VOCAL_ONLY, train=True)\n",
    "val_set = DSDDataset(spectrograms_path=SPECTROGRAMS_PATH, vocal_only=VOCAL_ONLY, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: torch.Size([8, 1, 1025, 173])\n",
      "\n",
      "Targets: dict_keys(['vocals', 'drums', 'guitar', 'other'])\n",
      "\n",
      "Vocal target shape: torch.Size([8, 1, 1025, 173])\n",
      "\n",
      "Guitar target shape: torch.Size([8, 1, 1025, 173])\n"
     ]
    }
   ],
   "source": [
    "for feature, target in train_loader:\n",
    "    print(f\"Feature shape: {feature.shape}\\n\")\n",
    "    print(f\"Targets: {target.keys()}\\n\")\n",
    "    print(f\"Vocal target shape: {target['vocals'].shape}\\n\")\n",
    "    print(f\"Guitar target shape: {target['guitar'].shape}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_loss(model, feature, target, loss_fn):\n",
    "\n",
    " outputs = model(feature)\n",
    "\n",
    " vocal_channel_output = outputs[:, 0, :, :].unsqueeze(1)\n",
    "\n",
    " if VOCAL_ONLY:\n",
    "    accompaniment_channel_output = outputs[:, 1, :, :].unsqueeze(1)\n",
    "    loss_list = [vocal_channel_output ,target['vocals'],  accompaniment_channel_output ,target['accompaniment']]\n",
    " else:\n",
    "    drums_channel_output = outputs[:, 1, :, :].unsqueeze(1)\n",
    "    guitar_channel_output = outputs[:, 2, :, :].unsqueeze(1)\n",
    "    other_channel_output = outputs[:, 3, :, :].unsqueeze(1)\n",
    "    loss_list = [vocal_channel_output ,target['vocals'],  drums_channel_output ,target['drums'], guitar_channel_output, target['guitar'], other_channel_output, target['other']]\n",
    "\n",
    " loss = loss_fn(*loss_list)\n",
    " return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(model, dataloader, loss_fn, optimizer, device):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "         \n",
    "        feature, target = data\n",
    "\n",
    "        feature = feature.to(device)\n",
    "        \n",
    "        for key in target:\n",
    "         target[key] = target[key].to(device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = determine_loss(model, feature, target, loss_fn )\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss+=loss.item()\n",
    "\n",
    "        if (i+1)%5==0:\n",
    "         last_loss = running_loss/5\n",
    "         print(f'Batch {i+1},  loss: {last_loss}')\n",
    "         running_loss=0\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_fn, optimizer, device, epochs):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f'EPOCH {epoch+1}:')\n",
    "\n",
    "        model.train(True)\n",
    "        avg_loss = train_one(model, train_loader, loss_fn, optimizer, device)\n",
    "\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                vfeature, vtarget = vdata\n",
    "                vfeature = vfeature.to(device)\n",
    "\n",
    "                for key in vtarget:\n",
    "                 vtarget[key] = vtarget[key].to(device)\n",
    "                \n",
    "                vloss = determine_loss(model, vfeature, vtarget, loss_fn)\n",
    "                running_val_loss += vloss.item()\n",
    "            avg_vloss = running_val_loss / (i + 1)\n",
    "            print(f'LOSS train {avg_loss}. Validation loss: {avg_vloss} \\n\\n\\n')\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "           checkpoint = {\n",
    "                'epoch': epoch,  \n",
    "                'model_state_dict': model.state_dict(), \n",
    "                'optimizer_state_dict': optimizer.state_dict(), \n",
    "                'avg_loss':avg_loss,\n",
    "                'avg_vloss':avg_vloss\n",
    "               \n",
    "                   }\n",
    "\n",
    "           torch.save(checkpoint, f'checkpoint_{epoch+1}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lr=LEARNING_RATE, params=model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = VocalLoss(alpha=VOCAL_ALPHA) if VOCAL_ONLY else InstrumentLoss(alpha1=ALPHA_VOCAL, alpha2=ALPHA_DRUM, alpha3=ALPHA_GUITAR, alpha4=ALPHA4_OTHER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "Batch 5,  loss: 0.4309066414833069\n",
      "Batch 10,  loss: 0.4229171216487885\n",
      "Batch 15,  loss: 0.3631866753101349\n",
      "Batch 20,  loss: 0.35952349901199343\n",
      "Batch 25,  loss: 0.28720176219940186\n",
      "Batch 30,  loss: 0.3011575162410736\n",
      "Batch 35,  loss: 0.3491924047470093\n",
      "Batch 40,  loss: 0.33786007165908816\n",
      "Batch 45,  loss: 0.37905105352401736\n",
      "Batch 50,  loss: 0.33777212500572207\n",
      "Batch 55,  loss: 0.30220515131950376\n",
      "Batch 60,  loss: 0.3172522604465485\n",
      "Batch 65,  loss: 0.28663956820964814\n",
      "Batch 70,  loss: 0.25768711864948274\n",
      "Batch 75,  loss: 0.26240317821502684\n",
      "Batch 80,  loss: 0.2849353015422821\n",
      "Batch 85,  loss: 0.29476993083953856\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, loss_fn, optimizer, device, epochs)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m running_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[37], line 24\u001b[0m, in \u001b[0;36mtrain_one\u001b[1;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 24\u001b[0m running_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     27\u001b[0m  last_loss \u001b[38;5;241m=\u001b[39m running_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, loss_fn, optimizer, device, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_PATH+'multisepmodelp1.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
