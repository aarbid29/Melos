{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "This is the training section of the Spectrogram U-Net for Music Source Separation. Before running this, make sure the Spectrograms directory (along with the data points for testing and training, of course) is generated by running the preprocessor.py file once.\n",
    "\n",
    "The training process utilizes the architecture of U-Net implemented in the UNet.py file and the loss functions implemented in the loss_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01marchitectures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mUNet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mUNet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpectrogramUNet\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\Code\\Minor Project\\venv\\Lib\\site-packages\\torch\\__init__.py:262\u001b[0m\n\u001b[0;32m    258\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    260\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 262\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\Code\\Minor Project\\venv\\Lib\\site-packages\\torch\\__init__.py:238\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    236\u001b[0m is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 238\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mkernel32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     last_error \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mget_last_error()\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from architectures.UNet.UNet import SpectrogramUNet\n",
    "from architectures.UNet.loss_functions import VocalLoss, InstrumentLoss\n",
    "from dataset import DSDDataset\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializations and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECTROGRAMS_PATH = './Spectrograms'\n",
    "VOCAL_ONLY = True\n",
    "MODEL_PATH = \"./models/vocal-accompaniment-separation/\" if VOCAL_ONLY else \"./models/all-separation/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_CHANNELS = 1\n",
    "OUT_CHANNELS = 2 if VOCAL_ONLY else 4\n",
    "FEATURES = [32, 64, 128, 256, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAL_ALPHA = 0.651 #as per my calculations that hinge on borderline delusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA1 = 0.297\n",
    "ALPHA2 = 0.262\n",
    "ALPHA3 = 0.232\n",
    "ALPHA4 = 0.209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpectrogramUNet(in_channel=IN_CHANNELS, out_channel=OUT_CHANNELS, features=FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "├─ModuleList: 1-1                             --\n",
      "|    └─DoubleConv: 2-1                        --\n",
      "|    |    └─Sequential: 3-1                   9,696\n",
      "|    └─DoubleConv: 2-2                        --\n",
      "|    |    └─Sequential: 3-2                   55,680\n",
      "|    └─DoubleConv: 2-3                        --\n",
      "|    |    └─Sequential: 3-3                   221,952\n",
      "|    └─DoubleConv: 2-4                        --\n",
      "|    |    └─Sequential: 3-4                   886,272\n",
      "|    └─DoubleConv: 2-5                        --\n",
      "|    |    └─Sequential: 3-5                   3,542,016\n",
      "├─ModuleList: 1-2                             --\n",
      "|    └─UpSampling: 2-6                        --\n",
      "|    |    └─Sequential: 3-6                   3,277,568\n",
      "|    └─DoubleDeConv: 2-7                      --\n",
      "|    |    └─Sequential: 3-7                   1,771,008\n",
      "|    └─UpSampling: 2-8                        --\n",
      "|    |    └─Sequential: 3-8                   819,584\n",
      "|    └─DoubleDeConv: 2-9                      --\n",
      "|    |    └─Sequential: 3-9                   443,136\n",
      "|    └─UpSampling: 2-10                       --\n",
      "|    |    └─Sequential: 3-10                  204,992\n",
      "|    └─DoubleDeConv: 2-11                     --\n",
      "|    |    └─Sequential: 3-11                  110,976\n",
      "|    └─UpSampling: 2-12                       --\n",
      "|    |    └─Sequential: 3-12                  51,296\n",
      "|    └─DoubleDeConv: 2-13                     --\n",
      "|    |    └─Sequential: 3-13                  27,840\n",
      "├─MaxPool2d: 1-3                              --\n",
      "├─Conv2d: 1-4                                 66\n",
      "======================================================================\n",
      "Total params: 11,422,082\n",
      "Trainable params: 11,422,082\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "├─ModuleList: 1-1                             --\n",
       "|    └─DoubleConv: 2-1                        --\n",
       "|    |    └─Sequential: 3-1                   9,696\n",
       "|    └─DoubleConv: 2-2                        --\n",
       "|    |    └─Sequential: 3-2                   55,680\n",
       "|    └─DoubleConv: 2-3                        --\n",
       "|    |    └─Sequential: 3-3                   221,952\n",
       "|    └─DoubleConv: 2-4                        --\n",
       "|    |    └─Sequential: 3-4                   886,272\n",
       "|    └─DoubleConv: 2-5                        --\n",
       "|    |    └─Sequential: 3-5                   3,542,016\n",
       "├─ModuleList: 1-2                             --\n",
       "|    └─UpSampling: 2-6                        --\n",
       "|    |    └─Sequential: 3-6                   3,277,568\n",
       "|    └─DoubleDeConv: 2-7                      --\n",
       "|    |    └─Sequential: 3-7                   1,771,008\n",
       "|    └─UpSampling: 2-8                        --\n",
       "|    |    └─Sequential: 3-8                   819,584\n",
       "|    └─DoubleDeConv: 2-9                      --\n",
       "|    |    └─Sequential: 3-9                   443,136\n",
       "|    └─UpSampling: 2-10                       --\n",
       "|    |    └─Sequential: 3-10                  204,992\n",
       "|    └─DoubleDeConv: 2-11                     --\n",
       "|    |    └─Sequential: 3-11                  110,976\n",
       "|    └─UpSampling: 2-12                       --\n",
       "|    |    └─Sequential: 3-12                  51,296\n",
       "|    └─DoubleDeConv: 2-13                     --\n",
       "|    |    └─Sequential: 3-13                  27,840\n",
       "├─MaxPool2d: 1-3                              --\n",
       "├─Conv2d: 1-4                                 66\n",
       "======================================================================\n",
       "Total params: 11,422,082\n",
       "Trainable params: 11,422,082\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = DSDDataset(spectrograms_path=SPECTROGRAMS_PATH, vocal_only=VOCAL_ONLY, train=True)\n",
    "val_set = DSDDataset(spectrograms_path=SPECTROGRAMS_PATH, vocal_only=VOCAL_ONLY, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: torch.Size([8, 1, 1025, 173])\n",
      "\n",
      "Targets: dict_keys(['vocals', 'accompaniment'])\n",
      "\n",
      "Vocal target shape: torch.Size([8, 1, 1025, 173])\n",
      "\n",
      "Accompaniment target shape: torch.Size([8, 1, 1025, 173])\n"
     ]
    }
   ],
   "source": [
    "for feature, target in train_loader:\n",
    "    print(f\"Feature shape: {feature.shape}\\n\")\n",
    "    print(f\"Targets: {target.keys()}\\n\")\n",
    "    print(f\"Vocal target shape: {target['vocals'].shape}\\n\")\n",
    "    print(f\"Accompaniment target shape: {target['accompaniment'].shape}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(model, dataloader, loss_fn, optimizer, device):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "         \n",
    "        feature, target = data\n",
    "\n",
    "        feature = feature.to(device)\n",
    "        target['accompaniment'] = target['accompaniment'].to(device)\n",
    "        target['vocals'] = target['vocals'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(feature)\n",
    "\n",
    "        vocal_channel_output = outputs[:, 0, :, :].unsqueeze(1)\n",
    "        accompaniment_channel_output = outputs[:, 1, :, :].unsqueeze(1)\n",
    "\n",
    "        loss = loss_fn(vocal_channel_output ,target['vocals'],  accompaniment_channel_output ,target['accompaniment'])\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss+=loss.item()\n",
    "\n",
    "        if (i+1)%5==0:\n",
    "         last_loss = running_loss/5\n",
    "         print(f'Batch {i+1},  loss: {last_loss}')\n",
    "         running_loss=0\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_fn, optimizer, device, epochs):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f'EPOCH {epoch+1}:')\n",
    "\n",
    "        model.train(True)\n",
    "        avg_loss = train_one(model, train_loader, loss_fn, optimizer, device)\n",
    "\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                vfeature, vtarget = vdata\n",
    "                vfeature = vfeature.to(device)\n",
    "                vtarget['accompaniment'] = vtarget['accompaniment'].to(device)\n",
    "                vtarget['vocals'] = vtarget['vocals'].to(device)\n",
    "                voutput = model(vfeature)\n",
    "\n",
    "                vocal_channel_output = voutput[:, 0, :, :].unsqueeze(1)\n",
    "                accompaniment_channel_output = voutput[:, 1, :, :].unsqueeze(1)\n",
    "                vloss = loss_fn(vocal_channel_output, vtarget['vocals'], accompaniment_channel_output, vtarget['accompaniment'])\n",
    "                running_val_loss += vloss.item()\n",
    "            avg_vloss = running_val_loss / (i + 1)\n",
    "            print(f'LOSS train {avg_loss}. Validation loss: {avg_vloss} \\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lr=LEARNING_RATE, params=model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = VocalLoss(alpha=VOCAL_ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, val_loader, loss_fn, optimizer, device, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_PATH+'voicemodelp2.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
